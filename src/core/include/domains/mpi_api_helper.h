/**
 * @file mpi_api_helper.h
 * @brief Helper functions for managing MPI API calls in the profiling framework.
 *
 * This file contains various utility functions used to handle MPI API calls 
 * within the profiling system, including retrieving argument values and 
 * managing function identifiers.
 * 
 * @note THIS FILE HAS BEEN AUTOMATICALLY GENERATED BY THE GILDA TOOL.
 * DO NOT MODIFY UNLESS YOU KNOW WHAT YOU ARE DOING.
 * ANY CHANGES MAY BE OVERWRITTEN BY SUBSEQUENT RUNS OF GILDA. 
 * 
 */

#ifndef MPI_API_HELPER_H
#define MPI_API_HELPER_H

#include <string.h>
#include <stdbool.h>
#include "domains/fun_proto/mpi_profiled_functions.h"
#include "mpi.h" 

#define MPI_STRING_SIZE_MAX 128

#ifdef ADD_API_PREFIX
#undef ADD_API_PREFIX
#endif
#define ADD_API_PREFIX(str) MPI_API_##str


#define FOR_EACH_MPI_FUNC(macro) \
IF_ENABLED(MPI_Init, macro)                        \
IF_ENABLED(MPI_Init_thread, macro)                 \
IF_ENABLED(MPI_Finalize, macro)                    \
IF_ENABLED(MPI_Initialized, macro)                 \
IF_ENABLED(MPI_Query_thread, macro)                \
IF_ENABLED(MPI_Abort, macro)                       \
IF_ENABLED(MPI_Send, macro)                        \
IF_ENABLED(MPI_Recv, macro)                        \
IF_ENABLED(MPI_Sendrecv, macro)                    \
IF_ENABLED(MPI_Sendrecv_replace, macro)            \
IF_ENABLED(MPI_Isend, macro)                       \
IF_ENABLED(MPI_Irecv, macro)                       \
IF_ENABLED(MPI_Wait, macro)                        \
IF_ENABLED(MPI_Waitall, macro)                     \
IF_ENABLED(MPI_Waitany, macro)                     \
IF_ENABLED(MPI_Waitsome, macro)                    \
IF_ENABLED(MPI_Test, macro)                        \
IF_ENABLED(MPI_Testall, macro)                     \
IF_ENABLED(MPI_Testany, macro)                     \
IF_ENABLED(MPI_Testsome, macro)                    \
IF_ENABLED(MPI_Request_free, macro)                \
IF_ENABLED(MPI_Cancel, macro)                      \
IF_ENABLED(MPI_Type_contiguous, macro)             \
IF_ENABLED(MPI_Type_vector, macro)                 \
IF_ENABLED(MPI_Type_indexed, macro)                \
IF_ENABLED(MPI_Type_create_indexed_block, macro)   \
IF_ENABLED(MPI_Type_create_struct, macro)          \
IF_ENABLED(MPI_Type_create_resized, macro)         \
IF_ENABLED(MPI_Type_commit, macro)                 \
IF_ENABLED(MPI_Type_free, macro)                   \
IF_ENABLED(MPI_Get_count, macro)                   \
IF_ENABLED(MPI_Get_elements, macro)                \
IF_ENABLED(MPI_Pack, macro)                        \
IF_ENABLED(MPI_Unpack, macro)                      \
IF_ENABLED(MPI_Pack_size, macro)                   \
IF_ENABLED(MPI_Barrier, macro)                     \
IF_ENABLED(MPI_Bcast, macro)                       \
IF_ENABLED(MPI_Gather, macro)                      \
IF_ENABLED(MPI_Gatherv, macro)                     \
IF_ENABLED(MPI_Scatter, macro)                     \
IF_ENABLED(MPI_Scatterv, macro)                    \
IF_ENABLED(MPI_Allgather, macro)                   \
IF_ENABLED(MPI_Allgatherv, macro)                  \
IF_ENABLED(MPI_Alltoall, macro)                    \
IF_ENABLED(MPI_Alltoallv, macro)                   \
IF_ENABLED(MPI_Reduce, macro)                      \
IF_ENABLED(MPI_Allreduce, macro)                   \
IF_ENABLED(MPI_Reduce_scatter, macro)              \
IF_ENABLED(MPI_Reduce_scatter_block, macro)        \
IF_ENABLED(MPI_Scan, macro)                        \
IF_ENABLED(MPI_Exscan, macro)                      \
IF_ENABLED(MPI_Comm_size, macro)                   \
IF_ENABLED(MPI_Comm_rank, macro)                   \
IF_ENABLED(MPI_Comm_group, macro)                  \
IF_ENABLED(MPI_Comm_dup, macro)                    \
IF_ENABLED(MPI_Comm_create, macro)                 \
IF_ENABLED(MPI_Comm_split, macro)                  \
IF_ENABLED(MPI_Comm_free, macro)                   \
IF_ENABLED(MPI_Comm_test_inter, macro)             \
IF_ENABLED(MPI_Comm_remote_size, macro)            \
IF_ENABLED(MPI_Comm_remote_group, macro)           \
IF_ENABLED(MPI_Comm_compare, macro)                \
IF_ENABLED(MPI_Comm_create_keyval, macro)          \
IF_ENABLED(MPI_Comm_set_attr, macro)               \
IF_ENABLED(MPI_Comm_get_attr, macro)               \
IF_ENABLED(MPI_Comm_delete_attr, macro)            \
IF_ENABLED(MPI_Comm_get_name, macro)               \
IF_ENABLED(MPI_Comm_set_name, macro)               \
IF_ENABLED(MPI_Group_size, macro)                  \
IF_ENABLED(MPI_Group_rank, macro)                  \
IF_ENABLED(MPI_Group_translate_ranks, macro)       \
IF_ENABLED(MPI_Group_compare, macro)               \
IF_ENABLED(MPI_Group_union, macro)                 \
IF_ENABLED(MPI_Group_intersection, macro)          \
IF_ENABLED(MPI_Group_difference, macro)            \
IF_ENABLED(MPI_Group_incl, macro)                  \
IF_ENABLED(MPI_Group_excl, macro)                  \
IF_ENABLED(MPI_Group_range_incl, macro)            \
IF_ENABLED(MPI_Group_range_excl, macro)            \
IF_ENABLED(MPI_Group_free, macro)                  \
IF_ENABLED(MPI_Op_create, macro)                   \
IF_ENABLED(MPI_Op_free, macro)                     \
IF_ENABLED(MPI_Wtime, macro)                       \
IF_ENABLED(MPI_Wtick, macro)                       \
IF_ENABLED(MPI_Get_address, macro)                 \
IF_ENABLED(MPI_Get_elements_x, macro)              \
IF_ENABLED(MPI_Cart_shift, macro)                  \
IF_ENABLED(MPI_Win_flush_local_all, macro)         \
IF_ENABLED(MPI_File_get_byte_offset, macro)        \
IF_ENABLED(MPI_Win_get_info, macro)                \
IF_ENABLED(MPI_Rput, macro)                        \
IF_ENABLED(MPI_Dist_graph_neighbors_count, macro)  \
IF_ENABLED(MPI_Ireduce, macro)                     \
IF_ENABLED(MPI_Psend_init, macro)                  \
IF_ENABLED(MPI_Reduce_init, macro)                 \
IF_ENABLED(MPI_Win_wait, macro)                    \
IF_ENABLED(MPI_Rsend_init, macro)                  \
IF_ENABLED(MPI_File_write_at_all, macro)           \
IF_ENABLED(MPI_File_write_ordered_end, macro)      \
IF_ENABLED(MPI_Errhandler_free, macro)             \
IF_ENABLED(MPI_Win_shared_query, macro)            \
IF_ENABLED(MPI_Win_lock, macro)                    \
IF_ENABLED(MPI_Get_accumulate, macro)              \
IF_ENABLED(MPI_Type_get_name, macro)               \
IF_ENABLED(MPI_File_get_atomicity, macro)          \
IF_ENABLED(MPI_Session_set_info, macro)            \
IF_ENABLED(MPI_Group_from_session_pset, macro)     \
IF_ENABLED(MPI_Comm_idup, macro)                   \
IF_ENABLED(MPI_Win_get_name, macro)                \
IF_ENABLED(MPI_Allgatherv_init, macro)             \
IF_ENABLED(MPI_Comm_dup_with_info, macro)          \
IF_ENABLED(MPI_Session_get_num_psets, macro)       \
IF_ENABLED(MPI_Igather, macro)                     \
IF_ENABLED(MPI_File_read_at, macro)                \
IF_ENABLED(MPI_Type_create_hvector, macro)         \
IF_ENABLED(MPI_File_write_at_all_begin, macro)     \
IF_ENABLED(MPI_Grequest_start, macro)              \
IF_ENABLED(MPI_Bsend_init, macro)                  \
IF_ENABLED(MPI_File_set_size, macro)               \
IF_ENABLED(MPI_Type_set_name, macro)               \
IF_ENABLED(MPI_Comm_split_type, macro)             \
IF_ENABLED(MPI_File_read_at_all_end, macro)        \
IF_ENABLED(MPI_File_write_all, macro)              \
IF_ENABLED(MPI_Improbe, macro)                     \
IF_ENABLED(MPI_Comm_get_info, macro)               \
IF_ENABLED(MPI_File_read_all_end, macro)           \
IF_ENABLED(MPI_Win_unlock_all, macro)              \
IF_ENABLED(MPI_Type_create_f90_integer, macro)     \
IF_ENABLED(MPI_Exscan_init, macro)                 \
IF_ENABLED(MPI_Ibsend, macro)                      \
IF_ENABLED(MPI_Win_flush_local, macro)             \
IF_ENABLED(MPI_Ialltoallw, macro)                  \
IF_ENABLED(MPI_Comm_create_from_group, macro)      \
IF_ENABLED(MPI_Type_get_contents, macro)           \
IF_ENABLED(MPI_File_iwrite_at, macro)              \
IF_ENABLED(MPI_Status_set_elements, macro)         \
IF_ENABLED(MPI_File_read_ordered, macro)           \
IF_ENABLED(MPI_Is_thread_main, macro)              \
IF_ENABLED(MPI_Allreduce_init, macro)              \
IF_ENABLED(MPI_Info_get_valuelen, macro)           \
IF_ENABLED(MPI_Comm_create_errhandler, macro)      \
IF_ENABLED(MPI_Info_free, macro)                   \
IF_ENABLED(MPI_Info_get_nthkey, macro)             \
IF_ENABLED(MPI_Ssend_init, macro)                  \
IF_ENABLED(MPI_Comm_set_info, macro)               \
IF_ENABLED(MPI_Cart_create, macro)                 \
IF_ENABLED(MPI_File_write_all_begin, macro)        \
IF_ENABLED(MPI_Scan_init, macro)                   \
IF_ENABLED(MPI_Irsend, macro)                      \
IF_ENABLED(MPI_Neighbor_alltoallv, macro)          \
IF_ENABLED(MPI_Pready_list, macro)                 \
IF_ENABLED(MPI_Alltoallw_init, macro)              \
IF_ENABLED(MPI_File_read_ordered_begin, macro)     \
IF_ENABLED(MPI_Dist_graph_create_adjacent, macro)  \
IF_ENABLED(MPI_Reduce_scatter_init, macro)         \
IF_ENABLED(MPI_Comm_get_parent, macro)             \
IF_ENABLED(MPI_Keyval_free, macro)                 \
IF_ENABLED(MPI_Info_set, macro)                    \
IF_ENABLED(MPI_Keyval_create, macro)               \
IF_ENABLED(MPI_Comm_connect, macro)                \
IF_ENABLED(MPI_Ssend, macro)                       \
IF_ENABLED(MPI_Scatterv_init, macro)               \
IF_ENABLED(MPI_File_write_at_all_end, macro)       \
IF_ENABLED(MPI_File_write_all_end, macro)          \
IF_ENABLED(MPI_Buffer_detach, macro)               \
IF_ENABLED(MPI_Startall, macro)                    \
IF_ENABLED(MPI_Neighbor_alltoall, macro)           \
IF_ENABLED(MPI_Put, macro)                         \
IF_ENABLED(MPI_File_read_ordered_end, macro)       \
IF_ENABLED(MPI_Win_call_errhandler, macro)         \
IF_ENABLED(MPI_File_write_at, macro)               \
IF_ENABLED(MPI_Session_get_pset_info, macro)       \
IF_ENABLED(MPI_Topo_test, macro)                   \
IF_ENABLED(MPI_Comm_disconnect, macro)             \
IF_ENABLED(MPI_Add_error_class, macro)             \
IF_ENABLED(MPI_Ireduce_scatter, macro)             \
IF_ENABLED(MPI_Cart_map, macro)                    \
IF_ENABLED(MPI_Intercomm_merge, macro)             \
IF_ENABLED(MPI_Type_create_hindexed, macro)        \
IF_ENABLED(MPI_Info_get_nkeys, macro)              \
IF_ENABLED(MPI_File_read, macro)                   \
IF_ENABLED(MPI_Ineighbor_allgatherv, macro)        \
IF_ENABLED(MPI_Attr_put, macro)                    \
IF_ENABLED(MPI_File_write_ordered_begin, macro)    \
IF_ENABLED(MPI_Status_set_elements_x, macro)       \
IF_ENABLED(MPI_Compare_and_swap, macro)            \
IF_ENABLED(MPI_Type_create_f90_real, macro)        \
IF_ENABLED(MPI_Type_delete_attr, macro)            \
IF_ENABLED(MPI_Probe, macro)                       \
IF_ENABLED(MPI_File_close, macro)                  \
IF_ENABLED(MPI_Request_get_status, macro)          \
IF_ENABLED(MPI_Session_call_errhandler, macro)     \
IF_ENABLED(MPI_Rget_accumulate, macro)             \
IF_ENABLED(MPI_File_iread_all, macro)              \
IF_ENABLED(MPI_Isendrecv, macro)                   \
IF_ENABLED(MPI_Pack_external, macro)               \
IF_ENABLED(MPI_Pready_range, macro)                \
IF_ENABLED(MPI_Type_get_envelope, macro)           \
IF_ENABLED(MPI_Win_create, macro)                  \
IF_ENABLED(MPI_Isendrecv_replace, macro)           \
IF_ENABLED(MPI_Win_set_errhandler, macro)          \
IF_ENABLED(MPI_Fetch_and_op, macro)                \
IF_ENABLED(MPI_Cartdim_get, macro)                 \
IF_ENABLED(MPI_Dist_graph_neighbors, macro)        \
IF_ENABLED(MPI_File_seek, macro)                   \
IF_ENABLED(MPI_Get, macro)                         \
IF_ENABLED(MPI_Pack_external_size, macro)          \
IF_ENABLED(MPI_Win_flush_all, macro)               \
IF_ENABLED(MPI_Rsend, macro)                       \
IF_ENABLED(MPI_Win_free, macro)                    \
IF_ENABLED(MPI_Type_create_f90_complex, macro)     \
IF_ENABLED(MPI_Neighbor_alltoallw_init, macro)     \
IF_ENABLED(MPI_Rget, macro)                        \
IF_ENABLED(MPI_Win_create_keyval, macro)           \
IF_ENABLED(MPI_Op_commutative, macro)              \
IF_ENABLED(MPI_Neighbor_allgather, macro)          \
IF_ENABLED(MPI_Comm_call_errhandler, macro)        \
IF_ENABLED(MPI_Scatter_init, macro)                \
IF_ENABLED(MPI_Info_get_string, macro)             \
IF_ENABLED(MPI_Mrecv, macro)                       \
IF_ENABLED(MPI_Open_port, macro)                   \
IF_ENABLED(MPI_Cart_get, macro)                    \
IF_ENABLED(MPI_Lookup_name, macro)                 \
IF_ENABLED(MPI_Type_get_extent, macro)             \
IF_ENABLED(MPI_Comm_spawn, macro)                  \
IF_ENABLED(MPI_Unpublish_name, macro)              \
IF_ENABLED(MPI_Grequest_complete, macro)           \
IF_ENABLED(MPI_File_get_group, macro)              \
IF_ENABLED(MPI_File_iread_at_all, macro)           \
IF_ENABLED(MPI_Graphdims_get, macro)               \
IF_ENABLED(MPI_File_iread_shared, macro)           \
IF_ENABLED(MPI_Comm_idup_with_info, macro)         \
IF_ENABLED(MPI_Get_version, macro)                 \
IF_ENABLED(MPI_Win_lock_all, macro)                \
IF_ENABLED(MPI_Intercomm_create_from_groups, macro) \
IF_ENABLED(MPI_Neighbor_alltoallv_init, macro)     \
IF_ENABLED(MPI_Type_create_darray, macro)          \
IF_ENABLED(MPI_File_get_position_shared, macro)    \
IF_ENABLED(MPI_Win_get_group, macro)               \
IF_ENABLED(MPI_Error_class, macro)                 \
IF_ENABLED(MPI_Win_get_attr, macro)                \
IF_ENABLED(MPI_Reduce_local, macro)                \
IF_ENABLED(MPI_Ireduce_scatter_block, macro)       \
IF_ENABLED(MPI_Status_set_cancelled, macro)        \
IF_ENABLED(MPI_Win_test, macro)                    \
IF_ENABLED(MPI_Test_cancelled, macro)              \
IF_ENABLED(MPI_File_seek_shared, macro)            \
IF_ENABLED(MPI_Error_string, macro)                \
IF_ENABLED(MPI_Graph_neighbors_count, macro)       \
IF_ENABLED(MPI_Session_create_errhandler, macro)   \
IF_ENABLED(MPI_Win_unlock, macro)                  \
IF_ENABLED(MPI_Iscatter, macro)                    \
IF_ENABLED(MPI_File_read_all, macro)               \
IF_ENABLED(MPI_File_set_errhandler, macro)         \
IF_ENABLED(MPI_Type_set_attr, macro)               \
IF_ENABLED(MPI_File_get_errhandler, macro)         \
IF_ENABLED(MPI_Session_finalize, macro)            \
IF_ENABLED(MPI_Comm_free_keyval, macro)            \
IF_ENABLED(MPI_File_iwrite_all, macro)             \
IF_ENABLED(MPI_Free_mem, macro)                    \
IF_ENABLED(MPI_Win_set_info, macro)                \
IF_ENABLED(MPI_Alltoallv_init, macro)              \
IF_ENABLED(MPI_Win_attach, macro)                  \
IF_ENABLED(MPI_File_get_position, macro)           \
IF_ENABLED(MPI_Accumulate, macro)                  \
IF_ENABLED(MPI_File_write_shared, macro)           \
IF_ENABLED(MPI_Win_create_dynamic, macro)          \
IF_ENABLED(MPI_Neighbor_alltoallw, macro)          \
IF_ENABLED(MPI_Iexscan, macro)                     \
IF_ENABLED(MPI_Graph_map, macro)                   \
IF_ENABLED(MPI_Recv_init, macro)                   \
IF_ENABLED(MPI_Type_create_subarray, macro)        \
IF_ENABLED(MPI_Comm_create_group, macro)           \
IF_ENABLED(MPI_Allgather_init, macro)              \
IF_ENABLED(MPI_Reduce_scatter_block_init, macro)   \
IF_ENABLED(MPI_Type_match_size, macro)             \
IF_ENABLED(MPI_Type_get_true_extent, macro)        \
IF_ENABLED(MPI_Alltoall_init, macro)               \
IF_ENABLED(MPI_Send_init, macro)                   \
IF_ENABLED(MPI_Neighbor_allgather_init, macro)     \
IF_ENABLED(MPI_Ibcast, macro)                      \
IF_ENABLED(MPI_File_iread, macro)                  \
IF_ENABLED(MPI_Neighbor_alltoall_init, macro)      \
IF_ENABLED(MPI_Cart_rank, macro)                   \
IF_ENABLED(MPI_Publish_name, macro)                \
IF_ENABLED(MPI_Win_set_attr, macro)                \
IF_ENABLED(MPI_Win_sync, macro)                    \
IF_ENABLED(MPI_Type_free_keyval, macro)            \
IF_ENABLED(MPI_File_write, macro)                  \
IF_ENABLED(MPI_Register_datarep, macro)            \
IF_ENABLED(MPI_Ineighbor_alltoall, macro)          \
IF_ENABLED(MPI_File_preallocate, macro)            \
IF_ENABLED(MPI_Iallgatherv, macro)                 \
IF_ENABLED(MPI_Neighbor_allgatherv_init, macro)    \
IF_ENABLED(MPI_Iprobe, macro)                      \
IF_ENABLED(MPI_Type_get_true_extent_x, macro)      \
IF_ENABLED(MPI_Win_complete, macro)                \
IF_ENABLED(MPI_File_set_atomicity, macro)          \
IF_ENABLED(MPI_Unpack_external, macro)             \
IF_ENABLED(MPI_Mprobe, macro)                      \
IF_ENABLED(MPI_Add_error_code, macro)              \
IF_ENABLED(MPI_Win_delete_attr, macro)             \
IF_ENABLED(MPI_File_read_at_all, macro)            \
IF_ENABLED(MPI_Pready, macro)                      \
IF_ENABLED(MPI_Iscatterv, macro)                   \
IF_ENABLED(MPI_Win_detach, macro)                  \
IF_ENABLED(MPI_File_call_errhandler, macro)        \
IF_ENABLED(MPI_Iallreduce, macro)                  \
IF_ENABLED(MPI_Get_processor_name, macro)          \
IF_ENABLED(MPI_Start, macro)                       \
IF_ENABLED(MPI_File_get_type_extent, macro)        \
IF_ENABLED(MPI_File_read_shared, macro)            \
IF_ENABLED(MPI_File_open, macro)                   \
IF_ENABLED(MPI_File_get_amode, macro)              \
IF_ENABLED(MPI_Type_create_hindexed_block, macro)  \
IF_ENABLED(MPI_Cart_coords, macro)                 \
IF_ENABLED(MPI_Issend, macro)                      \
IF_ENABLED(MPI_Graph_get, macro)                   \
IF_ENABLED(MPI_Win_free_keyval, macro)             \
IF_ENABLED(MPI_Ineighbor_alltoallw, macro)         \
IF_ENABLED(MPI_File_set_info, macro)               \
IF_ENABLED(MPI_File_iread_at, macro)               \
IF_ENABLED(MPI_Attr_delete, macro)                 \
IF_ENABLED(MPI_Session_get_info, macro)            \
IF_ENABLED(MPI_Session_get_nth_pset, macro)        \
IF_ENABLED(MPI_Type_create_keyval, macro)          \
IF_ENABLED(MPI_Attr_get, macro)                    \
IF_ENABLED(MPI_Add_error_string, macro)            \
IF_ENABLED(MPI_Ineighbor_alltoallv, macro)         \
IF_ENABLED(MPI_Imrecv, macro)                      \
IF_ENABLED(MPI_Alltoallw, macro)                   \
IF_ENABLED(MPI_Bcast_init, macro)                  \
IF_ENABLED(MPI_Ibarrier, macro)                    \
IF_ENABLED(MPI_File_iwrite_at_all, macro)          \
IF_ENABLED(MPI_File_get_size, macro)               \
IF_ENABLED(MPI_Barrier_init, macro)                \
IF_ENABLED(MPI_File_get_view, macro)               \
IF_ENABLED(MPI_Win_allocate_shared, macro)         \
IF_ENABLED(MPI_Close_port, macro)                  \
IF_ENABLED(MPI_Finalized, macro)                   \
IF_ENABLED(MPI_Info_dup, macro)                    \
IF_ENABLED(MPI_Info_get, macro)                    \
IF_ENABLED(MPI_Get_library_version, macro)         \
IF_ENABLED(MPI_Info_create, macro)                 \
IF_ENABLED(MPI_Win_fence, macro)                   \
IF_ENABLED(MPI_Iallgather, macro)                  \
IF_ENABLED(MPI_Comm_spawn_multiple, macro)         \
IF_ENABLED(MPI_Precv_init, macro)                  \
IF_ENABLED(MPI_Comm_set_errhandler, macro)         \
IF_ENABLED(MPI_File_set_view, macro)               \
IF_ENABLED(MPI_Bsend, macro)                       \
IF_ENABLED(MPI_Type_size, macro)                   \
IF_ENABLED(MPI_Type_get_attr, macro)               \
IF_ENABLED(MPI_File_write_ordered, macro)          \
IF_ENABLED(MPI_File_get_info, macro)               \
IF_ENABLED(MPI_Graph_neighbors, macro)             \
IF_ENABLED(MPI_Igatherv, macro)                    \
IF_ENABLED(MPI_Info_delete, macro)                 \
IF_ENABLED(MPI_Alloc_mem, macro)                   \
IF_ENABLED(MPI_Comm_get_errhandler, macro)         \
IF_ENABLED(MPI_Session_init, macro)                \
IF_ENABLED(MPI_Win_post, macro)                    \
IF_ENABLED(MPI_Intercomm_create, macro)            \
IF_ENABLED(MPI_File_read_all_begin, macro)         \
IF_ENABLED(MPI_Ialltoallv, macro)                  \
IF_ENABLED(MPI_File_delete, macro)                 \
IF_ENABLED(MPI_Dims_create, macro)                 \
IF_ENABLED(MPI_Cart_sub, macro)                    \
IF_ENABLED(MPI_Win_allocate, macro)                \
IF_ENABLED(MPI_Session_get_errhandler, macro)      \
IF_ENABLED(MPI_Parrived, macro)                    \
IF_ENABLED(MPI_Info_create_env, macro)             \
IF_ENABLED(MPI_File_create_errhandler, macro)      \
IF_ENABLED(MPI_Ialltoall, macro)                   \
IF_ENABLED(MPI_Raccumulate, macro)                 \
IF_ENABLED(MPI_Type_size_x, macro)                 \
IF_ENABLED(MPI_Type_get_extent_x, macro)           \
IF_ENABLED(MPI_File_read_at_all_begin, macro)      \
IF_ENABLED(MPI_Dist_graph_create, macro)           \
IF_ENABLED(MPI_Comm_join, macro)                   \
IF_ENABLED(MPI_Gatherv_init, macro)                \
IF_ENABLED(MPI_File_sync, macro)                   \
IF_ENABLED(MPI_Comm_accept, macro)                 \
IF_ENABLED(MPI_Ineighbor_allgather, macro)         \
IF_ENABLED(MPI_Type_dup, macro)                    \
IF_ENABLED(MPI_File_iwrite_shared, macro)          \
IF_ENABLED(MPI_Win_get_errhandler, macro)          \
IF_ENABLED(MPI_Iscan, macro)                       \
IF_ENABLED(MPI_Win_flush, macro)                   \
IF_ENABLED(MPI_Graph_create, macro)                \
IF_ENABLED(MPI_Win_set_name, macro)                \
IF_ENABLED(MPI_Win_create_errhandler, macro)       \
IF_ENABLED(MPI_Gather_init, macro)                 \
IF_ENABLED(MPI_Neighbor_allgatherv, macro)         \
IF_ENABLED(MPI_File_iwrite, macro)                 \
IF_ENABLED(MPI_Buffer_attach, macro)               \
IF_ENABLED(MPI_Session_set_errhandler, macro)      \
IF_ENABLED(MPI_Win_start, macro)                   \
IF_ENABLED(MPI_Info_f2c, macro)                    \
IF_ENABLED(MPI_Info_c2f, macro)                    \
IF_ENABLED(MPI_Op_c2f, macro)                      \
IF_ENABLED(MPI_Win_c2f, macro)                     \
IF_ENABLED(MPI_Group_f2c, macro)                   \
IF_ENABLED(MPI_File_c2f, macro)                    \
IF_ENABLED(MPI_Request_c2f, macro)                 \
IF_ENABLED(MPI_File_f2c, macro)                    \
IF_ENABLED(MPI_Session_f2c, macro)                 \
IF_ENABLED(MPI_Status_f082f, macro)                \
IF_ENABLED(MPI_Status_c2f08, macro)                \
IF_ENABLED(MPI_Type_f2c, macro)                    \
IF_ENABLED(MPI_Message_c2f, macro)                 \
IF_ENABLED(MPI_Session_c2f, macro)                 \
IF_ENABLED(MPI_Message_f2c, macro)                 \
IF_ENABLED(MPI_Errhandler_f2c, macro)              \
IF_ENABLED(MPI_Request_f2c, macro)                 \
IF_ENABLED(MPI_Status_c2f, macro)                  \
IF_ENABLED(MPI_Comm_f2c, macro)                    \
IF_ENABLED(MPI_Comm_c2f, macro)                    \
IF_ENABLED(MPI_Group_c2f, macro)                   \
IF_ENABLED(MPI_Win_f2c, macro)                     \
IF_ENABLED(MPI_Status_f082c, macro)                \
IF_ENABLED(MPI_Errhandler_c2f, macro)              \
IF_ENABLED(MPI_Status_f2f08, macro)                \
IF_ENABLED(MPI_Type_c2f, macro)                    \
IF_ENABLED(MPI_Status_f2c, macro)                  \
IF_ENABLED(MPI_Op_f2c, macro)                      \


/**
 * @enum mpi_api_id_t 
 * @brief Enumeration of MPI API function identifiers.
 *
 * This enumeration defines unique identifiers for various MPI API functions. 
 * These identifiers are used for profiling, tracking, and identifying specific MPI function calls.
 */
typedef enum mpi_api_id_e {
    FOR_EACH_MPI_FUNC(GET_FUNC_API_ID)
    MPI_API_ID_NB_FUNCTION,
    MPI_API_ID_UNKNOWN,
} mpi_api_id_t;


/**
 * @brief Retrieves the function name corresponding to a given MPI API function ID.
 *
 * This function maps a MPI API function identifier (`mpi_api_id_t`) to its corresponding function name
 * as a string. If the provided function ID does not match any known functions, the function returns `NULL`.
 *
 * @param id The MPI API function identifier of type `mpi_api_id_t`.
 * @return A constant string representing the function name, or `NULL` if the ID is unknown.
 */
static inline const char* get_mpi_funame_by_id(mpi_api_id_t id) 
{
    switch(id) {
        FOR_EACH_MPI_FUNC(GET_FUNAME_BY_ID_OF)
        default : return NULL;
    }
    return NULL;
}


/**
 * @brief Retrieves the function pointer corresponding to a given MPI API function ID.
 *
 * This function maps a MPI API function ID (`mpi_api_id_t`) to its corresponding function address.
 * If the provided function ID does not match any known functions, the function returns `NULL`.
 *
 * @param id The function ID of type `mpi_api_id_t`.
 * @return A pointer to the corresponding MPI API function, or `NULL` if not found.
 */
static inline void* get_mpi_funaddr_by_id(mpi_api_id_t id) 
{
    switch(id) {
        FOR_EACH_MPI_FUNC(GET_FUNADDR_BY_ID_OF)
        default : return NULL;
    }
    return NULL;
}


/**
 * @brief Retrieves the MPI API function ID corresponding to a given function name.
 *
 * This function maps a MPI API function name (string) to its corresponding function ID (`mpi_api_id_t`).
 * If the provided function name does not match any known functions, the function returns `MPI_API_ID_UNKNOWN`.
 *
 * @param name The function name as a null-terminated string.
 * @return The corresponding MPI API function ID of type `mpi_api_id_t`, or `MPI_API_ID_UNKNOWN` if not found.
 */
static inline mpi_api_id_t get_mpi_funid_by_name(const char* name) 
{
    if (name == NULL) return MPI_API_ID_UNKNOWN;
    FOR_EACH_MPI_FUNC(GET_FUNID_BY_NAME_OF)
    return MPI_API_ID_UNKNOWN;
}


// MPI API Args Data
/**
 * @brief Structure to hold the arguments for the `MPI_Init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Init` function call.
 *
 * @struct args_MPI_Init_t
 *
 * @note 
 *	int
 *	MPI_Init (
 *			int * argc (int *)
 *			char *** argv (char ***)
 *	)
 */
#if HAVE_MPI_Init
struct args_MPI_Init_t {
	int * argc;
	struct {
		int val;
	} argc__ref;
	char *** argv;
	struct {
		void* ptr1;
		void* ptr2;
		char val[MPI_STRING_SIZE_MAX];
	} argv__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Init(activity) { \
	activity->mpi_args.MPI_Init.argc = (int *) argc; \
	activity->mpi_args.MPI_Init.argv = (char ***) argv; \
};

#define GET_PTRS_VALUE_MPI_Init(args) { \
	if (args->MPI_Init.argc != NULL) { \
		args->MPI_Init.argc__ref.val = *args->MPI_Init.argc; \
	} \
	if (args->MPI_Init.argv != NULL) { \
		args->MPI_Init.argv__ref.ptr1 = *args->MPI_Init.argv; \
		if (args->MPI_Init.argv__ref.ptr1 != NULL) { \
			args->MPI_Init.argv__ref.ptr2 = **args->MPI_Init.argv; \
			if (args->MPI_Init.argv__ref.ptr2 != NULL) { \
				strncpy(args->MPI_Init.argv__ref.val, args->MPI_Init.argv__ref.ptr2, MPI_STRING_SIZE_MAX-1); \
			} \
		} \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Init_thread` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Init_thread` function call.
 *
 * @struct args_MPI_Init_thread_t
 *
 * @note 
 *	int
 *	MPI_Init_thread (
 *			int * argc (int *)
 *			char *** argv (char ***)
 *			int required (int)
 *			int * provided (int *)
 *	)
 */
#if HAVE_MPI_Init_thread
struct args_MPI_Init_thread_t {
	int * argc;
	struct {
		int val;
	} argc__ref;
	char *** argv;
	struct {
		void* ptr1;
		void* ptr2;
		char val[MPI_STRING_SIZE_MAX];
	} argv__ref;
	int required;
	int * provided;
	struct {
		int val;
	} provided__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Init_thread(activity) { \
	activity->mpi_args.MPI_Init_thread.argc = (int *) argc; \
	activity->mpi_args.MPI_Init_thread.argv = (char ***) argv; \
	activity->mpi_args.MPI_Init_thread.required = (int) required; \
	activity->mpi_args.MPI_Init_thread.provided = (int *) provided; \
};

#define GET_PTRS_VALUE_MPI_Init_thread(args) { \
	if (args->MPI_Init_thread.argc != NULL) { \
		args->MPI_Init_thread.argc__ref.val = *args->MPI_Init_thread.argc; \
	} \
	if (args->MPI_Init_thread.argv != NULL) { \
		args->MPI_Init_thread.argv__ref.ptr1 = *args->MPI_Init_thread.argv; \
		if (args->MPI_Init_thread.argv__ref.ptr1 != NULL) { \
			args->MPI_Init_thread.argv__ref.ptr2 = **args->MPI_Init_thread.argv; \
			if (args->MPI_Init_thread.argv__ref.ptr2 != NULL) { \
				strncpy(args->MPI_Init_thread.argv__ref.val, args->MPI_Init_thread.argv__ref.ptr2, MPI_STRING_SIZE_MAX-1); \
			} \
		} \
	} \
	if (args->MPI_Init_thread.provided != NULL) { \
		args->MPI_Init_thread.provided__ref.val = *args->MPI_Init_thread.provided; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Finalize` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Finalize` function call.
 *
 * @struct args_MPI_Finalize_t
 *
 * @note 
 *	int
 *	MPI_Finalize (
 *	)
 */
#if HAVE_MPI_Finalize
struct args_MPI_Finalize_t {
	int retval;
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Initialized` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Initialized` function call.
 *
 * @struct args_MPI_Initialized_t
 *
 * @note 
 *	int
 *	MPI_Initialized (
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Initialized
struct args_MPI_Initialized_t {
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Initialized(activity) { \
	activity->mpi_args.MPI_Initialized.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Initialized(args) { \
	if (args->MPI_Initialized.flag != NULL) { \
		args->MPI_Initialized.flag__ref.val = *args->MPI_Initialized.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Query_thread` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Query_thread` function call.
 *
 * @struct args_MPI_Query_thread_t
 *
 * @note 
 *	int
 *	MPI_Query_thread (
 *			int * provided (int *)
 *	)
 */
#if HAVE_MPI_Query_thread
struct args_MPI_Query_thread_t {
	int * provided;
	struct {
		int val;
	} provided__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Query_thread(activity) { \
	activity->mpi_args.MPI_Query_thread.provided = (int *) provided; \
};

#define GET_PTRS_VALUE_MPI_Query_thread(args) { \
	if (args->MPI_Query_thread.provided != NULL) { \
		args->MPI_Query_thread.provided__ref.val = *args->MPI_Query_thread.provided; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Abort` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Abort` function call.
 *
 * @struct args_MPI_Abort_t
 *
 * @note 
 *	int
 *	MPI_Abort (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int errorcode (int)
 *	)
 */
#if HAVE_MPI_Abort
struct args_MPI_Abort_t {
	MPI_Comm comm;
	int errorcode;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Abort(activity) { \
	activity->mpi_args.MPI_Abort.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Abort.errorcode = (int) errorcode; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Send` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Send` function call.
 *
 * @struct args_MPI_Send_t
 *
 * @note 
 *	int
 *	MPI_Send (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Send
struct args_MPI_Send_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Send(activity) { \
	activity->mpi_args.MPI_Send.buf = (void *) buf; \
	activity->mpi_args.MPI_Send.count = (int) count; \
	activity->mpi_args.MPI_Send.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Send.dest = (int) dest; \
	activity->mpi_args.MPI_Send.tag = (int) tag; \
	activity->mpi_args.MPI_Send.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Recv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Recv` function call.
 *
 * @struct args_MPI_Recv_t
 *
 * @note 
 *	int
 *	MPI_Recv (
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Recv
struct args_MPI_Recv_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int source;
	int tag;
	MPI_Comm comm;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Recv(activity) { \
	activity->mpi_args.MPI_Recv.buf = (void *) buf; \
	activity->mpi_args.MPI_Recv.count = (int) count; \
	activity->mpi_args.MPI_Recv.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Recv.source = (int) source; \
	activity->mpi_args.MPI_Recv.tag = (int) tag; \
	activity->mpi_args.MPI_Recv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Recv.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Recv(args) { \
	if (args->MPI_Recv.status != NULL) { \
		args->MPI_Recv.status__ref.val = *args->MPI_Recv.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Sendrecv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Sendrecv` function call.
 *
 * @struct args_MPI_Sendrecv_t
 *
 * @note 
 *	int
 *	MPI_Sendrecv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int sendtag (int)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int source (int)
 *			int recvtag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Sendrecv
struct args_MPI_Sendrecv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	int dest;
	int sendtag;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int source;
	int recvtag;
	MPI_Comm comm;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Sendrecv(activity) { \
	activity->mpi_args.MPI_Sendrecv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Sendrecv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Sendrecv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Sendrecv.dest = (int) dest; \
	activity->mpi_args.MPI_Sendrecv.sendtag = (int) sendtag; \
	activity->mpi_args.MPI_Sendrecv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Sendrecv.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Sendrecv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Sendrecv.source = (int) source; \
	activity->mpi_args.MPI_Sendrecv.recvtag = (int) recvtag; \
	activity->mpi_args.MPI_Sendrecv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Sendrecv.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Sendrecv(args) { \
	if (args->MPI_Sendrecv.status != NULL) { \
		args->MPI_Sendrecv.status__ref.val = *args->MPI_Sendrecv.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Sendrecv_replace` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Sendrecv_replace` function call.
 *
 * @struct args_MPI_Sendrecv_replace_t
 *
 * @note 
 *	int
 *	MPI_Sendrecv_replace (
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int sendtag (int)
 *			int source (int)
 *			int recvtag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Sendrecv_replace
struct args_MPI_Sendrecv_replace_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int sendtag;
	int source;
	int recvtag;
	MPI_Comm comm;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Sendrecv_replace(activity) { \
	activity->mpi_args.MPI_Sendrecv_replace.buf = (void *) buf; \
	activity->mpi_args.MPI_Sendrecv_replace.count = (int) count; \
	activity->mpi_args.MPI_Sendrecv_replace.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Sendrecv_replace.dest = (int) dest; \
	activity->mpi_args.MPI_Sendrecv_replace.sendtag = (int) sendtag; \
	activity->mpi_args.MPI_Sendrecv_replace.source = (int) source; \
	activity->mpi_args.MPI_Sendrecv_replace.recvtag = (int) recvtag; \
	activity->mpi_args.MPI_Sendrecv_replace.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Sendrecv_replace.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Sendrecv_replace(args) { \
	if (args->MPI_Sendrecv_replace.status != NULL) { \
		args->MPI_Sendrecv_replace.status__ref.val = *args->MPI_Sendrecv_replace.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Isend` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Isend` function call.
 *
 * @struct args_MPI_Isend_t
 *
 * @note 
 *	int
 *	MPI_Isend (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Isend
struct args_MPI_Isend_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Isend(activity) { \
	activity->mpi_args.MPI_Isend.buf = (void *) buf; \
	activity->mpi_args.MPI_Isend.count = (int) count; \
	activity->mpi_args.MPI_Isend.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Isend.dest = (int) dest; \
	activity->mpi_args.MPI_Isend.tag = (int) tag; \
	activity->mpi_args.MPI_Isend.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Isend.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Isend(args) { \
	if (args->MPI_Isend.request != NULL) { \
		args->MPI_Isend.request__ref.val = *args->MPI_Isend.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Irecv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Irecv` function call.
 *
 * @struct args_MPI_Irecv_t
 *
 * @note 
 *	int
 *	MPI_Irecv (
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Irecv
struct args_MPI_Irecv_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int source;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Irecv(activity) { \
	activity->mpi_args.MPI_Irecv.buf = (void *) buf; \
	activity->mpi_args.MPI_Irecv.count = (int) count; \
	activity->mpi_args.MPI_Irecv.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Irecv.source = (int) source; \
	activity->mpi_args.MPI_Irecv.tag = (int) tag; \
	activity->mpi_args.MPI_Irecv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Irecv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Irecv(args) { \
	if (args->MPI_Irecv.request != NULL) { \
		args->MPI_Irecv.request__ref.val = *args->MPI_Irecv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Wait` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Wait` function call.
 *
 * @struct args_MPI_Wait_t
 *
 * @note 
 *	int
 *	MPI_Wait (
 *			MPI_Request * request (struct mpi_request_t **)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Wait
struct args_MPI_Wait_t {
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Wait(activity) { \
	activity->mpi_args.MPI_Wait.request = (MPI_Request *) request; \
	activity->mpi_args.MPI_Wait.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Wait(args) { \
	if (args->MPI_Wait.request != NULL) { \
		args->MPI_Wait.request__ref.val = *args->MPI_Wait.request; \
	} \
	if (args->MPI_Wait.status != NULL) { \
		args->MPI_Wait.status__ref.val = *args->MPI_Wait.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Waitall` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Waitall` function call.
 *
 * @struct args_MPI_Waitall_t
 *
 * @note 
 *	int
 *	MPI_Waitall (
 *			int count (int)
 *			MPI_Request[] array_of_requests (struct mpi_request_t *[])
 *			MPI_Status * array_of_statuses (struct opaque **)
 *	)
 */
#if HAVE_MPI_Waitall
struct args_MPI_Waitall_t {
	int count;
	MPI_Request(* array_of_requests);
	struct {
		MPI_Request val;
	} array_of_requests__ref;
	MPI_Status * array_of_statuses;
	struct {
		MPI_Status val;
	} array_of_statuses__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Waitall(activity) { \
	activity->mpi_args.MPI_Waitall.count = (int) count; \
	activity->mpi_args.MPI_Waitall.array_of_requests = (MPI_Request(*)) array_of_requests; \
	activity->mpi_args.MPI_Waitall.array_of_statuses = (MPI_Status *) array_of_statuses; \
};

#define GET_PTRS_VALUE_MPI_Waitall(args) { \
	if (args->MPI_Waitall.array_of_requests != NULL) { \
		args->MPI_Waitall.array_of_requests__ref.val = *args->MPI_Waitall.array_of_requests; \
	} \
	if (args->MPI_Waitall.array_of_statuses != NULL) { \
		args->MPI_Waitall.array_of_statuses__ref.val = *args->MPI_Waitall.array_of_statuses; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Waitany` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Waitany` function call.
 *
 * @struct args_MPI_Waitany_t
 *
 * @note 
 *	int
 *	MPI_Waitany (
 *			int count (int)
 *			MPI_Request[] array_of_requests (struct mpi_request_t *[])
 *			int * index (int *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Waitany
struct args_MPI_Waitany_t {
	int count;
	MPI_Request(* array_of_requests);
	struct {
		MPI_Request val;
	} array_of_requests__ref;
	int * index;
	struct {
		int val;
	} index__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Waitany(activity) { \
	activity->mpi_args.MPI_Waitany.count = (int) count; \
	activity->mpi_args.MPI_Waitany.array_of_requests = (MPI_Request(*)) array_of_requests; \
	activity->mpi_args.MPI_Waitany.index = (int *) index; \
	activity->mpi_args.MPI_Waitany.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Waitany(args) { \
	if (args->MPI_Waitany.array_of_requests != NULL) { \
		args->MPI_Waitany.array_of_requests__ref.val = *args->MPI_Waitany.array_of_requests; \
	} \
	if (args->MPI_Waitany.index != NULL) { \
		args->MPI_Waitany.index__ref.val = *args->MPI_Waitany.index; \
	} \
	if (args->MPI_Waitany.status != NULL) { \
		args->MPI_Waitany.status__ref.val = *args->MPI_Waitany.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Waitsome` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Waitsome` function call.
 *
 * @struct args_MPI_Waitsome_t
 *
 * @note 
 *	int
 *	MPI_Waitsome (
 *			int incount (int)
 *			MPI_Request[] array_of_requests (struct mpi_request_t *[])
 *			int * outcount (int *)
 *			int[] array_of_indices (int[])
 *			MPI_Status[] array_of_statuses (struct opaque *[])
 *	)
 */
#if HAVE_MPI_Waitsome
struct args_MPI_Waitsome_t {
	int incount;
	MPI_Request(* array_of_requests);
	struct {
		MPI_Request val;
	} array_of_requests__ref;
	int * outcount;
	struct {
		int val;
	} outcount__ref;
	int(* array_of_indices);
	struct {
		int val;
	} array_of_indices__ref;
	MPI_Status(* array_of_statuses);
	struct {
		MPI_Status val;
	} array_of_statuses__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Waitsome(activity) { \
	activity->mpi_args.MPI_Waitsome.incount = (int) incount; \
	activity->mpi_args.MPI_Waitsome.array_of_requests = (MPI_Request(*)) array_of_requests; \
	activity->mpi_args.MPI_Waitsome.outcount = (int *) outcount; \
	activity->mpi_args.MPI_Waitsome.array_of_indices = (int(*)) array_of_indices; \
	activity->mpi_args.MPI_Waitsome.array_of_statuses = (MPI_Status(*)) array_of_statuses; \
};

#define GET_PTRS_VALUE_MPI_Waitsome(args) { \
	if (args->MPI_Waitsome.array_of_requests != NULL) { \
		args->MPI_Waitsome.array_of_requests__ref.val = *args->MPI_Waitsome.array_of_requests; \
	} \
	if (args->MPI_Waitsome.outcount != NULL) { \
		args->MPI_Waitsome.outcount__ref.val = *args->MPI_Waitsome.outcount; \
	} \
	if (args->MPI_Waitsome.array_of_indices != NULL) { \
		args->MPI_Waitsome.array_of_indices__ref.val = *args->MPI_Waitsome.array_of_indices; \
	} \
	if (args->MPI_Waitsome.array_of_statuses != NULL) { \
		args->MPI_Waitsome.array_of_statuses__ref.val = *args->MPI_Waitsome.array_of_statuses; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Test` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Test` function call.
 *
 * @struct args_MPI_Test_t
 *
 * @note 
 *	int
 *	MPI_Test (
 *			MPI_Request * request (struct mpi_request_t **)
 *			int * flag (int *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Test
struct args_MPI_Test_t {
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int * flag;
	struct {
		int val;
	} flag__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Test(activity) { \
	activity->mpi_args.MPI_Test.request = (MPI_Request *) request; \
	activity->mpi_args.MPI_Test.flag = (int *) flag; \
	activity->mpi_args.MPI_Test.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Test(args) { \
	if (args->MPI_Test.request != NULL) { \
		args->MPI_Test.request__ref.val = *args->MPI_Test.request; \
	} \
	if (args->MPI_Test.flag != NULL) { \
		args->MPI_Test.flag__ref.val = *args->MPI_Test.flag; \
	} \
	if (args->MPI_Test.status != NULL) { \
		args->MPI_Test.status__ref.val = *args->MPI_Test.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Testall` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Testall` function call.
 *
 * @struct args_MPI_Testall_t
 *
 * @note 
 *	int
 *	MPI_Testall (
 *			int count (int)
 *			MPI_Request[] array_of_requests (struct mpi_request_t *[])
 *			int * flag (int *)
 *			MPI_Status[] array_of_statuses (struct opaque *[])
 *	)
 */
#if HAVE_MPI_Testall
struct args_MPI_Testall_t {
	int count;
	MPI_Request(* array_of_requests);
	struct {
		MPI_Request val;
	} array_of_requests__ref;
	int * flag;
	struct {
		int val;
	} flag__ref;
	MPI_Status(* array_of_statuses);
	struct {
		MPI_Status val;
	} array_of_statuses__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Testall(activity) { \
	activity->mpi_args.MPI_Testall.count = (int) count; \
	activity->mpi_args.MPI_Testall.array_of_requests = (MPI_Request(*)) array_of_requests; \
	activity->mpi_args.MPI_Testall.flag = (int *) flag; \
	activity->mpi_args.MPI_Testall.array_of_statuses = (MPI_Status(*)) array_of_statuses; \
};

#define GET_PTRS_VALUE_MPI_Testall(args) { \
	if (args->MPI_Testall.array_of_requests != NULL) { \
		args->MPI_Testall.array_of_requests__ref.val = *args->MPI_Testall.array_of_requests; \
	} \
	if (args->MPI_Testall.flag != NULL) { \
		args->MPI_Testall.flag__ref.val = *args->MPI_Testall.flag; \
	} \
	if (args->MPI_Testall.array_of_statuses != NULL) { \
		args->MPI_Testall.array_of_statuses__ref.val = *args->MPI_Testall.array_of_statuses; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Testany` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Testany` function call.
 *
 * @struct args_MPI_Testany_t
 *
 * @note 
 *	int
 *	MPI_Testany (
 *			int count (int)
 *			MPI_Request[] array_of_requests (struct mpi_request_t *[])
 *			int * index (int *)
 *			int * flag (int *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Testany
struct args_MPI_Testany_t {
	int count;
	MPI_Request(* array_of_requests);
	struct {
		MPI_Request val;
	} array_of_requests__ref;
	int * index;
	struct {
		int val;
	} index__ref;
	int * flag;
	struct {
		int val;
	} flag__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Testany(activity) { \
	activity->mpi_args.MPI_Testany.count = (int) count; \
	activity->mpi_args.MPI_Testany.array_of_requests = (MPI_Request(*)) array_of_requests; \
	activity->mpi_args.MPI_Testany.index = (int *) index; \
	activity->mpi_args.MPI_Testany.flag = (int *) flag; \
	activity->mpi_args.MPI_Testany.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Testany(args) { \
	if (args->MPI_Testany.array_of_requests != NULL) { \
		args->MPI_Testany.array_of_requests__ref.val = *args->MPI_Testany.array_of_requests; \
	} \
	if (args->MPI_Testany.index != NULL) { \
		args->MPI_Testany.index__ref.val = *args->MPI_Testany.index; \
	} \
	if (args->MPI_Testany.flag != NULL) { \
		args->MPI_Testany.flag__ref.val = *args->MPI_Testany.flag; \
	} \
	if (args->MPI_Testany.status != NULL) { \
		args->MPI_Testany.status__ref.val = *args->MPI_Testany.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Testsome` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Testsome` function call.
 *
 * @struct args_MPI_Testsome_t
 *
 * @note 
 *	int
 *	MPI_Testsome (
 *			int incount (int)
 *			MPI_Request[] array_of_requests (struct mpi_request_t *[])
 *			int * outcount (int *)
 *			int[] array_of_indices (int[])
 *			MPI_Status[] array_of_statuses (struct opaque *[])
 *	)
 */
#if HAVE_MPI_Testsome
struct args_MPI_Testsome_t {
	int incount;
	MPI_Request(* array_of_requests);
	struct {
		MPI_Request val;
	} array_of_requests__ref;
	int * outcount;
	struct {
		int val;
	} outcount__ref;
	int(* array_of_indices);
	struct {
		int val;
	} array_of_indices__ref;
	MPI_Status(* array_of_statuses);
	struct {
		MPI_Status val;
	} array_of_statuses__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Testsome(activity) { \
	activity->mpi_args.MPI_Testsome.incount = (int) incount; \
	activity->mpi_args.MPI_Testsome.array_of_requests = (MPI_Request(*)) array_of_requests; \
	activity->mpi_args.MPI_Testsome.outcount = (int *) outcount; \
	activity->mpi_args.MPI_Testsome.array_of_indices = (int(*)) array_of_indices; \
	activity->mpi_args.MPI_Testsome.array_of_statuses = (MPI_Status(*)) array_of_statuses; \
};

#define GET_PTRS_VALUE_MPI_Testsome(args) { \
	if (args->MPI_Testsome.array_of_requests != NULL) { \
		args->MPI_Testsome.array_of_requests__ref.val = *args->MPI_Testsome.array_of_requests; \
	} \
	if (args->MPI_Testsome.outcount != NULL) { \
		args->MPI_Testsome.outcount__ref.val = *args->MPI_Testsome.outcount; \
	} \
	if (args->MPI_Testsome.array_of_indices != NULL) { \
		args->MPI_Testsome.array_of_indices__ref.val = *args->MPI_Testsome.array_of_indices; \
	} \
	if (args->MPI_Testsome.array_of_statuses != NULL) { \
		args->MPI_Testsome.array_of_statuses__ref.val = *args->MPI_Testsome.array_of_statuses; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Request_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Request_free` function call.
 *
 * @struct args_MPI_Request_free_t
 *
 * @note 
 *	int
 *	MPI_Request_free (
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Request_free
struct args_MPI_Request_free_t {
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Request_free(activity) { \
	activity->mpi_args.MPI_Request_free.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Request_free(args) { \
	if (args->MPI_Request_free.request != NULL) { \
		args->MPI_Request_free.request__ref.val = *args->MPI_Request_free.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cancel` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cancel` function call.
 *
 * @struct args_MPI_Cancel_t
 *
 * @note 
 *	int
 *	MPI_Cancel (
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Cancel
struct args_MPI_Cancel_t {
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cancel(activity) { \
	activity->mpi_args.MPI_Cancel.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Cancel(args) { \
	if (args->MPI_Cancel.request != NULL) { \
		args->MPI_Cancel.request__ref.val = *args->MPI_Cancel.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_contiguous` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_contiguous` function call.
 *
 * @struct args_MPI_Type_contiguous_t
 *
 * @note 
 *	int
 *	MPI_Type_contiguous (
 *			int count (int)
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_contiguous
struct args_MPI_Type_contiguous_t {
	int count;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_contiguous(activity) { \
	activity->mpi_args.MPI_Type_contiguous.count = (int) count; \
	activity->mpi_args.MPI_Type_contiguous.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_contiguous.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_contiguous(args) { \
	if (args->MPI_Type_contiguous.newtype != NULL) { \
		args->MPI_Type_contiguous.newtype__ref.val = *args->MPI_Type_contiguous.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_vector` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_vector` function call.
 *
 * @struct args_MPI_Type_vector_t
 *
 * @note 
 *	int
 *	MPI_Type_vector (
 *			int count (int)
 *			int blocklength (int)
 *			int stride (int)
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_vector
struct args_MPI_Type_vector_t {
	int count;
	int blocklength;
	int stride;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_vector(activity) { \
	activity->mpi_args.MPI_Type_vector.count = (int) count; \
	activity->mpi_args.MPI_Type_vector.blocklength = (int) blocklength; \
	activity->mpi_args.MPI_Type_vector.stride = (int) stride; \
	activity->mpi_args.MPI_Type_vector.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_vector.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_vector(args) { \
	if (args->MPI_Type_vector.newtype != NULL) { \
		args->MPI_Type_vector.newtype__ref.val = *args->MPI_Type_vector.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_indexed` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_indexed` function call.
 *
 * @struct args_MPI_Type_indexed_t
 *
 * @note 
 *	int
 *	MPI_Type_indexed (
 *			int count (int)
 *			const int[] array_of_blocklengths (const int[])
 *			const int[] array_of_displacements (const int[])
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_indexed
struct args_MPI_Type_indexed_t {
	int count;
	int(* array_of_blocklengths);
	struct {
		int val;
	} array_of_blocklengths__ref;
	int(* array_of_displacements);
	struct {
		int val;
	} array_of_displacements__ref;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_indexed(activity) { \
	activity->mpi_args.MPI_Type_indexed.count = (int) count; \
	activity->mpi_args.MPI_Type_indexed.array_of_blocklengths = (int(*)) array_of_blocklengths; \
	activity->mpi_args.MPI_Type_indexed.array_of_displacements = (int(*)) array_of_displacements; \
	activity->mpi_args.MPI_Type_indexed.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_indexed.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_indexed(args) { \
	if (args->MPI_Type_indexed.array_of_blocklengths != NULL) { \
		args->MPI_Type_indexed.array_of_blocklengths__ref.val = *args->MPI_Type_indexed.array_of_blocklengths; \
	} \
	if (args->MPI_Type_indexed.array_of_displacements != NULL) { \
		args->MPI_Type_indexed.array_of_displacements__ref.val = *args->MPI_Type_indexed.array_of_displacements; \
	} \
	if (args->MPI_Type_indexed.newtype != NULL) { \
		args->MPI_Type_indexed.newtype__ref.val = *args->MPI_Type_indexed.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_indexed_block` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_indexed_block` function call.
 *
 * @struct args_MPI_Type_create_indexed_block_t
 *
 * @note 
 *	int
 *	MPI_Type_create_indexed_block (
 *			int count (int)
 *			int blocklength (int)
 *			const int[] array_of_displacements (const int[])
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_indexed_block
struct args_MPI_Type_create_indexed_block_t {
	int count;
	int blocklength;
	int(* array_of_displacements);
	struct {
		int val;
	} array_of_displacements__ref;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_indexed_block(activity) { \
	activity->mpi_args.MPI_Type_create_indexed_block.count = (int) count; \
	activity->mpi_args.MPI_Type_create_indexed_block.blocklength = (int) blocklength; \
	activity->mpi_args.MPI_Type_create_indexed_block.array_of_displacements = (int(*)) array_of_displacements; \
	activity->mpi_args.MPI_Type_create_indexed_block.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_create_indexed_block.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_indexed_block(args) { \
	if (args->MPI_Type_create_indexed_block.array_of_displacements != NULL) { \
		args->MPI_Type_create_indexed_block.array_of_displacements__ref.val = *args->MPI_Type_create_indexed_block.array_of_displacements; \
	} \
	if (args->MPI_Type_create_indexed_block.newtype != NULL) { \
		args->MPI_Type_create_indexed_block.newtype__ref.val = *args->MPI_Type_create_indexed_block.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_struct` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_struct` function call.
 *
 * @struct args_MPI_Type_create_struct_t
 *
 * @note 
 *	int
 *	MPI_Type_create_struct (
 *			int count (int)
 *			const int[] array_of_block_lengths (const int[])
 *			const MPI_Aint[] array_of_displacements (const long[])
 *			const MPI_Datatype[] array_of_types (const struct mpi_datatype_t *[])
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_struct
struct args_MPI_Type_create_struct_t {
	int count;
	int(* array_of_block_lengths);
	struct {
		int val;
	} array_of_block_lengths__ref;
	MPI_Aint(* array_of_displacements);
	struct {
		MPI_Aint val;
	} array_of_displacements__ref;
	MPI_Datatype(* array_of_types);
	struct {
		MPI_Datatype val;
	} array_of_types__ref;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_struct(activity) { \
	activity->mpi_args.MPI_Type_create_struct.count = (int) count; \
	activity->mpi_args.MPI_Type_create_struct.array_of_block_lengths = (int(*)) array_of_block_lengths; \
	activity->mpi_args.MPI_Type_create_struct.array_of_displacements = (MPI_Aint(*)) array_of_displacements; \
	activity->mpi_args.MPI_Type_create_struct.array_of_types = (MPI_Datatype(*)) array_of_types; \
	activity->mpi_args.MPI_Type_create_struct.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_struct(args) { \
	if (args->MPI_Type_create_struct.array_of_block_lengths != NULL) { \
		args->MPI_Type_create_struct.array_of_block_lengths__ref.val = *args->MPI_Type_create_struct.array_of_block_lengths; \
	} \
	if (args->MPI_Type_create_struct.array_of_displacements != NULL) { \
		args->MPI_Type_create_struct.array_of_displacements__ref.val = *args->MPI_Type_create_struct.array_of_displacements; \
	} \
	if (args->MPI_Type_create_struct.array_of_types != NULL) { \
		args->MPI_Type_create_struct.array_of_types__ref.val = *args->MPI_Type_create_struct.array_of_types; \
	} \
	if (args->MPI_Type_create_struct.newtype != NULL) { \
		args->MPI_Type_create_struct.newtype__ref.val = *args->MPI_Type_create_struct.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_resized` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_resized` function call.
 *
 * @struct args_MPI_Type_create_resized_t
 *
 * @note 
 *	int
 *	MPI_Type_create_resized (
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Aint lb (long)
 *			MPI_Aint extent (long)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_resized
struct args_MPI_Type_create_resized_t {
	MPI_Datatype oldtype;
	MPI_Aint lb;
	MPI_Aint extent;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_resized(activity) { \
	activity->mpi_args.MPI_Type_create_resized.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_create_resized.lb = (MPI_Aint) lb; \
	activity->mpi_args.MPI_Type_create_resized.extent = (MPI_Aint) extent; \
	activity->mpi_args.MPI_Type_create_resized.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_resized(args) { \
	if (args->MPI_Type_create_resized.newtype != NULL) { \
		args->MPI_Type_create_resized.newtype__ref.val = *args->MPI_Type_create_resized.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_commit` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_commit` function call.
 *
 * @struct args_MPI_Type_commit_t
 *
 * @note 
 *	int
 *	MPI_Type_commit (
 *			MPI_Datatype * type (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_commit
struct args_MPI_Type_commit_t {
	MPI_Datatype * type;
	struct {
		MPI_Datatype val;
	} type__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_commit(activity) { \
	activity->mpi_args.MPI_Type_commit.type = (MPI_Datatype *) type; \
};

#define GET_PTRS_VALUE_MPI_Type_commit(args) { \
	if (args->MPI_Type_commit.type != NULL) { \
		args->MPI_Type_commit.type__ref.val = *args->MPI_Type_commit.type; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_free` function call.
 *
 * @struct args_MPI_Type_free_t
 *
 * @note 
 *	int
 *	MPI_Type_free (
 *			MPI_Datatype * type (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_free
struct args_MPI_Type_free_t {
	MPI_Datatype * type;
	struct {
		MPI_Datatype val;
	} type__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_free(activity) { \
	activity->mpi_args.MPI_Type_free.type = (MPI_Datatype *) type; \
};

#define GET_PTRS_VALUE_MPI_Type_free(args) { \
	if (args->MPI_Type_free.type != NULL) { \
		args->MPI_Type_free.type__ref.val = *args->MPI_Type_free.type; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_count` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_count` function call.
 *
 * @struct args_MPI_Get_count_t
 *
 * @note 
 *	int
 *	MPI_Get_count (
 *			const MPI_Status * status (const struct opaque * *)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int * count (int *)
 *	)
 */
#if HAVE_MPI_Get_count
struct args_MPI_Get_count_t {
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	MPI_Datatype datatype;
	int * count;
	struct {
		int val;
	} count__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_count(activity) { \
	activity->mpi_args.MPI_Get_count.status = (MPI_Status *) status; \
	activity->mpi_args.MPI_Get_count.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Get_count.count = (int *) count; \
};

#define GET_PTRS_VALUE_MPI_Get_count(args) { \
	if (args->MPI_Get_count.status != NULL) { \
		args->MPI_Get_count.status__ref.val = *args->MPI_Get_count.status; \
	} \
	if (args->MPI_Get_count.count != NULL) { \
		args->MPI_Get_count.count__ref.val = *args->MPI_Get_count.count; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_elements` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_elements` function call.
 *
 * @struct args_MPI_Get_elements_t
 *
 * @note 
 *	int
 *	MPI_Get_elements (
 *			const MPI_Status * status (const struct opaque * *)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int * count (int *)
 *	)
 */
#if HAVE_MPI_Get_elements
struct args_MPI_Get_elements_t {
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	MPI_Datatype datatype;
	int * count;
	struct {
		int val;
	} count__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_elements(activity) { \
	activity->mpi_args.MPI_Get_elements.status = (MPI_Status *) status; \
	activity->mpi_args.MPI_Get_elements.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Get_elements.count = (int *) count; \
};

#define GET_PTRS_VALUE_MPI_Get_elements(args) { \
	if (args->MPI_Get_elements.status != NULL) { \
		args->MPI_Get_elements.status__ref.val = *args->MPI_Get_elements.status; \
	} \
	if (args->MPI_Get_elements.count != NULL) { \
		args->MPI_Get_elements.count__ref.val = *args->MPI_Get_elements.count; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Pack` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Pack` function call.
 *
 * @struct args_MPI_Pack_t
 *
 * @note 
 *	int
 *	MPI_Pack (
 *			const void * inbuf (const void *)
 *			int incount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			void * outbuf (void *)
 *			int outsize (int)
 *			int * position (int *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Pack
struct args_MPI_Pack_t {
	void * inbuf;
	int incount;
	MPI_Datatype datatype;
	void * outbuf;
	int outsize;
	int * position;
	struct {
		int val;
	} position__ref;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Pack(activity) { \
	activity->mpi_args.MPI_Pack.inbuf = (void *) inbuf; \
	activity->mpi_args.MPI_Pack.incount = (int) incount; \
	activity->mpi_args.MPI_Pack.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Pack.outbuf = (void *) outbuf; \
	activity->mpi_args.MPI_Pack.outsize = (int) outsize; \
	activity->mpi_args.MPI_Pack.position = (int *) position; \
	activity->mpi_args.MPI_Pack.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Pack(args) { \
	if (args->MPI_Pack.position != NULL) { \
		args->MPI_Pack.position__ref.val = *args->MPI_Pack.position; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Unpack` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Unpack` function call.
 *
 * @struct args_MPI_Unpack_t
 *
 * @note 
 *	int
 *	MPI_Unpack (
 *			const void * inbuf (const void *)
 *			int insize (int)
 *			int * position (int *)
 *			void * outbuf (void *)
 *			int outcount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Unpack
struct args_MPI_Unpack_t {
	void * inbuf;
	int insize;
	int * position;
	struct {
		int val;
	} position__ref;
	void * outbuf;
	int outcount;
	MPI_Datatype datatype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Unpack(activity) { \
	activity->mpi_args.MPI_Unpack.inbuf = (void *) inbuf; \
	activity->mpi_args.MPI_Unpack.insize = (int) insize; \
	activity->mpi_args.MPI_Unpack.position = (int *) position; \
	activity->mpi_args.MPI_Unpack.outbuf = (void *) outbuf; \
	activity->mpi_args.MPI_Unpack.outcount = (int) outcount; \
	activity->mpi_args.MPI_Unpack.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Unpack.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Unpack(args) { \
	if (args->MPI_Unpack.position != NULL) { \
		args->MPI_Unpack.position__ref.val = *args->MPI_Unpack.position; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Pack_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Pack_size` function call.
 *
 * @struct args_MPI_Pack_size_t
 *
 * @note 
 *	int
 *	MPI_Pack_size (
 *			int incount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * size (int *)
 *	)
 */
#if HAVE_MPI_Pack_size
struct args_MPI_Pack_size_t {
	int incount;
	MPI_Datatype datatype;
	MPI_Comm comm;
	int * size;
	struct {
		int val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Pack_size(activity) { \
	activity->mpi_args.MPI_Pack_size.incount = (int) incount; \
	activity->mpi_args.MPI_Pack_size.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Pack_size.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Pack_size.size = (int *) size; \
};

#define GET_PTRS_VALUE_MPI_Pack_size(args) { \
	if (args->MPI_Pack_size.size != NULL) { \
		args->MPI_Pack_size.size__ref.val = *args->MPI_Pack_size.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Barrier` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Barrier` function call.
 *
 * @struct args_MPI_Barrier_t
 *
 * @note 
 *	int
 *	MPI_Barrier (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Barrier
struct args_MPI_Barrier_t {
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Barrier(activity) { \
	activity->mpi_args.MPI_Barrier.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Bcast` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Bcast` function call.
 *
 * @struct args_MPI_Bcast_t
 *
 * @note 
 *	int
 *	MPI_Bcast (
 *			void * buffer (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Bcast
struct args_MPI_Bcast_t {
	void * buffer;
	int count;
	MPI_Datatype datatype;
	int root;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Bcast(activity) { \
	activity->mpi_args.MPI_Bcast.buffer = (void *) buffer; \
	activity->mpi_args.MPI_Bcast.count = (int) count; \
	activity->mpi_args.MPI_Bcast.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Bcast.root = (int) root; \
	activity->mpi_args.MPI_Bcast.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Gather` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Gather` function call.
 *
 * @struct args_MPI_Gather_t
 *
 * @note 
 *	int
 *	MPI_Gather (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Gather
struct args_MPI_Gather_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Gather(activity) { \
	activity->mpi_args.MPI_Gather.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Gather.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Gather.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Gather.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Gather.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Gather.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Gather.root = (int) root; \
	activity->mpi_args.MPI_Gather.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Gatherv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Gatherv` function call.
 *
 * @struct args_MPI_Gatherv_t
 *
 * @note 
 *	int
 *	MPI_Gatherv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Gatherv
struct args_MPI_Gatherv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Gatherv(activity) { \
	activity->mpi_args.MPI_Gatherv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Gatherv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Gatherv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Gatherv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Gatherv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Gatherv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Gatherv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Gatherv.root = (int) root; \
	activity->mpi_args.MPI_Gatherv.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Gatherv(args) { \
	if (args->MPI_Gatherv.recvcounts != NULL) { \
		args->MPI_Gatherv.recvcounts__ref.val = *args->MPI_Gatherv.recvcounts; \
	} \
	if (args->MPI_Gatherv.displs != NULL) { \
		args->MPI_Gatherv.displs__ref.val = *args->MPI_Gatherv.displs; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Scatter` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Scatter` function call.
 *
 * @struct args_MPI_Scatter_t
 *
 * @note 
 *	int
 *	MPI_Scatter (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Scatter
struct args_MPI_Scatter_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Scatter(activity) { \
	activity->mpi_args.MPI_Scatter.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Scatter.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Scatter.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Scatter.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Scatter.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Scatter.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Scatter.root = (int) root; \
	activity->mpi_args.MPI_Scatter.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Scatterv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Scatterv` function call.
 *
 * @struct args_MPI_Scatterv_t
 *
 * @note 
 *	int
 *	MPI_Scatterv (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Scatterv
struct args_MPI_Scatterv_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Scatterv(activity) { \
	activity->mpi_args.MPI_Scatterv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Scatterv.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Scatterv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Scatterv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Scatterv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Scatterv.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Scatterv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Scatterv.root = (int) root; \
	activity->mpi_args.MPI_Scatterv.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Scatterv(args) { \
	if (args->MPI_Scatterv.sendcounts != NULL) { \
		args->MPI_Scatterv.sendcounts__ref.val = *args->MPI_Scatterv.sendcounts; \
	} \
	if (args->MPI_Scatterv.displs != NULL) { \
		args->MPI_Scatterv.displs__ref.val = *args->MPI_Scatterv.displs; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Allgather` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Allgather` function call.
 *
 * @struct args_MPI_Allgather_t
 *
 * @note 
 *	int
 *	MPI_Allgather (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Allgather
struct args_MPI_Allgather_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Allgather(activity) { \
	activity->mpi_args.MPI_Allgather.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Allgather.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Allgather.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Allgather.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Allgather.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Allgather.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Allgather.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Allgatherv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Allgatherv` function call.
 *
 * @struct args_MPI_Allgatherv_t
 *
 * @note 
 *	int
 *	MPI_Allgatherv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Allgatherv
struct args_MPI_Allgatherv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Allgatherv(activity) { \
	activity->mpi_args.MPI_Allgatherv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Allgatherv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Allgatherv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Allgatherv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Allgatherv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Allgatherv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Allgatherv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Allgatherv.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Allgatherv(args) { \
	if (args->MPI_Allgatherv.recvcounts != NULL) { \
		args->MPI_Allgatherv.recvcounts__ref.val = *args->MPI_Allgatherv.recvcounts; \
	} \
	if (args->MPI_Allgatherv.displs != NULL) { \
		args->MPI_Allgatherv.displs__ref.val = *args->MPI_Allgatherv.displs; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Alltoall` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Alltoall` function call.
 *
 * @struct args_MPI_Alltoall_t
 *
 * @note 
 *	int
 *	MPI_Alltoall (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Alltoall
struct args_MPI_Alltoall_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Alltoall(activity) { \
	activity->mpi_args.MPI_Alltoall.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Alltoall.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Alltoall.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Alltoall.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Alltoall.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Alltoall.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Alltoall.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Alltoallv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Alltoallv` function call.
 *
 * @struct args_MPI_Alltoallv_t
 *
 * @note 
 *	int
 *	MPI_Alltoallv (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Alltoallv
struct args_MPI_Alltoallv_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Alltoallv(activity) { \
	activity->mpi_args.MPI_Alltoallv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Alltoallv.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Alltoallv.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Alltoallv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Alltoallv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Alltoallv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Alltoallv.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Alltoallv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Alltoallv.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Alltoallv(args) { \
	if (args->MPI_Alltoallv.sendcounts != NULL) { \
		args->MPI_Alltoallv.sendcounts__ref.val = *args->MPI_Alltoallv.sendcounts; \
	} \
	if (args->MPI_Alltoallv.sdispls != NULL) { \
		args->MPI_Alltoallv.sdispls__ref.val = *args->MPI_Alltoallv.sdispls; \
	} \
	if (args->MPI_Alltoallv.recvcounts != NULL) { \
		args->MPI_Alltoallv.recvcounts__ref.val = *args->MPI_Alltoallv.recvcounts; \
	} \
	if (args->MPI_Alltoallv.rdispls != NULL) { \
		args->MPI_Alltoallv.rdispls__ref.val = *args->MPI_Alltoallv.rdispls; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Reduce` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Reduce` function call.
 *
 * @struct args_MPI_Reduce_t
 *
 * @note 
 *	int
 *	MPI_Reduce (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Reduce
struct args_MPI_Reduce_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	int root;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Reduce(activity) { \
	activity->mpi_args.MPI_Reduce.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Reduce.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Reduce.count = (int) count; \
	activity->mpi_args.MPI_Reduce.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Reduce.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Reduce.root = (int) root; \
	activity->mpi_args.MPI_Reduce.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Allreduce` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Allreduce` function call.
 *
 * @struct args_MPI_Allreduce_t
 *
 * @note 
 *	int
 *	MPI_Allreduce (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Allreduce
struct args_MPI_Allreduce_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Allreduce(activity) { \
	activity->mpi_args.MPI_Allreduce.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Allreduce.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Allreduce.count = (int) count; \
	activity->mpi_args.MPI_Allreduce.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Allreduce.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Allreduce.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Reduce_scatter` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Reduce_scatter` function call.
 *
 * @struct args_MPI_Reduce_scatter_t
 *
 * @note 
 *	int
 *	MPI_Reduce_scatter (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Reduce_scatter
struct args_MPI_Reduce_scatter_t {
	void * sendbuf;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Reduce_scatter(activity) { \
	activity->mpi_args.MPI_Reduce_scatter.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Reduce_scatter.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Reduce_scatter.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Reduce_scatter.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Reduce_scatter.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Reduce_scatter.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Reduce_scatter(args) { \
	if (args->MPI_Reduce_scatter.recvcounts != NULL) { \
		args->MPI_Reduce_scatter.recvcounts__ref.val = *args->MPI_Reduce_scatter.recvcounts; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Reduce_scatter_block` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Reduce_scatter_block` function call.
 *
 * @struct args_MPI_Reduce_scatter_block_t
 *
 * @note 
 *	int
 *	MPI_Reduce_scatter_block (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Reduce_scatter_block
struct args_MPI_Reduce_scatter_block_t {
	void * sendbuf;
	void * recvbuf;
	int recvcount;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Reduce_scatter_block(activity) { \
	activity->mpi_args.MPI_Reduce_scatter_block.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Reduce_scatter_block.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Reduce_scatter_block.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Reduce_scatter_block.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Reduce_scatter_block.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Reduce_scatter_block.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Scan` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Scan` function call.
 *
 * @struct args_MPI_Scan_t
 *
 * @note 
 *	int
 *	MPI_Scan (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Scan
struct args_MPI_Scan_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Scan(activity) { \
	activity->mpi_args.MPI_Scan.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Scan.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Scan.count = (int) count; \
	activity->mpi_args.MPI_Scan.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Scan.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Scan.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Exscan` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Exscan` function call.
 *
 * @struct args_MPI_Exscan_t
 *
 * @note 
 *	int
 *	MPI_Exscan (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Exscan
struct args_MPI_Exscan_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Exscan(activity) { \
	activity->mpi_args.MPI_Exscan.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Exscan.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Exscan.count = (int) count; \
	activity->mpi_args.MPI_Exscan.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Exscan.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Exscan.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_size` function call.
 *
 * @struct args_MPI_Comm_size_t
 *
 * @note 
 *	int
 *	MPI_Comm_size (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * size (int *)
 *	)
 */
#if HAVE_MPI_Comm_size
struct args_MPI_Comm_size_t {
	MPI_Comm comm;
	int * size;
	struct {
		int val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_size(activity) { \
	activity->mpi_args.MPI_Comm_size.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_size.size = (int *) size; \
};

#define GET_PTRS_VALUE_MPI_Comm_size(args) { \
	if (args->MPI_Comm_size.size != NULL) { \
		args->MPI_Comm_size.size__ref.val = *args->MPI_Comm_size.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_rank` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_rank` function call.
 *
 * @struct args_MPI_Comm_rank_t
 *
 * @note 
 *	int
 *	MPI_Comm_rank (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * rank (int *)
 *	)
 */
#if HAVE_MPI_Comm_rank
struct args_MPI_Comm_rank_t {
	MPI_Comm comm;
	int * rank;
	struct {
		int val;
	} rank__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_rank(activity) { \
	activity->mpi_args.MPI_Comm_rank.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_rank.rank = (int *) rank; \
};

#define GET_PTRS_VALUE_MPI_Comm_rank(args) { \
	if (args->MPI_Comm_rank.rank != NULL) { \
		args->MPI_Comm_rank.rank__ref.val = *args->MPI_Comm_rank.rank; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_group` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_group` function call.
 *
 * @struct args_MPI_Comm_group_t
 *
 * @note 
 *	int
 *	MPI_Comm_group (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Group * group (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Comm_group
struct args_MPI_Comm_group_t {
	MPI_Comm comm;
	MPI_Group * group;
	struct {
		MPI_Group val;
	} group__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_group(activity) { \
	activity->mpi_args.MPI_Comm_group.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_group.group = (MPI_Group *) group; \
};

#define GET_PTRS_VALUE_MPI_Comm_group(args) { \
	if (args->MPI_Comm_group.group != NULL) { \
		args->MPI_Comm_group.group__ref.val = *args->MPI_Comm_group.group; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_dup` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_dup` function call.
 *
 * @struct args_MPI_Comm_dup_t
 *
 * @note 
 *	int
 *	MPI_Comm_dup (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_dup
struct args_MPI_Comm_dup_t {
	MPI_Comm comm;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_dup(activity) { \
	activity->mpi_args.MPI_Comm_dup.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_dup.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_dup(args) { \
	if (args->MPI_Comm_dup.newcomm != NULL) { \
		args->MPI_Comm_dup.newcomm__ref.val = *args->MPI_Comm_dup.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_create` function call.
 *
 * @struct args_MPI_Comm_create_t
 *
 * @note 
 *	int
 *	MPI_Comm_create (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Group group (struct mpi_group_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_create
struct args_MPI_Comm_create_t {
	MPI_Comm comm;
	MPI_Group group;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_create(activity) { \
	activity->mpi_args.MPI_Comm_create.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_create.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Comm_create.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_create(args) { \
	if (args->MPI_Comm_create.newcomm != NULL) { \
		args->MPI_Comm_create.newcomm__ref.val = *args->MPI_Comm_create.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_split` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_split` function call.
 *
 * @struct args_MPI_Comm_split_t
 *
 * @note 
 *	int
 *	MPI_Comm_split (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int color (int)
 *			int key (int)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_split
struct args_MPI_Comm_split_t {
	MPI_Comm comm;
	int color;
	int key;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_split(activity) { \
	activity->mpi_args.MPI_Comm_split.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_split.color = (int) color; \
	activity->mpi_args.MPI_Comm_split.key = (int) key; \
	activity->mpi_args.MPI_Comm_split.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_split(args) { \
	if (args->MPI_Comm_split.newcomm != NULL) { \
		args->MPI_Comm_split.newcomm__ref.val = *args->MPI_Comm_split.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_free` function call.
 *
 * @struct args_MPI_Comm_free_t
 *
 * @note 
 *	int
 *	MPI_Comm_free (
 *			MPI_Comm * comm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_free
struct args_MPI_Comm_free_t {
	MPI_Comm * comm;
	struct {
		MPI_Comm val;
	} comm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_free(activity) { \
	activity->mpi_args.MPI_Comm_free.comm = (MPI_Comm *) comm; \
};

#define GET_PTRS_VALUE_MPI_Comm_free(args) { \
	if (args->MPI_Comm_free.comm != NULL) { \
		args->MPI_Comm_free.comm__ref.val = *args->MPI_Comm_free.comm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_test_inter` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_test_inter` function call.
 *
 * @struct args_MPI_Comm_test_inter_t
 *
 * @note 
 *	int
 *	MPI_Comm_test_inter (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Comm_test_inter
struct args_MPI_Comm_test_inter_t {
	MPI_Comm comm;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_test_inter(activity) { \
	activity->mpi_args.MPI_Comm_test_inter.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_test_inter.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Comm_test_inter(args) { \
	if (args->MPI_Comm_test_inter.flag != NULL) { \
		args->MPI_Comm_test_inter.flag__ref.val = *args->MPI_Comm_test_inter.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_remote_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_remote_size` function call.
 *
 * @struct args_MPI_Comm_remote_size_t
 *
 * @note 
 *	int
 *	MPI_Comm_remote_size (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * size (int *)
 *	)
 */
#if HAVE_MPI_Comm_remote_size
struct args_MPI_Comm_remote_size_t {
	MPI_Comm comm;
	int * size;
	struct {
		int val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_remote_size(activity) { \
	activity->mpi_args.MPI_Comm_remote_size.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_remote_size.size = (int *) size; \
};

#define GET_PTRS_VALUE_MPI_Comm_remote_size(args) { \
	if (args->MPI_Comm_remote_size.size != NULL) { \
		args->MPI_Comm_remote_size.size__ref.val = *args->MPI_Comm_remote_size.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_remote_group` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_remote_group` function call.
 *
 * @struct args_MPI_Comm_remote_group_t
 *
 * @note 
 *	int
 *	MPI_Comm_remote_group (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Group * group (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Comm_remote_group
struct args_MPI_Comm_remote_group_t {
	MPI_Comm comm;
	MPI_Group * group;
	struct {
		MPI_Group val;
	} group__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_remote_group(activity) { \
	activity->mpi_args.MPI_Comm_remote_group.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_remote_group.group = (MPI_Group *) group; \
};

#define GET_PTRS_VALUE_MPI_Comm_remote_group(args) { \
	if (args->MPI_Comm_remote_group.group != NULL) { \
		args->MPI_Comm_remote_group.group__ref.val = *args->MPI_Comm_remote_group.group; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_compare` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_compare` function call.
 *
 * @struct args_MPI_Comm_compare_t
 *
 * @note 
 *	int
 *	MPI_Comm_compare (
 *			MPI_Comm comm1 (struct mpi_communicator_t *)
 *			MPI_Comm comm2 (struct mpi_communicator_t *)
 *			int * result (int *)
 *	)
 */
#if HAVE_MPI_Comm_compare
struct args_MPI_Comm_compare_t {
	MPI_Comm comm1;
	MPI_Comm comm2;
	int * result;
	struct {
		int val;
	} result__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_compare(activity) { \
	activity->mpi_args.MPI_Comm_compare.comm1 = (MPI_Comm) comm1; \
	activity->mpi_args.MPI_Comm_compare.comm2 = (MPI_Comm) comm2; \
	activity->mpi_args.MPI_Comm_compare.result = (int *) result; \
};

#define GET_PTRS_VALUE_MPI_Comm_compare(args) { \
	if (args->MPI_Comm_compare.result != NULL) { \
		args->MPI_Comm_compare.result__ref.val = *args->MPI_Comm_compare.result; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_create_keyval` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_create_keyval` function call.
 *
 * @struct args_MPI_Comm_create_keyval_t
 *
 * @note 
 *	int
 *	MPI_Comm_create_keyval (
 *			MPI_Comm_copy_attr_function * comm_copy_attr_fn (int (*)(struct mpi_communicator_t *, int, void *, void *, void *, int *))
 *			MPI_Comm_delete_attr_function * comm_delete_attr_fn (int (*)(struct mpi_communicator_t *, int, void *, void *))
 *			int * comm_keyval (int *)
 *			void * extra_state (void *)
 *	)
 */
#if HAVE_MPI_Comm_create_keyval
struct args_MPI_Comm_create_keyval_t {
	MPI_Comm_copy_attr_function * comm_copy_attr_fn;
	MPI_Comm_delete_attr_function * comm_delete_attr_fn;
	int * comm_keyval;
	struct {
		int val;
	} comm_keyval__ref;
	void * extra_state;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_create_keyval(activity) { \
	activity->mpi_args.MPI_Comm_create_keyval.comm_copy_attr_fn = (MPI_Comm_copy_attr_function *) comm_copy_attr_fn; \
	activity->mpi_args.MPI_Comm_create_keyval.comm_delete_attr_fn = (MPI_Comm_delete_attr_function *) comm_delete_attr_fn; \
	activity->mpi_args.MPI_Comm_create_keyval.comm_keyval = (int *) comm_keyval; \
	activity->mpi_args.MPI_Comm_create_keyval.extra_state = (void *) extra_state; \
};

#define GET_PTRS_VALUE_MPI_Comm_create_keyval(args) { \
	if (args->MPI_Comm_create_keyval.comm_keyval != NULL) { \
		args->MPI_Comm_create_keyval.comm_keyval__ref.val = *args->MPI_Comm_create_keyval.comm_keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_set_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_set_attr` function call.
 *
 * @struct args_MPI_Comm_set_attr_t
 *
 * @note 
 *	int
 *	MPI_Comm_set_attr (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int comm_keyval (int)
 *			void * attribute_val (void *)
 *	)
 */
#if HAVE_MPI_Comm_set_attr
struct args_MPI_Comm_set_attr_t {
	MPI_Comm comm;
	int comm_keyval;
	void * attribute_val;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_set_attr(activity) { \
	activity->mpi_args.MPI_Comm_set_attr.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_set_attr.comm_keyval = (int) comm_keyval; \
	activity->mpi_args.MPI_Comm_set_attr.attribute_val = (void *) attribute_val; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_get_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_get_attr` function call.
 *
 * @struct args_MPI_Comm_get_attr_t
 *
 * @note 
 *	int
 *	MPI_Comm_get_attr (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int comm_keyval (int)
 *			void * attribute_val (void *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Comm_get_attr
struct args_MPI_Comm_get_attr_t {
	MPI_Comm comm;
	int comm_keyval;
	void * attribute_val;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_get_attr(activity) { \
	activity->mpi_args.MPI_Comm_get_attr.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_get_attr.comm_keyval = (int) comm_keyval; \
	activity->mpi_args.MPI_Comm_get_attr.attribute_val = (void *) attribute_val; \
	activity->mpi_args.MPI_Comm_get_attr.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Comm_get_attr(args) { \
	if (args->MPI_Comm_get_attr.flag != NULL) { \
		args->MPI_Comm_get_attr.flag__ref.val = *args->MPI_Comm_get_attr.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_delete_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_delete_attr` function call.
 *
 * @struct args_MPI_Comm_delete_attr_t
 *
 * @note 
 *	int
 *	MPI_Comm_delete_attr (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int comm_keyval (int)
 *	)
 */
#if HAVE_MPI_Comm_delete_attr
struct args_MPI_Comm_delete_attr_t {
	MPI_Comm comm;
	int comm_keyval;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_delete_attr(activity) { \
	activity->mpi_args.MPI_Comm_delete_attr.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_delete_attr.comm_keyval = (int) comm_keyval; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_get_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_get_name` function call.
 *
 * @struct args_MPI_Comm_get_name_t
 *
 * @note 
 *	int
 *	MPI_Comm_get_name (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			char * comm_name (char *)
 *			int * resultlen (int *)
 *	)
 */
#if HAVE_MPI_Comm_get_name
struct args_MPI_Comm_get_name_t {
	MPI_Comm comm;
	char * comm_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} comm_name__ref;
	int * resultlen;
	struct {
		int val;
	} resultlen__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_get_name(activity) { \
	activity->mpi_args.MPI_Comm_get_name.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_get_name.comm_name = (char *) comm_name; \
	activity->mpi_args.MPI_Comm_get_name.resultlen = (int *) resultlen; \
};

#define GET_PTRS_VALUE_MPI_Comm_get_name(args) { \
	if (args->MPI_Comm_get_name.comm_name != NULL) { \
		strncpy(args->MPI_Comm_get_name.comm_name__ref.val, args->MPI_Comm_get_name.comm_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Comm_get_name.resultlen != NULL) { \
		args->MPI_Comm_get_name.resultlen__ref.val = *args->MPI_Comm_get_name.resultlen; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_set_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_set_name` function call.
 *
 * @struct args_MPI_Comm_set_name_t
 *
 * @note 
 *	int
 *	MPI_Comm_set_name (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			const char * comm_name (const char *)
 *	)
 */
#if HAVE_MPI_Comm_set_name
struct args_MPI_Comm_set_name_t {
	MPI_Comm comm;
	char * comm_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} comm_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_set_name(activity) { \
	activity->mpi_args.MPI_Comm_set_name.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_set_name.comm_name = (char *) comm_name; \
};

#define GET_PTRS_VALUE_MPI_Comm_set_name(args) { \
	if (args->MPI_Comm_set_name.comm_name != NULL) { \
		strncpy(args->MPI_Comm_set_name.comm_name__ref.val, args->MPI_Comm_set_name.comm_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_size` function call.
 *
 * @struct args_MPI_Group_size_t
 *
 * @note 
 *	int
 *	MPI_Group_size (
 *			MPI_Group group (struct mpi_group_t *)
 *			int * size (int *)
 *	)
 */
#if HAVE_MPI_Group_size
struct args_MPI_Group_size_t {
	MPI_Group group;
	int * size;
	struct {
		int val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_size(activity) { \
	activity->mpi_args.MPI_Group_size.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Group_size.size = (int *) size; \
};

#define GET_PTRS_VALUE_MPI_Group_size(args) { \
	if (args->MPI_Group_size.size != NULL) { \
		args->MPI_Group_size.size__ref.val = *args->MPI_Group_size.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_rank` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_rank` function call.
 *
 * @struct args_MPI_Group_rank_t
 *
 * @note 
 *	int
 *	MPI_Group_rank (
 *			MPI_Group group (struct mpi_group_t *)
 *			int * rank (int *)
 *	)
 */
#if HAVE_MPI_Group_rank
struct args_MPI_Group_rank_t {
	MPI_Group group;
	int * rank;
	struct {
		int val;
	} rank__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_rank(activity) { \
	activity->mpi_args.MPI_Group_rank.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Group_rank.rank = (int *) rank; \
};

#define GET_PTRS_VALUE_MPI_Group_rank(args) { \
	if (args->MPI_Group_rank.rank != NULL) { \
		args->MPI_Group_rank.rank__ref.val = *args->MPI_Group_rank.rank; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_translate_ranks` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_translate_ranks` function call.
 *
 * @struct args_MPI_Group_translate_ranks_t
 *
 * @note 
 *	int
 *	MPI_Group_translate_ranks (
 *			MPI_Group group1 (struct mpi_group_t *)
 *			int n (int)
 *			const int[] ranks1 (const int[])
 *			MPI_Group group2 (struct mpi_group_t *)
 *			int[] ranks2 (int[])
 *	)
 */
#if HAVE_MPI_Group_translate_ranks
struct args_MPI_Group_translate_ranks_t {
	MPI_Group group1;
	int n;
	int(* ranks1);
	struct {
		int val;
	} ranks1__ref;
	MPI_Group group2;
	int(* ranks2);
	struct {
		int val;
	} ranks2__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_translate_ranks(activity) { \
	activity->mpi_args.MPI_Group_translate_ranks.group1 = (MPI_Group) group1; \
	activity->mpi_args.MPI_Group_translate_ranks.n = (int) n; \
	activity->mpi_args.MPI_Group_translate_ranks.ranks1 = (int(*)) ranks1; \
	activity->mpi_args.MPI_Group_translate_ranks.group2 = (MPI_Group) group2; \
	activity->mpi_args.MPI_Group_translate_ranks.ranks2 = (int(*)) ranks2; \
};

#define GET_PTRS_VALUE_MPI_Group_translate_ranks(args) { \
	if (args->MPI_Group_translate_ranks.ranks1 != NULL) { \
		args->MPI_Group_translate_ranks.ranks1__ref.val = *args->MPI_Group_translate_ranks.ranks1; \
	} \
	if (args->MPI_Group_translate_ranks.ranks2 != NULL) { \
		args->MPI_Group_translate_ranks.ranks2__ref.val = *args->MPI_Group_translate_ranks.ranks2; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_compare` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_compare` function call.
 *
 * @struct args_MPI_Group_compare_t
 *
 * @note 
 *	int
 *	MPI_Group_compare (
 *			MPI_Group group1 (struct mpi_group_t *)
 *			MPI_Group group2 (struct mpi_group_t *)
 *			int * result (int *)
 *	)
 */
#if HAVE_MPI_Group_compare
struct args_MPI_Group_compare_t {
	MPI_Group group1;
	MPI_Group group2;
	int * result;
	struct {
		int val;
	} result__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_compare(activity) { \
	activity->mpi_args.MPI_Group_compare.group1 = (MPI_Group) group1; \
	activity->mpi_args.MPI_Group_compare.group2 = (MPI_Group) group2; \
	activity->mpi_args.MPI_Group_compare.result = (int *) result; \
};

#define GET_PTRS_VALUE_MPI_Group_compare(args) { \
	if (args->MPI_Group_compare.result != NULL) { \
		args->MPI_Group_compare.result__ref.val = *args->MPI_Group_compare.result; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_union` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_union` function call.
 *
 * @struct args_MPI_Group_union_t
 *
 * @note 
 *	int
 *	MPI_Group_union (
 *			MPI_Group group1 (struct mpi_group_t *)
 *			MPI_Group group2 (struct mpi_group_t *)
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_union
struct args_MPI_Group_union_t {
	MPI_Group group1;
	MPI_Group group2;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_union(activity) { \
	activity->mpi_args.MPI_Group_union.group1 = (MPI_Group) group1; \
	activity->mpi_args.MPI_Group_union.group2 = (MPI_Group) group2; \
	activity->mpi_args.MPI_Group_union.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_union(args) { \
	if (args->MPI_Group_union.newgroup != NULL) { \
		args->MPI_Group_union.newgroup__ref.val = *args->MPI_Group_union.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_intersection` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_intersection` function call.
 *
 * @struct args_MPI_Group_intersection_t
 *
 * @note 
 *	int
 *	MPI_Group_intersection (
 *			MPI_Group group1 (struct mpi_group_t *)
 *			MPI_Group group2 (struct mpi_group_t *)
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_intersection
struct args_MPI_Group_intersection_t {
	MPI_Group group1;
	MPI_Group group2;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_intersection(activity) { \
	activity->mpi_args.MPI_Group_intersection.group1 = (MPI_Group) group1; \
	activity->mpi_args.MPI_Group_intersection.group2 = (MPI_Group) group2; \
	activity->mpi_args.MPI_Group_intersection.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_intersection(args) { \
	if (args->MPI_Group_intersection.newgroup != NULL) { \
		args->MPI_Group_intersection.newgroup__ref.val = *args->MPI_Group_intersection.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_difference` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_difference` function call.
 *
 * @struct args_MPI_Group_difference_t
 *
 * @note 
 *	int
 *	MPI_Group_difference (
 *			MPI_Group group1 (struct mpi_group_t *)
 *			MPI_Group group2 (struct mpi_group_t *)
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_difference
struct args_MPI_Group_difference_t {
	MPI_Group group1;
	MPI_Group group2;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_difference(activity) { \
	activity->mpi_args.MPI_Group_difference.group1 = (MPI_Group) group1; \
	activity->mpi_args.MPI_Group_difference.group2 = (MPI_Group) group2; \
	activity->mpi_args.MPI_Group_difference.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_difference(args) { \
	if (args->MPI_Group_difference.newgroup != NULL) { \
		args->MPI_Group_difference.newgroup__ref.val = *args->MPI_Group_difference.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_incl` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_incl` function call.
 *
 * @struct args_MPI_Group_incl_t
 *
 * @note 
 *	int
 *	MPI_Group_incl (
 *			MPI_Group group (struct mpi_group_t *)
 *			int n (int)
 *			const int[] ranks (const int[])
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_incl
struct args_MPI_Group_incl_t {
	MPI_Group group;
	int n;
	int(* ranks);
	struct {
		int val;
	} ranks__ref;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_incl(activity) { \
	activity->mpi_args.MPI_Group_incl.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Group_incl.n = (int) n; \
	activity->mpi_args.MPI_Group_incl.ranks = (int(*)) ranks; \
	activity->mpi_args.MPI_Group_incl.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_incl(args) { \
	if (args->MPI_Group_incl.ranks != NULL) { \
		args->MPI_Group_incl.ranks__ref.val = *args->MPI_Group_incl.ranks; \
	} \
	if (args->MPI_Group_incl.newgroup != NULL) { \
		args->MPI_Group_incl.newgroup__ref.val = *args->MPI_Group_incl.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_excl` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_excl` function call.
 *
 * @struct args_MPI_Group_excl_t
 *
 * @note 
 *	int
 *	MPI_Group_excl (
 *			MPI_Group group (struct mpi_group_t *)
 *			int n (int)
 *			const int[] ranks (const int[])
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_excl
struct args_MPI_Group_excl_t {
	MPI_Group group;
	int n;
	int(* ranks);
	struct {
		int val;
	} ranks__ref;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_excl(activity) { \
	activity->mpi_args.MPI_Group_excl.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Group_excl.n = (int) n; \
	activity->mpi_args.MPI_Group_excl.ranks = (int(*)) ranks; \
	activity->mpi_args.MPI_Group_excl.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_excl(args) { \
	if (args->MPI_Group_excl.ranks != NULL) { \
		args->MPI_Group_excl.ranks__ref.val = *args->MPI_Group_excl.ranks; \
	} \
	if (args->MPI_Group_excl.newgroup != NULL) { \
		args->MPI_Group_excl.newgroup__ref.val = *args->MPI_Group_excl.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_range_incl` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_range_incl` function call.
 *
 * @struct args_MPI_Group_range_incl_t
 *
 * @note 
 *	int
 *	MPI_Group_range_incl (
 *			MPI_Group group (struct mpi_group_t *)
 *			int n (int)
 *			int[][3] ranges (int[][3])
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_range_incl
struct args_MPI_Group_range_incl_t {
	MPI_Group group;
	int n;
	int(* ranges)[3];
	struct {
		int val[3];
	} ranges__ref;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_range_incl(activity) { \
	activity->mpi_args.MPI_Group_range_incl.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Group_range_incl.n = (int) n; \
	activity->mpi_args.MPI_Group_range_incl.ranges = (int(*)[3]) ranges; \
	activity->mpi_args.MPI_Group_range_incl.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_range_incl(args) { \
	if (args->MPI_Group_range_incl.ranges != NULL) { \
		memcpy(args->MPI_Group_range_incl.ranges__ref.val, args->MPI_Group_range_incl.ranges, sizeof(args->MPI_Group_range_incl.ranges__ref.val)); \
	} \
	if (args->MPI_Group_range_incl.newgroup != NULL) { \
		args->MPI_Group_range_incl.newgroup__ref.val = *args->MPI_Group_range_incl.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_range_excl` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_range_excl` function call.
 *
 * @struct args_MPI_Group_range_excl_t
 *
 * @note 
 *	int
 *	MPI_Group_range_excl (
 *			MPI_Group group (struct mpi_group_t *)
 *			int n (int)
 *			int[][3] ranges (int[][3])
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_range_excl
struct args_MPI_Group_range_excl_t {
	MPI_Group group;
	int n;
	int(* ranges)[3];
	struct {
		int val[3];
	} ranges__ref;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_range_excl(activity) { \
	activity->mpi_args.MPI_Group_range_excl.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Group_range_excl.n = (int) n; \
	activity->mpi_args.MPI_Group_range_excl.ranges = (int(*)[3]) ranges; \
	activity->mpi_args.MPI_Group_range_excl.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_range_excl(args) { \
	if (args->MPI_Group_range_excl.ranges != NULL) { \
		memcpy(args->MPI_Group_range_excl.ranges__ref.val, args->MPI_Group_range_excl.ranges, sizeof(args->MPI_Group_range_excl.ranges__ref.val)); \
	} \
	if (args->MPI_Group_range_excl.newgroup != NULL) { \
		args->MPI_Group_range_excl.newgroup__ref.val = *args->MPI_Group_range_excl.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_free` function call.
 *
 * @struct args_MPI_Group_free_t
 *
 * @note 
 *	int
 *	MPI_Group_free (
 *			MPI_Group * group (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_free
struct args_MPI_Group_free_t {
	MPI_Group * group;
	struct {
		MPI_Group val;
	} group__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_free(activity) { \
	activity->mpi_args.MPI_Group_free.group = (MPI_Group *) group; \
};

#define GET_PTRS_VALUE_MPI_Group_free(args) { \
	if (args->MPI_Group_free.group != NULL) { \
		args->MPI_Group_free.group__ref.val = *args->MPI_Group_free.group; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Op_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Op_create` function call.
 *
 * @struct args_MPI_Op_create_t
 *
 * @note 
 *	int
 *	MPI_Op_create (
 *			MPI_User_function * function (void (*)(void *, void *, int *, struct mpi_datatype_t * *))
 *			int commute (int)
 *			MPI_Op * op (struct mpi_op_t **)
 *	)
 */
#if HAVE_MPI_Op_create
struct args_MPI_Op_create_t {
	MPI_User_function * function;
	int commute;
	MPI_Op * op;
	struct {
		MPI_Op val;
	} op__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Op_create(activity) { \
	activity->mpi_args.MPI_Op_create.function = (MPI_User_function *) function; \
	activity->mpi_args.MPI_Op_create.commute = (int) commute; \
	activity->mpi_args.MPI_Op_create.op = (MPI_Op *) op; \
};

#define GET_PTRS_VALUE_MPI_Op_create(args) { \
	if (args->MPI_Op_create.op != NULL) { \
		args->MPI_Op_create.op__ref.val = *args->MPI_Op_create.op; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Op_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Op_free` function call.
 *
 * @struct args_MPI_Op_free_t
 *
 * @note 
 *	int
 *	MPI_Op_free (
 *			MPI_Op * op (struct mpi_op_t **)
 *	)
 */
#if HAVE_MPI_Op_free
struct args_MPI_Op_free_t {
	MPI_Op * op;
	struct {
		MPI_Op val;
	} op__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Op_free(activity) { \
	activity->mpi_args.MPI_Op_free.op = (MPI_Op *) op; \
};

#define GET_PTRS_VALUE_MPI_Op_free(args) { \
	if (args->MPI_Op_free.op != NULL) { \
		args->MPI_Op_free.op__ref.val = *args->MPI_Op_free.op; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Wtime` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Wtime` function call.
 *
 * @struct args_MPI_Wtime_t
 *
 * @note 
 *	double
 *	MPI_Wtime (
 *	)
 */
#if HAVE_MPI_Wtime
struct args_MPI_Wtime_t {
	double retval;
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Wtick` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Wtick` function call.
 *
 * @struct args_MPI_Wtick_t
 *
 * @note 
 *	double
 *	MPI_Wtick (
 *	)
 */
#if HAVE_MPI_Wtick
struct args_MPI_Wtick_t {
	double retval;
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_address` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_address` function call.
 *
 * @struct args_MPI_Get_address_t
 *
 * @note 
 *	int
 *	MPI_Get_address (
 *			const void * location (const void *)
 *			MPI_Aint * address (long*)
 *	)
 */
#if HAVE_MPI_Get_address
struct args_MPI_Get_address_t {
	void * location;
	MPI_Aint * address;
	struct {
		MPI_Aint val;
	} address__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_address(activity) { \
	activity->mpi_args.MPI_Get_address.location = (void *) location; \
	activity->mpi_args.MPI_Get_address.address = (MPI_Aint *) address; \
};

#define GET_PTRS_VALUE_MPI_Get_address(args) { \
	if (args->MPI_Get_address.address != NULL) { \
		args->MPI_Get_address.address__ref.val = *args->MPI_Get_address.address; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_elements_x` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_elements_x` function call.
 *
 * @struct args_MPI_Get_elements_x_t
 *
 * @note 
 *	int
 *	MPI_Get_elements_x (
 *			const MPI_Status * status (const struct opaque * *)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Count * count (long long*)
 *	)
 */
#if HAVE_MPI_Get_elements_x
struct args_MPI_Get_elements_x_t {
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	MPI_Datatype datatype;
	MPI_Count * count;
	struct {
		MPI_Count val;
	} count__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_elements_x(activity) { \
	activity->mpi_args.MPI_Get_elements_x.status = (MPI_Status *) status; \
	activity->mpi_args.MPI_Get_elements_x.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Get_elements_x.count = (MPI_Count *) count; \
};

#define GET_PTRS_VALUE_MPI_Get_elements_x(args) { \
	if (args->MPI_Get_elements_x.status != NULL) { \
		args->MPI_Get_elements_x.status__ref.val = *args->MPI_Get_elements_x.status; \
	} \
	if (args->MPI_Get_elements_x.count != NULL) { \
		args->MPI_Get_elements_x.count__ref.val = *args->MPI_Get_elements_x.count; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cart_shift` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cart_shift` function call.
 *
 * @struct args_MPI_Cart_shift_t
 *
 * @note 
 *	int
 *	MPI_Cart_shift (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int direction (int)
 *			int disp (int)
 *			int * rank_source (int *)
 *			int * rank_dest (int *)
 *	)
 */
#if HAVE_MPI_Cart_shift
struct args_MPI_Cart_shift_t {
	MPI_Comm comm;
	int direction;
	int disp;
	int * rank_source;
	struct {
		int val;
	} rank_source__ref;
	int * rank_dest;
	struct {
		int val;
	} rank_dest__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cart_shift(activity) { \
	activity->mpi_args.MPI_Cart_shift.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Cart_shift.direction = (int) direction; \
	activity->mpi_args.MPI_Cart_shift.disp = (int) disp; \
	activity->mpi_args.MPI_Cart_shift.rank_source = (int *) rank_source; \
	activity->mpi_args.MPI_Cart_shift.rank_dest = (int *) rank_dest; \
};

#define GET_PTRS_VALUE_MPI_Cart_shift(args) { \
	if (args->MPI_Cart_shift.rank_source != NULL) { \
		args->MPI_Cart_shift.rank_source__ref.val = *args->MPI_Cart_shift.rank_source; \
	} \
	if (args->MPI_Cart_shift.rank_dest != NULL) { \
		args->MPI_Cart_shift.rank_dest__ref.val = *args->MPI_Cart_shift.rank_dest; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_flush_local_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_flush_local_all` function call.
 *
 * @struct args_MPI_Win_flush_local_all_t
 *
 * @note 
 *	int
 *	MPI_Win_flush_local_all (
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_flush_local_all
struct args_MPI_Win_flush_local_all_t {
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_flush_local_all(activity) { \
	activity->mpi_args.MPI_Win_flush_local_all.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_byte_offset` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_byte_offset` function call.
 *
 * @struct args_MPI_File_get_byte_offset_t
 *
 * @note 
 *	int
 *	MPI_File_get_byte_offset (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			MPI_Offset * disp (long long*)
 *	)
 */
#if HAVE_MPI_File_get_byte_offset
struct args_MPI_File_get_byte_offset_t {
	MPI_File fh;
	MPI_Offset offset;
	MPI_Offset * disp;
	struct {
		MPI_Offset val;
	} disp__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_byte_offset(activity) { \
	activity->mpi_args.MPI_File_get_byte_offset.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_byte_offset.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_get_byte_offset.disp = (MPI_Offset *) disp; \
};

#define GET_PTRS_VALUE_MPI_File_get_byte_offset(args) { \
	if (args->MPI_File_get_byte_offset.disp != NULL) { \
		args->MPI_File_get_byte_offset.disp__ref.val = *args->MPI_File_get_byte_offset.disp; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_get_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_get_info` function call.
 *
 * @struct args_MPI_Win_get_info_t
 *
 * @note 
 *	int
 *	MPI_Win_get_info (
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Info * info_used (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Win_get_info
struct args_MPI_Win_get_info_t {
	MPI_Win win;
	MPI_Info * info_used;
	struct {
		MPI_Info val;
	} info_used__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_get_info(activity) { \
	activity->mpi_args.MPI_Win_get_info.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_get_info.info_used = (MPI_Info *) info_used; \
};

#define GET_PTRS_VALUE_MPI_Win_get_info(args) { \
	if (args->MPI_Win_get_info.info_used != NULL) { \
		args->MPI_Win_get_info.info_used__ref.val = *args->MPI_Win_get_info.info_used; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Rput` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Rput` function call.
 *
 * @struct args_MPI_Rput_t
 *
 * @note 
 *	int
 *	MPI_Rput (
 *			const void * origin_addr (const void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_cout (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Rput
struct args_MPI_Rput_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_cout;
	MPI_Datatype target_datatype;
	MPI_Win win;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Rput(activity) { \
	activity->mpi_args.MPI_Rput.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Rput.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Rput.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Rput.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Rput.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Rput.target_cout = (int) target_cout; \
	activity->mpi_args.MPI_Rput.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Rput.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Rput.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Rput(args) { \
	if (args->MPI_Rput.request != NULL) { \
		args->MPI_Rput.request__ref.val = *args->MPI_Rput.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Dist_graph_neighbors_count` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Dist_graph_neighbors_count` function call.
 *
 * @struct args_MPI_Dist_graph_neighbors_count_t
 *
 * @note 
 *	int
 *	MPI_Dist_graph_neighbors_count (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * inneighbors (int *)
 *			int * outneighbors (int *)
 *			int * weighted (int *)
 *	)
 */
#if HAVE_MPI_Dist_graph_neighbors_count
struct args_MPI_Dist_graph_neighbors_count_t {
	MPI_Comm comm;
	int * inneighbors;
	struct {
		int val;
	} inneighbors__ref;
	int * outneighbors;
	struct {
		int val;
	} outneighbors__ref;
	int * weighted;
	struct {
		int val;
	} weighted__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Dist_graph_neighbors_count(activity) { \
	activity->mpi_args.MPI_Dist_graph_neighbors_count.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Dist_graph_neighbors_count.inneighbors = (int *) inneighbors; \
	activity->mpi_args.MPI_Dist_graph_neighbors_count.outneighbors = (int *) outneighbors; \
	activity->mpi_args.MPI_Dist_graph_neighbors_count.weighted = (int *) weighted; \
};

#define GET_PTRS_VALUE_MPI_Dist_graph_neighbors_count(args) { \
	if (args->MPI_Dist_graph_neighbors_count.inneighbors != NULL) { \
		args->MPI_Dist_graph_neighbors_count.inneighbors__ref.val = *args->MPI_Dist_graph_neighbors_count.inneighbors; \
	} \
	if (args->MPI_Dist_graph_neighbors_count.outneighbors != NULL) { \
		args->MPI_Dist_graph_neighbors_count.outneighbors__ref.val = *args->MPI_Dist_graph_neighbors_count.outneighbors; \
	} \
	if (args->MPI_Dist_graph_neighbors_count.weighted != NULL) { \
		args->MPI_Dist_graph_neighbors_count.weighted__ref.val = *args->MPI_Dist_graph_neighbors_count.weighted; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ireduce` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ireduce` function call.
 *
 * @struct args_MPI_Ireduce_t
 *
 * @note 
 *	int
 *	MPI_Ireduce (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ireduce
struct args_MPI_Ireduce_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	int root;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ireduce(activity) { \
	activity->mpi_args.MPI_Ireduce.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ireduce.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ireduce.count = (int) count; \
	activity->mpi_args.MPI_Ireduce.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Ireduce.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Ireduce.root = (int) root; \
	activity->mpi_args.MPI_Ireduce.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ireduce.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ireduce(args) { \
	if (args->MPI_Ireduce.request != NULL) { \
		args->MPI_Ireduce.request__ref.val = *args->MPI_Ireduce.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Psend_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Psend_init` function call.
 *
 * @struct args_MPI_Psend_init_t
 *
 * @note 
 *	int
 *	MPI_Psend_init (
 *			const void * buf (const void *)
 *			int partitions (int)
 *			MPI_Count count (long long)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Psend_init
struct args_MPI_Psend_init_t {
	void * buf;
	int partitions;
	MPI_Count count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Psend_init(activity) { \
	activity->mpi_args.MPI_Psend_init.buf = (void *) buf; \
	activity->mpi_args.MPI_Psend_init.partitions = (int) partitions; \
	activity->mpi_args.MPI_Psend_init.count = (MPI_Count) count; \
	activity->mpi_args.MPI_Psend_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Psend_init.dest = (int) dest; \
	activity->mpi_args.MPI_Psend_init.tag = (int) tag; \
	activity->mpi_args.MPI_Psend_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Psend_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Psend_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Psend_init(args) { \
	if (args->MPI_Psend_init.request != NULL) { \
		args->MPI_Psend_init.request__ref.val = *args->MPI_Psend_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Reduce_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Reduce_init` function call.
 *
 * @struct args_MPI_Reduce_init_t
 *
 * @note 
 *	int
 *	MPI_Reduce_init (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Reduce_init
struct args_MPI_Reduce_init_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	int root;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Reduce_init(activity) { \
	activity->mpi_args.MPI_Reduce_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Reduce_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Reduce_init.count = (int) count; \
	activity->mpi_args.MPI_Reduce_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Reduce_init.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Reduce_init.root = (int) root; \
	activity->mpi_args.MPI_Reduce_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Reduce_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Reduce_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Reduce_init(args) { \
	if (args->MPI_Reduce_init.request != NULL) { \
		args->MPI_Reduce_init.request__ref.val = *args->MPI_Reduce_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_wait` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_wait` function call.
 *
 * @struct args_MPI_Win_wait_t
 *
 * @note 
 *	int
 *	MPI_Win_wait (
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_wait
struct args_MPI_Win_wait_t {
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_wait(activity) { \
	activity->mpi_args.MPI_Win_wait.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Rsend_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Rsend_init` function call.
 *
 * @struct args_MPI_Rsend_init_t
 *
 * @note 
 *	int
 *	MPI_Rsend_init (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Rsend_init
struct args_MPI_Rsend_init_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Rsend_init(activity) { \
	activity->mpi_args.MPI_Rsend_init.buf = (void *) buf; \
	activity->mpi_args.MPI_Rsend_init.count = (int) count; \
	activity->mpi_args.MPI_Rsend_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Rsend_init.dest = (int) dest; \
	activity->mpi_args.MPI_Rsend_init.tag = (int) tag; \
	activity->mpi_args.MPI_Rsend_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Rsend_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Rsend_init(args) { \
	if (args->MPI_Rsend_init.request != NULL) { \
		args->MPI_Rsend_init.request__ref.val = *args->MPI_Rsend_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_at_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_at_all` function call.
 *
 * @struct args_MPI_File_write_at_all_t
 *
 * @note 
 *	int
 *	MPI_File_write_at_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_at_all
struct args_MPI_File_write_at_all_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_at_all(activity) { \
	activity->mpi_args.MPI_File_write_at_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_at_all.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_write_at_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_at_all.count = (int) count; \
	activity->mpi_args.MPI_File_write_at_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_write_at_all.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_at_all(args) { \
	if (args->MPI_File_write_at_all.status != NULL) { \
		args->MPI_File_write_at_all.status__ref.val = *args->MPI_File_write_at_all.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_ordered_end` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_ordered_end` function call.
 *
 * @struct args_MPI_File_write_ordered_end_t
 *
 * @note 
 *	int
 *	MPI_File_write_ordered_end (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_ordered_end
struct args_MPI_File_write_ordered_end_t {
	MPI_File fh;
	void * buf;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_ordered_end(activity) { \
	activity->mpi_args.MPI_File_write_ordered_end.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_ordered_end.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_ordered_end.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_ordered_end(args) { \
	if (args->MPI_File_write_ordered_end.status != NULL) { \
		args->MPI_File_write_ordered_end.status__ref.val = *args->MPI_File_write_ordered_end.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Errhandler_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Errhandler_free` function call.
 *
 * @struct args_MPI_Errhandler_free_t
 *
 * @note 
 *	int
 *	MPI_Errhandler_free (
 *			MPI_Errhandler * errhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_Errhandler_free
struct args_MPI_Errhandler_free_t {
	MPI_Errhandler * errhandler;
	struct {
		MPI_Errhandler val;
	} errhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Errhandler_free(activity) { \
	activity->mpi_args.MPI_Errhandler_free.errhandler = (MPI_Errhandler *) errhandler; \
};

#define GET_PTRS_VALUE_MPI_Errhandler_free(args) { \
	if (args->MPI_Errhandler_free.errhandler != NULL) { \
		args->MPI_Errhandler_free.errhandler__ref.val = *args->MPI_Errhandler_free.errhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_shared_query` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_shared_query` function call.
 *
 * @struct args_MPI_Win_shared_query_t
 *
 * @note 
 *	int
 *	MPI_Win_shared_query (
 *			MPI_Win win (struct mpi_win_t *)
 *			int rank (int)
 *			MPI_Aint * size (long*)
 *			int * disp_unit (int *)
 *			void * baseptr (void *)
 *	)
 */
#if HAVE_MPI_Win_shared_query
struct args_MPI_Win_shared_query_t {
	MPI_Win win;
	int rank;
	MPI_Aint * size;
	struct {
		MPI_Aint val;
	} size__ref;
	int * disp_unit;
	struct {
		int val;
	} disp_unit__ref;
	void * baseptr;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_shared_query(activity) { \
	activity->mpi_args.MPI_Win_shared_query.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_shared_query.rank = (int) rank; \
	activity->mpi_args.MPI_Win_shared_query.size = (MPI_Aint *) size; \
	activity->mpi_args.MPI_Win_shared_query.disp_unit = (int *) disp_unit; \
	activity->mpi_args.MPI_Win_shared_query.baseptr = (void *) baseptr; \
};

#define GET_PTRS_VALUE_MPI_Win_shared_query(args) { \
	if (args->MPI_Win_shared_query.size != NULL) { \
		args->MPI_Win_shared_query.size__ref.val = *args->MPI_Win_shared_query.size; \
	} \
	if (args->MPI_Win_shared_query.disp_unit != NULL) { \
		args->MPI_Win_shared_query.disp_unit__ref.val = *args->MPI_Win_shared_query.disp_unit; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_lock` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_lock` function call.
 *
 * @struct args_MPI_Win_lock_t
 *
 * @note 
 *	int
 *	MPI_Win_lock (
 *			int lock_type (int)
 *			int rank (int)
 *			int mpi_assert (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_lock
struct args_MPI_Win_lock_t {
	int lock_type;
	int rank;
	int mpi_assert;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_lock(activity) { \
	activity->mpi_args.MPI_Win_lock.lock_type = (int) lock_type; \
	activity->mpi_args.MPI_Win_lock.rank = (int) rank; \
	activity->mpi_args.MPI_Win_lock.mpi_assert = (int) mpi_assert; \
	activity->mpi_args.MPI_Win_lock.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_accumulate` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_accumulate` function call.
 *
 * @struct args_MPI_Get_accumulate_t
 *
 * @note 
 *	int
 *	MPI_Get_accumulate (
 *			const void * origin_addr (const void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			void * result_addr (void *)
 *			int result_count (int)
 *			MPI_Datatype result_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_count (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Get_accumulate
struct args_MPI_Get_accumulate_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	void * result_addr;
	int result_count;
	MPI_Datatype result_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_count;
	MPI_Datatype target_datatype;
	MPI_Op op;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_accumulate(activity) { \
	activity->mpi_args.MPI_Get_accumulate.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Get_accumulate.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Get_accumulate.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Get_accumulate.result_addr = (void *) result_addr; \
	activity->mpi_args.MPI_Get_accumulate.result_count = (int) result_count; \
	activity->mpi_args.MPI_Get_accumulate.result_datatype = (MPI_Datatype) result_datatype; \
	activity->mpi_args.MPI_Get_accumulate.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Get_accumulate.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Get_accumulate.target_count = (int) target_count; \
	activity->mpi_args.MPI_Get_accumulate.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Get_accumulate.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Get_accumulate.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_name` function call.
 *
 * @struct args_MPI_Type_get_name_t
 *
 * @note 
 *	int
 *	MPI_Type_get_name (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			char * type_name (char *)
 *			int * resultlen (int *)
 *	)
 */
#if HAVE_MPI_Type_get_name
struct args_MPI_Type_get_name_t {
	MPI_Datatype type;
	char * type_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} type_name__ref;
	int * resultlen;
	struct {
		int val;
	} resultlen__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_name(activity) { \
	activity->mpi_args.MPI_Type_get_name.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_get_name.type_name = (char *) type_name; \
	activity->mpi_args.MPI_Type_get_name.resultlen = (int *) resultlen; \
};

#define GET_PTRS_VALUE_MPI_Type_get_name(args) { \
	if (args->MPI_Type_get_name.type_name != NULL) { \
		strncpy(args->MPI_Type_get_name.type_name__ref.val, args->MPI_Type_get_name.type_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Type_get_name.resultlen != NULL) { \
		args->MPI_Type_get_name.resultlen__ref.val = *args->MPI_Type_get_name.resultlen; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_atomicity` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_atomicity` function call.
 *
 * @struct args_MPI_File_get_atomicity_t
 *
 * @note 
 *	int
 *	MPI_File_get_atomicity (
 *			MPI_File fh (struct mpi_file_t *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_File_get_atomicity
struct args_MPI_File_get_atomicity_t {
	MPI_File fh;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_atomicity(activity) { \
	activity->mpi_args.MPI_File_get_atomicity.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_atomicity.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_File_get_atomicity(args) { \
	if (args->MPI_File_get_atomicity.flag != NULL) { \
		args->MPI_File_get_atomicity.flag__ref.val = *args->MPI_File_get_atomicity.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_set_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_set_info` function call.
 *
 * @struct args_MPI_Session_set_info_t
 *
 * @note 
 *	int
 *	MPI_Session_set_info (
 *			MPI_Session session (struct mpi_instance_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *	)
 */
#if HAVE_MPI_Session_set_info
struct args_MPI_Session_set_info_t {
	MPI_Session session;
	MPI_Info info;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_set_info(activity) { \
	activity->mpi_args.MPI_Session_set_info.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_set_info.info = (MPI_Info) info; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_from_session_pset` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_from_session_pset` function call.
 *
 * @struct args_MPI_Group_from_session_pset_t
 *
 * @note 
 *	int
 *	MPI_Group_from_session_pset (
 *			MPI_Session session (struct mpi_instance_t *)
 *			const char * pset_name (const char *)
 *			MPI_Group * newgroup (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Group_from_session_pset
struct args_MPI_Group_from_session_pset_t {
	MPI_Session session;
	char * pset_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} pset_name__ref;
	MPI_Group * newgroup;
	struct {
		MPI_Group val;
	} newgroup__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_from_session_pset(activity) { \
	activity->mpi_args.MPI_Group_from_session_pset.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Group_from_session_pset.pset_name = (char *) pset_name; \
	activity->mpi_args.MPI_Group_from_session_pset.newgroup = (MPI_Group *) newgroup; \
};

#define GET_PTRS_VALUE_MPI_Group_from_session_pset(args) { \
	if (args->MPI_Group_from_session_pset.pset_name != NULL) { \
		strncpy(args->MPI_Group_from_session_pset.pset_name__ref.val, args->MPI_Group_from_session_pset.pset_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Group_from_session_pset.newgroup != NULL) { \
		args->MPI_Group_from_session_pset.newgroup__ref.val = *args->MPI_Group_from_session_pset.newgroup; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_idup` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_idup` function call.
 *
 * @struct args_MPI_Comm_idup_t
 *
 * @note 
 *	int
 *	MPI_Comm_idup (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Comm_idup
struct args_MPI_Comm_idup_t {
	MPI_Comm comm;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_idup(activity) { \
	activity->mpi_args.MPI_Comm_idup.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_idup.newcomm = (MPI_Comm *) newcomm; \
	activity->mpi_args.MPI_Comm_idup.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Comm_idup(args) { \
	if (args->MPI_Comm_idup.newcomm != NULL) { \
		args->MPI_Comm_idup.newcomm__ref.val = *args->MPI_Comm_idup.newcomm; \
	} \
	if (args->MPI_Comm_idup.request != NULL) { \
		args->MPI_Comm_idup.request__ref.val = *args->MPI_Comm_idup.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_get_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_get_name` function call.
 *
 * @struct args_MPI_Win_get_name_t
 *
 * @note 
 *	int
 *	MPI_Win_get_name (
 *			MPI_Win win (struct mpi_win_t *)
 *			char * win_name (char *)
 *			int * resultlen (int *)
 *	)
 */
#if HAVE_MPI_Win_get_name
struct args_MPI_Win_get_name_t {
	MPI_Win win;
	char * win_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} win_name__ref;
	int * resultlen;
	struct {
		int val;
	} resultlen__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_get_name(activity) { \
	activity->mpi_args.MPI_Win_get_name.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_get_name.win_name = (char *) win_name; \
	activity->mpi_args.MPI_Win_get_name.resultlen = (int *) resultlen; \
};

#define GET_PTRS_VALUE_MPI_Win_get_name(args) { \
	if (args->MPI_Win_get_name.win_name != NULL) { \
		strncpy(args->MPI_Win_get_name.win_name__ref.val, args->MPI_Win_get_name.win_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Win_get_name.resultlen != NULL) { \
		args->MPI_Win_get_name.resultlen__ref.val = *args->MPI_Win_get_name.resultlen; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Allgatherv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Allgatherv_init` function call.
 *
 * @struct args_MPI_Allgatherv_init_t
 *
 * @note 
 *	int
 *	MPI_Allgatherv_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Allgatherv_init
struct args_MPI_Allgatherv_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Allgatherv_init(activity) { \
	activity->mpi_args.MPI_Allgatherv_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Allgatherv_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Allgatherv_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Allgatherv_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Allgatherv_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Allgatherv_init.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Allgatherv_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Allgatherv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Allgatherv_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Allgatherv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Allgatherv_init(args) { \
	if (args->MPI_Allgatherv_init.recvcounts != NULL) { \
		args->MPI_Allgatherv_init.recvcounts__ref.val = *args->MPI_Allgatherv_init.recvcounts; \
	} \
	if (args->MPI_Allgatherv_init.displs != NULL) { \
		args->MPI_Allgatherv_init.displs__ref.val = *args->MPI_Allgatherv_init.displs; \
	} \
	if (args->MPI_Allgatherv_init.request != NULL) { \
		args->MPI_Allgatherv_init.request__ref.val = *args->MPI_Allgatherv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_dup_with_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_dup_with_info` function call.
 *
 * @struct args_MPI_Comm_dup_with_info_t
 *
 * @note 
 *	int
 *	MPI_Comm_dup_with_info (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_dup_with_info
struct args_MPI_Comm_dup_with_info_t {
	MPI_Comm comm;
	MPI_Info info;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_dup_with_info(activity) { \
	activity->mpi_args.MPI_Comm_dup_with_info.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_dup_with_info.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Comm_dup_with_info.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_dup_with_info(args) { \
	if (args->MPI_Comm_dup_with_info.newcomm != NULL) { \
		args->MPI_Comm_dup_with_info.newcomm__ref.val = *args->MPI_Comm_dup_with_info.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_get_num_psets` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_get_num_psets` function call.
 *
 * @struct args_MPI_Session_get_num_psets_t
 *
 * @note 
 *	int
 *	MPI_Session_get_num_psets (
 *			MPI_Session session (struct mpi_instance_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			int * npset_names (int *)
 *	)
 */
#if HAVE_MPI_Session_get_num_psets
struct args_MPI_Session_get_num_psets_t {
	MPI_Session session;
	MPI_Info info;
	int * npset_names;
	struct {
		int val;
	} npset_names__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_get_num_psets(activity) { \
	activity->mpi_args.MPI_Session_get_num_psets.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_get_num_psets.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Session_get_num_psets.npset_names = (int *) npset_names; \
};

#define GET_PTRS_VALUE_MPI_Session_get_num_psets(args) { \
	if (args->MPI_Session_get_num_psets.npset_names != NULL) { \
		args->MPI_Session_get_num_psets.npset_names__ref.val = *args->MPI_Session_get_num_psets.npset_names; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Igather` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Igather` function call.
 *
 * @struct args_MPI_Igather_t
 *
 * @note 
 *	int
 *	MPI_Igather (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Igather
struct args_MPI_Igather_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Igather(activity) { \
	activity->mpi_args.MPI_Igather.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Igather.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Igather.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Igather.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Igather.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Igather.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Igather.root = (int) root; \
	activity->mpi_args.MPI_Igather.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Igather.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Igather(args) { \
	if (args->MPI_Igather.request != NULL) { \
		args->MPI_Igather.request__ref.val = *args->MPI_Igather.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_at` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_at` function call.
 *
 * @struct args_MPI_File_read_at_t
 *
 * @note 
 *	int
 *	MPI_File_read_at (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_at
struct args_MPI_File_read_at_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_at(activity) { \
	activity->mpi_args.MPI_File_read_at.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_at.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_read_at.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_at.count = (int) count; \
	activity->mpi_args.MPI_File_read_at.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_read_at.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_at(args) { \
	if (args->MPI_File_read_at.status != NULL) { \
		args->MPI_File_read_at.status__ref.val = *args->MPI_File_read_at.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_hvector` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_hvector` function call.
 *
 * @struct args_MPI_Type_create_hvector_t
 *
 * @note 
 *	int
 *	MPI_Type_create_hvector (
 *			int count (int)
 *			int blocklength (int)
 *			MPI_Aint stride (long)
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_hvector
struct args_MPI_Type_create_hvector_t {
	int count;
	int blocklength;
	MPI_Aint stride;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_hvector(activity) { \
	activity->mpi_args.MPI_Type_create_hvector.count = (int) count; \
	activity->mpi_args.MPI_Type_create_hvector.blocklength = (int) blocklength; \
	activity->mpi_args.MPI_Type_create_hvector.stride = (MPI_Aint) stride; \
	activity->mpi_args.MPI_Type_create_hvector.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_create_hvector.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_hvector(args) { \
	if (args->MPI_Type_create_hvector.newtype != NULL) { \
		args->MPI_Type_create_hvector.newtype__ref.val = *args->MPI_Type_create_hvector.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_at_all_begin` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_at_all_begin` function call.
 *
 * @struct args_MPI_File_write_at_all_begin_t
 *
 * @note 
 *	int
 *	MPI_File_write_at_all_begin (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_File_write_at_all_begin
struct args_MPI_File_write_at_all_begin_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_at_all_begin(activity) { \
	activity->mpi_args.MPI_File_write_at_all_begin.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_at_all_begin.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_write_at_all_begin.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_at_all_begin.count = (int) count; \
	activity->mpi_args.MPI_File_write_at_all_begin.datatype = (MPI_Datatype) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Grequest_start` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Grequest_start` function call.
 *
 * @struct args_MPI_Grequest_start_t
 *
 * @note 
 *	int
 *	MPI_Grequest_start (
 *			MPI_Grequest_query_function * query_fn (int (*)(void *, struct opaque * *))
 *			MPI_Grequest_free_function * free_fn (int (*)(void *))
 *			MPI_Grequest_cancel_function * cancel_fn (int (*)(void *, int))
 *			void * extra_state (void *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Grequest_start
struct args_MPI_Grequest_start_t {
	MPI_Grequest_query_function * query_fn;
	MPI_Grequest_free_function * free_fn;
	MPI_Grequest_cancel_function * cancel_fn;
	void * extra_state;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Grequest_start(activity) { \
	activity->mpi_args.MPI_Grequest_start.query_fn = (MPI_Grequest_query_function *) query_fn; \
	activity->mpi_args.MPI_Grequest_start.free_fn = (MPI_Grequest_free_function *) free_fn; \
	activity->mpi_args.MPI_Grequest_start.cancel_fn = (MPI_Grequest_cancel_function *) cancel_fn; \
	activity->mpi_args.MPI_Grequest_start.extra_state = (void *) extra_state; \
	activity->mpi_args.MPI_Grequest_start.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Grequest_start(args) { \
	if (args->MPI_Grequest_start.request != NULL) { \
		args->MPI_Grequest_start.request__ref.val = *args->MPI_Grequest_start.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Bsend_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Bsend_init` function call.
 *
 * @struct args_MPI_Bsend_init_t
 *
 * @note 
 *	int
 *	MPI_Bsend_init (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Bsend_init
struct args_MPI_Bsend_init_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Bsend_init(activity) { \
	activity->mpi_args.MPI_Bsend_init.buf = (void *) buf; \
	activity->mpi_args.MPI_Bsend_init.count = (int) count; \
	activity->mpi_args.MPI_Bsend_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Bsend_init.dest = (int) dest; \
	activity->mpi_args.MPI_Bsend_init.tag = (int) tag; \
	activity->mpi_args.MPI_Bsend_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Bsend_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Bsend_init(args) { \
	if (args->MPI_Bsend_init.request != NULL) { \
		args->MPI_Bsend_init.request__ref.val = *args->MPI_Bsend_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_set_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_set_size` function call.
 *
 * @struct args_MPI_File_set_size_t
 *
 * @note 
 *	int
 *	MPI_File_set_size (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset size (long long)
 *	)
 */
#if HAVE_MPI_File_set_size
struct args_MPI_File_set_size_t {
	MPI_File fh;
	MPI_Offset size;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_set_size(activity) { \
	activity->mpi_args.MPI_File_set_size.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_set_size.size = (MPI_Offset) size; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_set_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_set_name` function call.
 *
 * @struct args_MPI_Type_set_name_t
 *
 * @note 
 *	int
 *	MPI_Type_set_name (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			const char * type_name (const char *)
 *	)
 */
#if HAVE_MPI_Type_set_name
struct args_MPI_Type_set_name_t {
	MPI_Datatype type;
	char * type_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} type_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_set_name(activity) { \
	activity->mpi_args.MPI_Type_set_name.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_set_name.type_name = (char *) type_name; \
};

#define GET_PTRS_VALUE_MPI_Type_set_name(args) { \
	if (args->MPI_Type_set_name.type_name != NULL) { \
		strncpy(args->MPI_Type_set_name.type_name__ref.val, args->MPI_Type_set_name.type_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_split_type` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_split_type` function call.
 *
 * @struct args_MPI_Comm_split_type_t
 *
 * @note 
 *	int
 *	MPI_Comm_split_type (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int split_type (int)
 *			int key (int)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_split_type
struct args_MPI_Comm_split_type_t {
	MPI_Comm comm;
	int split_type;
	int key;
	MPI_Info info;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_split_type(activity) { \
	activity->mpi_args.MPI_Comm_split_type.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_split_type.split_type = (int) split_type; \
	activity->mpi_args.MPI_Comm_split_type.key = (int) key; \
	activity->mpi_args.MPI_Comm_split_type.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Comm_split_type.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_split_type(args) { \
	if (args->MPI_Comm_split_type.newcomm != NULL) { \
		args->MPI_Comm_split_type.newcomm__ref.val = *args->MPI_Comm_split_type.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_at_all_end` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_at_all_end` function call.
 *
 * @struct args_MPI_File_read_at_all_end_t
 *
 * @note 
 *	int
 *	MPI_File_read_at_all_end (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_at_all_end
struct args_MPI_File_read_at_all_end_t {
	MPI_File fh;
	void * buf;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_at_all_end(activity) { \
	activity->mpi_args.MPI_File_read_at_all_end.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_at_all_end.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_at_all_end.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_at_all_end(args) { \
	if (args->MPI_File_read_at_all_end.status != NULL) { \
		args->MPI_File_read_at_all_end.status__ref.val = *args->MPI_File_read_at_all_end.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_all` function call.
 *
 * @struct args_MPI_File_write_all_t
 *
 * @note 
 *	int
 *	MPI_File_write_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_all
struct args_MPI_File_write_all_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_all(activity) { \
	activity->mpi_args.MPI_File_write_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_all.count = (int) count; \
	activity->mpi_args.MPI_File_write_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_write_all.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_all(args) { \
	if (args->MPI_File_write_all.status != NULL) { \
		args->MPI_File_write_all.status__ref.val = *args->MPI_File_write_all.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Improbe` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Improbe` function call.
 *
 * @struct args_MPI_Improbe_t
 *
 * @note 
 *	int
 *	MPI_Improbe (
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * flag (int *)
 *			MPI_Message * message (struct mpi_message_t **)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Improbe
struct args_MPI_Improbe_t {
	int source;
	int tag;
	MPI_Comm comm;
	int * flag;
	struct {
		int val;
	} flag__ref;
	MPI_Message * message;
	struct {
		MPI_Message val;
	} message__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Improbe(activity) { \
	activity->mpi_args.MPI_Improbe.source = (int) source; \
	activity->mpi_args.MPI_Improbe.tag = (int) tag; \
	activity->mpi_args.MPI_Improbe.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Improbe.flag = (int *) flag; \
	activity->mpi_args.MPI_Improbe.message = (MPI_Message *) message; \
	activity->mpi_args.MPI_Improbe.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Improbe(args) { \
	if (args->MPI_Improbe.flag != NULL) { \
		args->MPI_Improbe.flag__ref.val = *args->MPI_Improbe.flag; \
	} \
	if (args->MPI_Improbe.message != NULL) { \
		args->MPI_Improbe.message__ref.val = *args->MPI_Improbe.message; \
	} \
	if (args->MPI_Improbe.status != NULL) { \
		args->MPI_Improbe.status__ref.val = *args->MPI_Improbe.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_get_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_get_info` function call.
 *
 * @struct args_MPI_Comm_get_info_t
 *
 * @note 
 *	int
 *	MPI_Comm_get_info (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info * info_used (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Comm_get_info
struct args_MPI_Comm_get_info_t {
	MPI_Comm comm;
	MPI_Info * info_used;
	struct {
		MPI_Info val;
	} info_used__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_get_info(activity) { \
	activity->mpi_args.MPI_Comm_get_info.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_get_info.info_used = (MPI_Info *) info_used; \
};

#define GET_PTRS_VALUE_MPI_Comm_get_info(args) { \
	if (args->MPI_Comm_get_info.info_used != NULL) { \
		args->MPI_Comm_get_info.info_used__ref.val = *args->MPI_Comm_get_info.info_used; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_all_end` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_all_end` function call.
 *
 * @struct args_MPI_File_read_all_end_t
 *
 * @note 
 *	int
 *	MPI_File_read_all_end (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_all_end
struct args_MPI_File_read_all_end_t {
	MPI_File fh;
	void * buf;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_all_end(activity) { \
	activity->mpi_args.MPI_File_read_all_end.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_all_end.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_all_end.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_all_end(args) { \
	if (args->MPI_File_read_all_end.status != NULL) { \
		args->MPI_File_read_all_end.status__ref.val = *args->MPI_File_read_all_end.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_unlock_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_unlock_all` function call.
 *
 * @struct args_MPI_Win_unlock_all_t
 *
 * @note 
 *	int
 *	MPI_Win_unlock_all (
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_unlock_all
struct args_MPI_Win_unlock_all_t {
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_unlock_all(activity) { \
	activity->mpi_args.MPI_Win_unlock_all.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_f90_integer` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_f90_integer` function call.
 *
 * @struct args_MPI_Type_create_f90_integer_t
 *
 * @note 
 *	int
 *	MPI_Type_create_f90_integer (
 *			int r (int)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_f90_integer
struct args_MPI_Type_create_f90_integer_t {
	int r;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_f90_integer(activity) { \
	activity->mpi_args.MPI_Type_create_f90_integer.r = (int) r; \
	activity->mpi_args.MPI_Type_create_f90_integer.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_f90_integer(args) { \
	if (args->MPI_Type_create_f90_integer.newtype != NULL) { \
		args->MPI_Type_create_f90_integer.newtype__ref.val = *args->MPI_Type_create_f90_integer.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Exscan_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Exscan_init` function call.
 *
 * @struct args_MPI_Exscan_init_t
 *
 * @note 
 *	int
 *	MPI_Exscan_init (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Exscan_init
struct args_MPI_Exscan_init_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Exscan_init(activity) { \
	activity->mpi_args.MPI_Exscan_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Exscan_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Exscan_init.count = (int) count; \
	activity->mpi_args.MPI_Exscan_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Exscan_init.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Exscan_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Exscan_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Exscan_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Exscan_init(args) { \
	if (args->MPI_Exscan_init.request != NULL) { \
		args->MPI_Exscan_init.request__ref.val = *args->MPI_Exscan_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ibsend` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ibsend` function call.
 *
 * @struct args_MPI_Ibsend_t
 *
 * @note 
 *	int
 *	MPI_Ibsend (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ibsend
struct args_MPI_Ibsend_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ibsend(activity) { \
	activity->mpi_args.MPI_Ibsend.buf = (void *) buf; \
	activity->mpi_args.MPI_Ibsend.count = (int) count; \
	activity->mpi_args.MPI_Ibsend.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Ibsend.dest = (int) dest; \
	activity->mpi_args.MPI_Ibsend.tag = (int) tag; \
	activity->mpi_args.MPI_Ibsend.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ibsend.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ibsend(args) { \
	if (args->MPI_Ibsend.request != NULL) { \
		args->MPI_Ibsend.request__ref.val = *args->MPI_Ibsend.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_flush_local` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_flush_local` function call.
 *
 * @struct args_MPI_Win_flush_local_t
 *
 * @note 
 *	int
 *	MPI_Win_flush_local (
 *			int rank (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_flush_local
struct args_MPI_Win_flush_local_t {
	int rank;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_flush_local(activity) { \
	activity->mpi_args.MPI_Win_flush_local.rank = (int) rank; \
	activity->mpi_args.MPI_Win_flush_local.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ialltoallw` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ialltoallw` function call.
 *
 * @struct args_MPI_Ialltoallw_t
 *
 * @note 
 *	int
 *	MPI_Ialltoallw (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			const MPI_Datatype[] sendtypes (const struct mpi_datatype_t *[])
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			const MPI_Datatype[] recvtypes (const struct mpi_datatype_t *[])
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ialltoallw
struct args_MPI_Ialltoallw_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype(* sendtypes);
	struct {
		MPI_Datatype val;
	} sendtypes__ref;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype(* recvtypes);
	struct {
		MPI_Datatype val;
	} recvtypes__ref;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ialltoallw(activity) { \
	activity->mpi_args.MPI_Ialltoallw.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ialltoallw.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Ialltoallw.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Ialltoallw.sendtypes = (MPI_Datatype(*)) sendtypes; \
	activity->mpi_args.MPI_Ialltoallw.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ialltoallw.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Ialltoallw.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Ialltoallw.recvtypes = (MPI_Datatype(*)) recvtypes; \
	activity->mpi_args.MPI_Ialltoallw.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ialltoallw.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ialltoallw(args) { \
	if (args->MPI_Ialltoallw.sendcounts != NULL) { \
		args->MPI_Ialltoallw.sendcounts__ref.val = *args->MPI_Ialltoallw.sendcounts; \
	} \
	if (args->MPI_Ialltoallw.sdispls != NULL) { \
		args->MPI_Ialltoallw.sdispls__ref.val = *args->MPI_Ialltoallw.sdispls; \
	} \
	if (args->MPI_Ialltoallw.sendtypes != NULL) { \
		args->MPI_Ialltoallw.sendtypes__ref.val = *args->MPI_Ialltoallw.sendtypes; \
	} \
	if (args->MPI_Ialltoallw.recvcounts != NULL) { \
		args->MPI_Ialltoallw.recvcounts__ref.val = *args->MPI_Ialltoallw.recvcounts; \
	} \
	if (args->MPI_Ialltoallw.rdispls != NULL) { \
		args->MPI_Ialltoallw.rdispls__ref.val = *args->MPI_Ialltoallw.rdispls; \
	} \
	if (args->MPI_Ialltoallw.recvtypes != NULL) { \
		args->MPI_Ialltoallw.recvtypes__ref.val = *args->MPI_Ialltoallw.recvtypes; \
	} \
	if (args->MPI_Ialltoallw.request != NULL) { \
		args->MPI_Ialltoallw.request__ref.val = *args->MPI_Ialltoallw.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_create_from_group` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_create_from_group` function call.
 *
 * @struct args_MPI_Comm_create_from_group_t
 *
 * @note 
 *	int
 *	MPI_Comm_create_from_group (
 *			MPI_Group group (struct mpi_group_t *)
 *			const char * tag (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_create_from_group
struct args_MPI_Comm_create_from_group_t {
	MPI_Group group;
	char * tag;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} tag__ref;
	MPI_Info info;
	MPI_Errhandler errhandler;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_create_from_group(activity) { \
	activity->mpi_args.MPI_Comm_create_from_group.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Comm_create_from_group.tag = (char *) tag; \
	activity->mpi_args.MPI_Comm_create_from_group.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Comm_create_from_group.errhandler = (MPI_Errhandler) errhandler; \
	activity->mpi_args.MPI_Comm_create_from_group.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_create_from_group(args) { \
	if (args->MPI_Comm_create_from_group.tag != NULL) { \
		strncpy(args->MPI_Comm_create_from_group.tag__ref.val, args->MPI_Comm_create_from_group.tag, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Comm_create_from_group.newcomm != NULL) { \
		args->MPI_Comm_create_from_group.newcomm__ref.val = *args->MPI_Comm_create_from_group.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_contents` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_contents` function call.
 *
 * @struct args_MPI_Type_get_contents_t
 *
 * @note 
 *	int
 *	MPI_Type_get_contents (
 *			MPI_Datatype mtype (struct mpi_datatype_t *)
 *			int max_integers (int)
 *			int max_addresses (int)
 *			int max_datatypes (int)
 *			int[] array_of_integers (int[])
 *			MPI_Aint[] array_of_addresses (long[])
 *			MPI_Datatype[] array_of_datatypes (struct mpi_datatype_t *[])
 *	)
 */
#if HAVE_MPI_Type_get_contents
struct args_MPI_Type_get_contents_t {
	MPI_Datatype mtype;
	int max_integers;
	int max_addresses;
	int max_datatypes;
	int(* array_of_integers);
	struct {
		int val;
	} array_of_integers__ref;
	MPI_Aint(* array_of_addresses);
	struct {
		MPI_Aint val;
	} array_of_addresses__ref;
	MPI_Datatype(* array_of_datatypes);
	struct {
		MPI_Datatype val;
	} array_of_datatypes__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_contents(activity) { \
	activity->mpi_args.MPI_Type_get_contents.mtype = (MPI_Datatype) mtype; \
	activity->mpi_args.MPI_Type_get_contents.max_integers = (int) max_integers; \
	activity->mpi_args.MPI_Type_get_contents.max_addresses = (int) max_addresses; \
	activity->mpi_args.MPI_Type_get_contents.max_datatypes = (int) max_datatypes; \
	activity->mpi_args.MPI_Type_get_contents.array_of_integers = (int(*)) array_of_integers; \
	activity->mpi_args.MPI_Type_get_contents.array_of_addresses = (MPI_Aint(*)) array_of_addresses; \
	activity->mpi_args.MPI_Type_get_contents.array_of_datatypes = (MPI_Datatype(*)) array_of_datatypes; \
};

#define GET_PTRS_VALUE_MPI_Type_get_contents(args) { \
	if (args->MPI_Type_get_contents.array_of_integers != NULL) { \
		args->MPI_Type_get_contents.array_of_integers__ref.val = *args->MPI_Type_get_contents.array_of_integers; \
	} \
	if (args->MPI_Type_get_contents.array_of_addresses != NULL) { \
		args->MPI_Type_get_contents.array_of_addresses__ref.val = *args->MPI_Type_get_contents.array_of_addresses; \
	} \
	if (args->MPI_Type_get_contents.array_of_datatypes != NULL) { \
		args->MPI_Type_get_contents.array_of_datatypes__ref.val = *args->MPI_Type_get_contents.array_of_datatypes; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iwrite_at` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iwrite_at` function call.
 *
 * @struct args_MPI_File_iwrite_at_t
 *
 * @note 
 *	int
 *	MPI_File_iwrite_at (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iwrite_at
struct args_MPI_File_iwrite_at_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iwrite_at(activity) { \
	activity->mpi_args.MPI_File_iwrite_at.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iwrite_at.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_iwrite_at.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iwrite_at.count = (int) count; \
	activity->mpi_args.MPI_File_iwrite_at.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iwrite_at.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iwrite_at(args) { \
	if (args->MPI_File_iwrite_at.request != NULL) { \
		args->MPI_File_iwrite_at.request__ref.val = *args->MPI_File_iwrite_at.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_set_elements` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_set_elements` function call.
 *
 * @struct args_MPI_Status_set_elements_t
 *
 * @note 
 *	int
 *	MPI_Status_set_elements (
 *			MPI_Status * status (struct opaque **)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int count (int)
 *	)
 */
#if HAVE_MPI_Status_set_elements
struct args_MPI_Status_set_elements_t {
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	MPI_Datatype datatype;
	int count;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_set_elements(activity) { \
	activity->mpi_args.MPI_Status_set_elements.status = (MPI_Status *) status; \
	activity->mpi_args.MPI_Status_set_elements.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Status_set_elements.count = (int) count; \
};

#define GET_PTRS_VALUE_MPI_Status_set_elements(args) { \
	if (args->MPI_Status_set_elements.status != NULL) { \
		args->MPI_Status_set_elements.status__ref.val = *args->MPI_Status_set_elements.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_ordered` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_ordered` function call.
 *
 * @struct args_MPI_File_read_ordered_t
 *
 * @note 
 *	int
 *	MPI_File_read_ordered (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_ordered
struct args_MPI_File_read_ordered_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_ordered(activity) { \
	activity->mpi_args.MPI_File_read_ordered.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_ordered.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_ordered.count = (int) count; \
	activity->mpi_args.MPI_File_read_ordered.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_read_ordered.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_ordered(args) { \
	if (args->MPI_File_read_ordered.status != NULL) { \
		args->MPI_File_read_ordered.status__ref.val = *args->MPI_File_read_ordered.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Is_thread_main` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Is_thread_main` function call.
 *
 * @struct args_MPI_Is_thread_main_t
 *
 * @note 
 *	int
 *	MPI_Is_thread_main (
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Is_thread_main
struct args_MPI_Is_thread_main_t {
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Is_thread_main(activity) { \
	activity->mpi_args.MPI_Is_thread_main.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Is_thread_main(args) { \
	if (args->MPI_Is_thread_main.flag != NULL) { \
		args->MPI_Is_thread_main.flag__ref.val = *args->MPI_Is_thread_main.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Allreduce_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Allreduce_init` function call.
 *
 * @struct args_MPI_Allreduce_init_t
 *
 * @note 
 *	int
 *	MPI_Allreduce_init (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Allreduce_init
struct args_MPI_Allreduce_init_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Allreduce_init(activity) { \
	activity->mpi_args.MPI_Allreduce_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Allreduce_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Allreduce_init.count = (int) count; \
	activity->mpi_args.MPI_Allreduce_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Allreduce_init.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Allreduce_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Allreduce_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Allreduce_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Allreduce_init(args) { \
	if (args->MPI_Allreduce_init.request != NULL) { \
		args->MPI_Allreduce_init.request__ref.val = *args->MPI_Allreduce_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_get_valuelen` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_get_valuelen` function call.
 *
 * @struct args_MPI_Info_get_valuelen_t
 *
 * @note 
 *	int
 *	MPI_Info_get_valuelen (
 *			MPI_Info info (struct mpi_info_t *)
 *			const char * key (const char *)
 *			int * valuelen (int *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Info_get_valuelen
struct args_MPI_Info_get_valuelen_t {
	MPI_Info info;
	char * key;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} key__ref;
	int * valuelen;
	struct {
		int val;
	} valuelen__ref;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_get_valuelen(activity) { \
	activity->mpi_args.MPI_Info_get_valuelen.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_get_valuelen.key = (char *) key; \
	activity->mpi_args.MPI_Info_get_valuelen.valuelen = (int *) valuelen; \
	activity->mpi_args.MPI_Info_get_valuelen.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Info_get_valuelen(args) { \
	if (args->MPI_Info_get_valuelen.key != NULL) { \
		strncpy(args->MPI_Info_get_valuelen.key__ref.val, args->MPI_Info_get_valuelen.key, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Info_get_valuelen.valuelen != NULL) { \
		args->MPI_Info_get_valuelen.valuelen__ref.val = *args->MPI_Info_get_valuelen.valuelen; \
	} \
	if (args->MPI_Info_get_valuelen.flag != NULL) { \
		args->MPI_Info_get_valuelen.flag__ref.val = *args->MPI_Info_get_valuelen.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_create_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_create_errhandler` function call.
 *
 * @struct args_MPI_Comm_create_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Comm_create_errhandler (
 *			MPI_Comm_errhandler_function * function (void (*)(struct mpi_communicator_t * *, int *, ...))
 *			MPI_Errhandler * errhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_Comm_create_errhandler
struct args_MPI_Comm_create_errhandler_t {
	MPI_Comm_errhandler_function * function;
	MPI_Errhandler * errhandler;
	struct {
		MPI_Errhandler val;
	} errhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_create_errhandler(activity) { \
	activity->mpi_args.MPI_Comm_create_errhandler.function = (MPI_Comm_errhandler_function *) function; \
	activity->mpi_args.MPI_Comm_create_errhandler.errhandler = (MPI_Errhandler *) errhandler; \
};

#define GET_PTRS_VALUE_MPI_Comm_create_errhandler(args) { \
	if (args->MPI_Comm_create_errhandler.errhandler != NULL) { \
		args->MPI_Comm_create_errhandler.errhandler__ref.val = *args->MPI_Comm_create_errhandler.errhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_free` function call.
 *
 * @struct args_MPI_Info_free_t
 *
 * @note 
 *	int
 *	MPI_Info_free (
 *			MPI_Info * info (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Info_free
struct args_MPI_Info_free_t {
	MPI_Info * info;
	struct {
		MPI_Info val;
	} info__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_free(activity) { \
	activity->mpi_args.MPI_Info_free.info = (MPI_Info *) info; \
};

#define GET_PTRS_VALUE_MPI_Info_free(args) { \
	if (args->MPI_Info_free.info != NULL) { \
		args->MPI_Info_free.info__ref.val = *args->MPI_Info_free.info; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_get_nthkey` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_get_nthkey` function call.
 *
 * @struct args_MPI_Info_get_nthkey_t
 *
 * @note 
 *	int
 *	MPI_Info_get_nthkey (
 *			MPI_Info info (struct mpi_info_t *)
 *			int n (int)
 *			char * key (char *)
 *	)
 */
#if HAVE_MPI_Info_get_nthkey
struct args_MPI_Info_get_nthkey_t {
	MPI_Info info;
	int n;
	char * key;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} key__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_get_nthkey(activity) { \
	activity->mpi_args.MPI_Info_get_nthkey.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_get_nthkey.n = (int) n; \
	activity->mpi_args.MPI_Info_get_nthkey.key = (char *) key; \
};

#define GET_PTRS_VALUE_MPI_Info_get_nthkey(args) { \
	if (args->MPI_Info_get_nthkey.key != NULL) { \
		strncpy(args->MPI_Info_get_nthkey.key__ref.val, args->MPI_Info_get_nthkey.key, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ssend_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ssend_init` function call.
 *
 * @struct args_MPI_Ssend_init_t
 *
 * @note 
 *	int
 *	MPI_Ssend_init (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ssend_init
struct args_MPI_Ssend_init_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ssend_init(activity) { \
	activity->mpi_args.MPI_Ssend_init.buf = (void *) buf; \
	activity->mpi_args.MPI_Ssend_init.count = (int) count; \
	activity->mpi_args.MPI_Ssend_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Ssend_init.dest = (int) dest; \
	activity->mpi_args.MPI_Ssend_init.tag = (int) tag; \
	activity->mpi_args.MPI_Ssend_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ssend_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ssend_init(args) { \
	if (args->MPI_Ssend_init.request != NULL) { \
		args->MPI_Ssend_init.request__ref.val = *args->MPI_Ssend_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_set_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_set_info` function call.
 *
 * @struct args_MPI_Comm_set_info_t
 *
 * @note 
 *	int
 *	MPI_Comm_set_info (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *	)
 */
#if HAVE_MPI_Comm_set_info
struct args_MPI_Comm_set_info_t {
	MPI_Comm comm;
	MPI_Info info;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_set_info(activity) { \
	activity->mpi_args.MPI_Comm_set_info.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_set_info.info = (MPI_Info) info; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cart_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cart_create` function call.
 *
 * @struct args_MPI_Cart_create_t
 *
 * @note 
 *	int
 *	MPI_Cart_create (
 *			MPI_Comm old_comm (struct mpi_communicator_t *)
 *			int ndims (int)
 *			const int[] dims (const int[])
 *			const int[] periods (const int[])
 *			int reorder (int)
 *			MPI_Comm * comm_cart (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Cart_create
struct args_MPI_Cart_create_t {
	MPI_Comm old_comm;
	int ndims;
	int(* dims);
	struct {
		int val;
	} dims__ref;
	int(* periods);
	struct {
		int val;
	} periods__ref;
	int reorder;
	MPI_Comm * comm_cart;
	struct {
		MPI_Comm val;
	} comm_cart__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cart_create(activity) { \
	activity->mpi_args.MPI_Cart_create.old_comm = (MPI_Comm) old_comm; \
	activity->mpi_args.MPI_Cart_create.ndims = (int) ndims; \
	activity->mpi_args.MPI_Cart_create.dims = (int(*)) dims; \
	activity->mpi_args.MPI_Cart_create.periods = (int(*)) periods; \
	activity->mpi_args.MPI_Cart_create.reorder = (int) reorder; \
	activity->mpi_args.MPI_Cart_create.comm_cart = (MPI_Comm *) comm_cart; \
};

#define GET_PTRS_VALUE_MPI_Cart_create(args) { \
	if (args->MPI_Cart_create.dims != NULL) { \
		args->MPI_Cart_create.dims__ref.val = *args->MPI_Cart_create.dims; \
	} \
	if (args->MPI_Cart_create.periods != NULL) { \
		args->MPI_Cart_create.periods__ref.val = *args->MPI_Cart_create.periods; \
	} \
	if (args->MPI_Cart_create.comm_cart != NULL) { \
		args->MPI_Cart_create.comm_cart__ref.val = *args->MPI_Cart_create.comm_cart; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_all_begin` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_all_begin` function call.
 *
 * @struct args_MPI_File_write_all_begin_t
 *
 * @note 
 *	int
 *	MPI_File_write_all_begin (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_File_write_all_begin
struct args_MPI_File_write_all_begin_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_all_begin(activity) { \
	activity->mpi_args.MPI_File_write_all_begin.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_all_begin.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_all_begin.count = (int) count; \
	activity->mpi_args.MPI_File_write_all_begin.datatype = (MPI_Datatype) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Scan_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Scan_init` function call.
 *
 * @struct args_MPI_Scan_init_t
 *
 * @note 
 *	int
 *	MPI_Scan_init (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Scan_init
struct args_MPI_Scan_init_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Scan_init(activity) { \
	activity->mpi_args.MPI_Scan_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Scan_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Scan_init.count = (int) count; \
	activity->mpi_args.MPI_Scan_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Scan_init.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Scan_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Scan_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Scan_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Scan_init(args) { \
	if (args->MPI_Scan_init.request != NULL) { \
		args->MPI_Scan_init.request__ref.val = *args->MPI_Scan_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Irsend` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Irsend` function call.
 *
 * @struct args_MPI_Irsend_t
 *
 * @note 
 *	int
 *	MPI_Irsend (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Irsend
struct args_MPI_Irsend_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Irsend(activity) { \
	activity->mpi_args.MPI_Irsend.buf = (void *) buf; \
	activity->mpi_args.MPI_Irsend.count = (int) count; \
	activity->mpi_args.MPI_Irsend.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Irsend.dest = (int) dest; \
	activity->mpi_args.MPI_Irsend.tag = (int) tag; \
	activity->mpi_args.MPI_Irsend.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Irsend.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Irsend(args) { \
	if (args->MPI_Irsend.request != NULL) { \
		args->MPI_Irsend.request__ref.val = *args->MPI_Irsend.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_alltoallv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_alltoallv` function call.
 *
 * @struct args_MPI_Neighbor_alltoallv_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_alltoallv (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Neighbor_alltoallv
struct args_MPI_Neighbor_alltoallv_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_alltoallv(activity) { \
	activity->mpi_args.MPI_Neighbor_alltoallv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallv.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallv.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_alltoallv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallv.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_alltoallv.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_alltoallv(args) { \
	if (args->MPI_Neighbor_alltoallv.sendcounts != NULL) { \
		args->MPI_Neighbor_alltoallv.sendcounts__ref.val = *args->MPI_Neighbor_alltoallv.sendcounts; \
	} \
	if (args->MPI_Neighbor_alltoallv.sdispls != NULL) { \
		args->MPI_Neighbor_alltoallv.sdispls__ref.val = *args->MPI_Neighbor_alltoallv.sdispls; \
	} \
	if (args->MPI_Neighbor_alltoallv.recvcounts != NULL) { \
		args->MPI_Neighbor_alltoallv.recvcounts__ref.val = *args->MPI_Neighbor_alltoallv.recvcounts; \
	} \
	if (args->MPI_Neighbor_alltoallv.rdispls != NULL) { \
		args->MPI_Neighbor_alltoallv.rdispls__ref.val = *args->MPI_Neighbor_alltoallv.rdispls; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Pready_list` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Pready_list` function call.
 *
 * @struct args_MPI_Pready_list_t
 *
 * @note 
 *	int
 *	MPI_Pready_list (
 *			int length (int)
 *			int[] partition_list (int[])
 *			MPI_Request request (struct mpi_request_t *)
 *	)
 */
#if HAVE_MPI_Pready_list
struct args_MPI_Pready_list_t {
	int length;
	int(* partition_list);
	struct {
		int val;
	} partition_list__ref;
	MPI_Request request;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Pready_list(activity) { \
	activity->mpi_args.MPI_Pready_list.length = (int) length; \
	activity->mpi_args.MPI_Pready_list.partition_list = (int(*)) partition_list; \
	activity->mpi_args.MPI_Pready_list.request = (MPI_Request) request; \
};

#define GET_PTRS_VALUE_MPI_Pready_list(args) { \
	if (args->MPI_Pready_list.partition_list != NULL) { \
		args->MPI_Pready_list.partition_list__ref.val = *args->MPI_Pready_list.partition_list; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Alltoallw_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Alltoallw_init` function call.
 *
 * @struct args_MPI_Alltoallw_init_t
 *
 * @note 
 *	int
 *	MPI_Alltoallw_init (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			const MPI_Datatype[] sendtypes (const struct mpi_datatype_t *[])
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			const MPI_Datatype[] recvtypes (const struct mpi_datatype_t *[])
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Alltoallw_init
struct args_MPI_Alltoallw_init_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype(* sendtypes);
	struct {
		MPI_Datatype val;
	} sendtypes__ref;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype(* recvtypes);
	struct {
		MPI_Datatype val;
	} recvtypes__ref;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Alltoallw_init(activity) { \
	activity->mpi_args.MPI_Alltoallw_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Alltoallw_init.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Alltoallw_init.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Alltoallw_init.sendtypes = (MPI_Datatype(*)) sendtypes; \
	activity->mpi_args.MPI_Alltoallw_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Alltoallw_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Alltoallw_init.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Alltoallw_init.recvtypes = (MPI_Datatype(*)) recvtypes; \
	activity->mpi_args.MPI_Alltoallw_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Alltoallw_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Alltoallw_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Alltoallw_init(args) { \
	if (args->MPI_Alltoallw_init.sendcounts != NULL) { \
		args->MPI_Alltoallw_init.sendcounts__ref.val = *args->MPI_Alltoallw_init.sendcounts; \
	} \
	if (args->MPI_Alltoallw_init.sdispls != NULL) { \
		args->MPI_Alltoallw_init.sdispls__ref.val = *args->MPI_Alltoallw_init.sdispls; \
	} \
	if (args->MPI_Alltoallw_init.sendtypes != NULL) { \
		args->MPI_Alltoallw_init.sendtypes__ref.val = *args->MPI_Alltoallw_init.sendtypes; \
	} \
	if (args->MPI_Alltoallw_init.recvcounts != NULL) { \
		args->MPI_Alltoallw_init.recvcounts__ref.val = *args->MPI_Alltoallw_init.recvcounts; \
	} \
	if (args->MPI_Alltoallw_init.rdispls != NULL) { \
		args->MPI_Alltoallw_init.rdispls__ref.val = *args->MPI_Alltoallw_init.rdispls; \
	} \
	if (args->MPI_Alltoallw_init.recvtypes != NULL) { \
		args->MPI_Alltoallw_init.recvtypes__ref.val = *args->MPI_Alltoallw_init.recvtypes; \
	} \
	if (args->MPI_Alltoallw_init.request != NULL) { \
		args->MPI_Alltoallw_init.request__ref.val = *args->MPI_Alltoallw_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_ordered_begin` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_ordered_begin` function call.
 *
 * @struct args_MPI_File_read_ordered_begin_t
 *
 * @note 
 *	int
 *	MPI_File_read_ordered_begin (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_File_read_ordered_begin
struct args_MPI_File_read_ordered_begin_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_ordered_begin(activity) { \
	activity->mpi_args.MPI_File_read_ordered_begin.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_ordered_begin.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_ordered_begin.count = (int) count; \
	activity->mpi_args.MPI_File_read_ordered_begin.datatype = (MPI_Datatype) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Dist_graph_create_adjacent` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Dist_graph_create_adjacent` function call.
 *
 * @struct args_MPI_Dist_graph_create_adjacent_t
 *
 * @note 
 *	int
 *	MPI_Dist_graph_create_adjacent (
 *			MPI_Comm comm_old (struct mpi_communicator_t *)
 *			int indegree (int)
 *			const int[] sources (const int[])
 *			const int[] sourceweights (const int[])
 *			int outdegree (int)
 *			const int[] destinations (const int[])
 *			const int[] destweights (const int[])
 *			MPI_Info info (struct mpi_info_t *)
 *			int reorder (int)
 *			MPI_Comm * comm_dist_graph (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Dist_graph_create_adjacent
struct args_MPI_Dist_graph_create_adjacent_t {
	MPI_Comm comm_old;
	int indegree;
	int(* sources);
	struct {
		int val;
	} sources__ref;
	int(* sourceweights);
	struct {
		int val;
	} sourceweights__ref;
	int outdegree;
	int(* destinations);
	struct {
		int val;
	} destinations__ref;
	int(* destweights);
	struct {
		int val;
	} destweights__ref;
	MPI_Info info;
	int reorder;
	MPI_Comm * comm_dist_graph;
	struct {
		MPI_Comm val;
	} comm_dist_graph__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Dist_graph_create_adjacent(activity) { \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.comm_old = (MPI_Comm) comm_old; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.indegree = (int) indegree; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.sources = (int(*)) sources; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.sourceweights = (int(*)) sourceweights; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.outdegree = (int) outdegree; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.destinations = (int(*)) destinations; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.destweights = (int(*)) destweights; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.reorder = (int) reorder; \
	activity->mpi_args.MPI_Dist_graph_create_adjacent.comm_dist_graph = (MPI_Comm *) comm_dist_graph; \
};

#define GET_PTRS_VALUE_MPI_Dist_graph_create_adjacent(args) { \
	if (args->MPI_Dist_graph_create_adjacent.sources != NULL) { \
		args->MPI_Dist_graph_create_adjacent.sources__ref.val = *args->MPI_Dist_graph_create_adjacent.sources; \
	} \
	if (args->MPI_Dist_graph_create_adjacent.sourceweights != NULL) { \
		args->MPI_Dist_graph_create_adjacent.sourceweights__ref.val = *args->MPI_Dist_graph_create_adjacent.sourceweights; \
	} \
	if (args->MPI_Dist_graph_create_adjacent.destinations != NULL) { \
		args->MPI_Dist_graph_create_adjacent.destinations__ref.val = *args->MPI_Dist_graph_create_adjacent.destinations; \
	} \
	if (args->MPI_Dist_graph_create_adjacent.destweights != NULL) { \
		args->MPI_Dist_graph_create_adjacent.destweights__ref.val = *args->MPI_Dist_graph_create_adjacent.destweights; \
	} \
	if (args->MPI_Dist_graph_create_adjacent.comm_dist_graph != NULL) { \
		args->MPI_Dist_graph_create_adjacent.comm_dist_graph__ref.val = *args->MPI_Dist_graph_create_adjacent.comm_dist_graph; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Reduce_scatter_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Reduce_scatter_init` function call.
 *
 * @struct args_MPI_Reduce_scatter_init_t
 *
 * @note 
 *	int
 *	MPI_Reduce_scatter_init (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Reduce_scatter_init
struct args_MPI_Reduce_scatter_init_t {
	void * sendbuf;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Reduce_scatter_init(activity) { \
	activity->mpi_args.MPI_Reduce_scatter_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Reduce_scatter_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Reduce_scatter_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Reduce_scatter_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Reduce_scatter_init.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Reduce_scatter_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Reduce_scatter_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Reduce_scatter_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Reduce_scatter_init(args) { \
	if (args->MPI_Reduce_scatter_init.recvcounts != NULL) { \
		args->MPI_Reduce_scatter_init.recvcounts__ref.val = *args->MPI_Reduce_scatter_init.recvcounts; \
	} \
	if (args->MPI_Reduce_scatter_init.request != NULL) { \
		args->MPI_Reduce_scatter_init.request__ref.val = *args->MPI_Reduce_scatter_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_get_parent` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_get_parent` function call.
 *
 * @struct args_MPI_Comm_get_parent_t
 *
 * @note 
 *	int
 *	MPI_Comm_get_parent (
 *			MPI_Comm * parent (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_get_parent
struct args_MPI_Comm_get_parent_t {
	MPI_Comm * parent;
	struct {
		MPI_Comm val;
	} parent__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_get_parent(activity) { \
	activity->mpi_args.MPI_Comm_get_parent.parent = (MPI_Comm *) parent; \
};

#define GET_PTRS_VALUE_MPI_Comm_get_parent(args) { \
	if (args->MPI_Comm_get_parent.parent != NULL) { \
		args->MPI_Comm_get_parent.parent__ref.val = *args->MPI_Comm_get_parent.parent; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Keyval_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Keyval_free` function call.
 *
 * @struct args_MPI_Keyval_free_t
 *
 * @note 
 *	int
 *	MPI_Keyval_free (
 *			int * keyval (int *)
 *	)
 */
#if HAVE_MPI_Keyval_free
struct args_MPI_Keyval_free_t {
	int * keyval;
	struct {
		int val;
	} keyval__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Keyval_free(activity) { \
	activity->mpi_args.MPI_Keyval_free.keyval = (int *) keyval; \
};

#define GET_PTRS_VALUE_MPI_Keyval_free(args) { \
	if (args->MPI_Keyval_free.keyval != NULL) { \
		args->MPI_Keyval_free.keyval__ref.val = *args->MPI_Keyval_free.keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_set` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_set` function call.
 *
 * @struct args_MPI_Info_set_t
 *
 * @note 
 *	int
 *	MPI_Info_set (
 *			MPI_Info info (struct mpi_info_t *)
 *			const char * key (const char *)
 *			const char * value (const char *)
 *	)
 */
#if HAVE_MPI_Info_set
struct args_MPI_Info_set_t {
	MPI_Info info;
	char * key;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} key__ref;
	char * value;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} value__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_set(activity) { \
	activity->mpi_args.MPI_Info_set.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_set.key = (char *) key; \
	activity->mpi_args.MPI_Info_set.value = (char *) value; \
};

#define GET_PTRS_VALUE_MPI_Info_set(args) { \
	if (args->MPI_Info_set.key != NULL) { \
		strncpy(args->MPI_Info_set.key__ref.val, args->MPI_Info_set.key, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Info_set.value != NULL) { \
		strncpy(args->MPI_Info_set.value__ref.val, args->MPI_Info_set.value, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Keyval_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Keyval_create` function call.
 *
 * @struct args_MPI_Keyval_create_t
 *
 * @note 
 *	int
 *	MPI_Keyval_create (
 *			MPI_Copy_function * copy_fn (int (*)(struct mpi_communicator_t *, int, void *, void *, void *, int *))
 *			MPI_Delete_function * delete_fn (int (*)(struct mpi_communicator_t *, int, void *, void *))
 *			int * keyval (int *)
 *			void * extra_state (void *)
 *	)
 */
#if HAVE_MPI_Keyval_create
struct args_MPI_Keyval_create_t {
	MPI_Copy_function * copy_fn;
	MPI_Delete_function * delete_fn;
	int * keyval;
	struct {
		int val;
	} keyval__ref;
	void * extra_state;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Keyval_create(activity) { \
	activity->mpi_args.MPI_Keyval_create.copy_fn = (MPI_Copy_function *) copy_fn; \
	activity->mpi_args.MPI_Keyval_create.delete_fn = (MPI_Delete_function *) delete_fn; \
	activity->mpi_args.MPI_Keyval_create.keyval = (int *) keyval; \
	activity->mpi_args.MPI_Keyval_create.extra_state = (void *) extra_state; \
};

#define GET_PTRS_VALUE_MPI_Keyval_create(args) { \
	if (args->MPI_Keyval_create.keyval != NULL) { \
		args->MPI_Keyval_create.keyval__ref.val = *args->MPI_Keyval_create.keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_connect` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_connect` function call.
 *
 * @struct args_MPI_Comm_connect_t
 *
 * @note 
 *	int
 *	MPI_Comm_connect (
 *			const char * port_name (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_connect
struct args_MPI_Comm_connect_t {
	char * port_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} port_name__ref;
	MPI_Info info;
	int root;
	MPI_Comm comm;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_connect(activity) { \
	activity->mpi_args.MPI_Comm_connect.port_name = (char *) port_name; \
	activity->mpi_args.MPI_Comm_connect.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Comm_connect.root = (int) root; \
	activity->mpi_args.MPI_Comm_connect.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_connect.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_connect(args) { \
	if (args->MPI_Comm_connect.port_name != NULL) { \
		strncpy(args->MPI_Comm_connect.port_name__ref.val, args->MPI_Comm_connect.port_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Comm_connect.newcomm != NULL) { \
		args->MPI_Comm_connect.newcomm__ref.val = *args->MPI_Comm_connect.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ssend` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ssend` function call.
 *
 * @struct args_MPI_Ssend_t
 *
 * @note 
 *	int
 *	MPI_Ssend (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Ssend
struct args_MPI_Ssend_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ssend(activity) { \
	activity->mpi_args.MPI_Ssend.buf = (void *) buf; \
	activity->mpi_args.MPI_Ssend.count = (int) count; \
	activity->mpi_args.MPI_Ssend.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Ssend.dest = (int) dest; \
	activity->mpi_args.MPI_Ssend.tag = (int) tag; \
	activity->mpi_args.MPI_Ssend.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Scatterv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Scatterv_init` function call.
 *
 * @struct args_MPI_Scatterv_init_t
 *
 * @note 
 *	int
 *	MPI_Scatterv_init (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Scatterv_init
struct args_MPI_Scatterv_init_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Scatterv_init(activity) { \
	activity->mpi_args.MPI_Scatterv_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Scatterv_init.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Scatterv_init.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Scatterv_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Scatterv_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Scatterv_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Scatterv_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Scatterv_init.root = (int) root; \
	activity->mpi_args.MPI_Scatterv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Scatterv_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Scatterv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Scatterv_init(args) { \
	if (args->MPI_Scatterv_init.sendcounts != NULL) { \
		args->MPI_Scatterv_init.sendcounts__ref.val = *args->MPI_Scatterv_init.sendcounts; \
	} \
	if (args->MPI_Scatterv_init.displs != NULL) { \
		args->MPI_Scatterv_init.displs__ref.val = *args->MPI_Scatterv_init.displs; \
	} \
	if (args->MPI_Scatterv_init.request != NULL) { \
		args->MPI_Scatterv_init.request__ref.val = *args->MPI_Scatterv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_at_all_end` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_at_all_end` function call.
 *
 * @struct args_MPI_File_write_at_all_end_t
 *
 * @note 
 *	int
 *	MPI_File_write_at_all_end (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_at_all_end
struct args_MPI_File_write_at_all_end_t {
	MPI_File fh;
	void * buf;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_at_all_end(activity) { \
	activity->mpi_args.MPI_File_write_at_all_end.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_at_all_end.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_at_all_end.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_at_all_end(args) { \
	if (args->MPI_File_write_at_all_end.status != NULL) { \
		args->MPI_File_write_at_all_end.status__ref.val = *args->MPI_File_write_at_all_end.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_all_end` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_all_end` function call.
 *
 * @struct args_MPI_File_write_all_end_t
 *
 * @note 
 *	int
 *	MPI_File_write_all_end (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_all_end
struct args_MPI_File_write_all_end_t {
	MPI_File fh;
	void * buf;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_all_end(activity) { \
	activity->mpi_args.MPI_File_write_all_end.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_all_end.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_all_end.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_all_end(args) { \
	if (args->MPI_File_write_all_end.status != NULL) { \
		args->MPI_File_write_all_end.status__ref.val = *args->MPI_File_write_all_end.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Buffer_detach` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Buffer_detach` function call.
 *
 * @struct args_MPI_Buffer_detach_t
 *
 * @note 
 *	int
 *	MPI_Buffer_detach (
 *			void * buffer (void *)
 *			int * size (int *)
 *	)
 */
#if HAVE_MPI_Buffer_detach
struct args_MPI_Buffer_detach_t {
	void * buffer;
	int * size;
	struct {
		int val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Buffer_detach(activity) { \
	activity->mpi_args.MPI_Buffer_detach.buffer = (void *) buffer; \
	activity->mpi_args.MPI_Buffer_detach.size = (int *) size; \
};

#define GET_PTRS_VALUE_MPI_Buffer_detach(args) { \
	if (args->MPI_Buffer_detach.size != NULL) { \
		args->MPI_Buffer_detach.size__ref.val = *args->MPI_Buffer_detach.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Startall` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Startall` function call.
 *
 * @struct args_MPI_Startall_t
 *
 * @note 
 *	int
 *	MPI_Startall (
 *			int count (int)
 *			MPI_Request[] array_of_requests (struct mpi_request_t *[])
 *	)
 */
#if HAVE_MPI_Startall
struct args_MPI_Startall_t {
	int count;
	MPI_Request(* array_of_requests);
	struct {
		MPI_Request val;
	} array_of_requests__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Startall(activity) { \
	activity->mpi_args.MPI_Startall.count = (int) count; \
	activity->mpi_args.MPI_Startall.array_of_requests = (MPI_Request(*)) array_of_requests; \
};

#define GET_PTRS_VALUE_MPI_Startall(args) { \
	if (args->MPI_Startall.array_of_requests != NULL) { \
		args->MPI_Startall.array_of_requests__ref.val = *args->MPI_Startall.array_of_requests; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_alltoall` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_alltoall` function call.
 *
 * @struct args_MPI_Neighbor_alltoall_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_alltoall (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Neighbor_alltoall
struct args_MPI_Neighbor_alltoall_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_alltoall(activity) { \
	activity->mpi_args.MPI_Neighbor_alltoall.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_alltoall.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Neighbor_alltoall.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_alltoall.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_alltoall.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Neighbor_alltoall.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_alltoall.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Put` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Put` function call.
 *
 * @struct args_MPI_Put_t
 *
 * @note 
 *	int
 *	MPI_Put (
 *			const void * origin_addr (const void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_count (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Put
struct args_MPI_Put_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_count;
	MPI_Datatype target_datatype;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Put(activity) { \
	activity->mpi_args.MPI_Put.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Put.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Put.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Put.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Put.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Put.target_count = (int) target_count; \
	activity->mpi_args.MPI_Put.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Put.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_ordered_end` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_ordered_end` function call.
 *
 * @struct args_MPI_File_read_ordered_end_t
 *
 * @note 
 *	int
 *	MPI_File_read_ordered_end (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_ordered_end
struct args_MPI_File_read_ordered_end_t {
	MPI_File fh;
	void * buf;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_ordered_end(activity) { \
	activity->mpi_args.MPI_File_read_ordered_end.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_ordered_end.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_ordered_end.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_ordered_end(args) { \
	if (args->MPI_File_read_ordered_end.status != NULL) { \
		args->MPI_File_read_ordered_end.status__ref.val = *args->MPI_File_read_ordered_end.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_call_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_call_errhandler` function call.
 *
 * @struct args_MPI_Win_call_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Win_call_errhandler (
 *			MPI_Win win (struct mpi_win_t *)
 *			int errorcode (int)
 *	)
 */
#if HAVE_MPI_Win_call_errhandler
struct args_MPI_Win_call_errhandler_t {
	MPI_Win win;
	int errorcode;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_call_errhandler(activity) { \
	activity->mpi_args.MPI_Win_call_errhandler.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_call_errhandler.errorcode = (int) errorcode; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_at` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_at` function call.
 *
 * @struct args_MPI_File_write_at_t
 *
 * @note 
 *	int
 *	MPI_File_write_at (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_at
struct args_MPI_File_write_at_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_at(activity) { \
	activity->mpi_args.MPI_File_write_at.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_at.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_write_at.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_at.count = (int) count; \
	activity->mpi_args.MPI_File_write_at.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_write_at.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_at(args) { \
	if (args->MPI_File_write_at.status != NULL) { \
		args->MPI_File_write_at.status__ref.val = *args->MPI_File_write_at.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_get_pset_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_get_pset_info` function call.
 *
 * @struct args_MPI_Session_get_pset_info_t
 *
 * @note 
 *	int
 *	MPI_Session_get_pset_info (
 *			MPI_Session session (struct mpi_instance_t *)
 *			const char * pset_name (const char *)
 *			MPI_Info * info_used (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Session_get_pset_info
struct args_MPI_Session_get_pset_info_t {
	MPI_Session session;
	char * pset_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} pset_name__ref;
	MPI_Info * info_used;
	struct {
		MPI_Info val;
	} info_used__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_get_pset_info(activity) { \
	activity->mpi_args.MPI_Session_get_pset_info.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_get_pset_info.pset_name = (char *) pset_name; \
	activity->mpi_args.MPI_Session_get_pset_info.info_used = (MPI_Info *) info_used; \
};

#define GET_PTRS_VALUE_MPI_Session_get_pset_info(args) { \
	if (args->MPI_Session_get_pset_info.pset_name != NULL) { \
		strncpy(args->MPI_Session_get_pset_info.pset_name__ref.val, args->MPI_Session_get_pset_info.pset_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Session_get_pset_info.info_used != NULL) { \
		args->MPI_Session_get_pset_info.info_used__ref.val = *args->MPI_Session_get_pset_info.info_used; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Topo_test` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Topo_test` function call.
 *
 * @struct args_MPI_Topo_test_t
 *
 * @note 
 *	int
 *	MPI_Topo_test (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * status (int *)
 *	)
 */
#if HAVE_MPI_Topo_test
struct args_MPI_Topo_test_t {
	MPI_Comm comm;
	int * status;
	struct {
		int val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Topo_test(activity) { \
	activity->mpi_args.MPI_Topo_test.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Topo_test.status = (int *) status; \
};

#define GET_PTRS_VALUE_MPI_Topo_test(args) { \
	if (args->MPI_Topo_test.status != NULL) { \
		args->MPI_Topo_test.status__ref.val = *args->MPI_Topo_test.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_disconnect` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_disconnect` function call.
 *
 * @struct args_MPI_Comm_disconnect_t
 *
 * @note 
 *	int
 *	MPI_Comm_disconnect (
 *			MPI_Comm * comm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_disconnect
struct args_MPI_Comm_disconnect_t {
	MPI_Comm * comm;
	struct {
		MPI_Comm val;
	} comm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_disconnect(activity) { \
	activity->mpi_args.MPI_Comm_disconnect.comm = (MPI_Comm *) comm; \
};

#define GET_PTRS_VALUE_MPI_Comm_disconnect(args) { \
	if (args->MPI_Comm_disconnect.comm != NULL) { \
		args->MPI_Comm_disconnect.comm__ref.val = *args->MPI_Comm_disconnect.comm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Add_error_class` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Add_error_class` function call.
 *
 * @struct args_MPI_Add_error_class_t
 *
 * @note 
 *	int
 *	MPI_Add_error_class (
 *			int * errorclass (int *)
 *	)
 */
#if HAVE_MPI_Add_error_class
struct args_MPI_Add_error_class_t {
	int * errorclass;
	struct {
		int val;
	} errorclass__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Add_error_class(activity) { \
	activity->mpi_args.MPI_Add_error_class.errorclass = (int *) errorclass; \
};

#define GET_PTRS_VALUE_MPI_Add_error_class(args) { \
	if (args->MPI_Add_error_class.errorclass != NULL) { \
		args->MPI_Add_error_class.errorclass__ref.val = *args->MPI_Add_error_class.errorclass; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ireduce_scatter` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ireduce_scatter` function call.
 *
 * @struct args_MPI_Ireduce_scatter_t
 *
 * @note 
 *	int
 *	MPI_Ireduce_scatter (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ireduce_scatter
struct args_MPI_Ireduce_scatter_t {
	void * sendbuf;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ireduce_scatter(activity) { \
	activity->mpi_args.MPI_Ireduce_scatter.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ireduce_scatter.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ireduce_scatter.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Ireduce_scatter.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Ireduce_scatter.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Ireduce_scatter.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ireduce_scatter.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ireduce_scatter(args) { \
	if (args->MPI_Ireduce_scatter.recvcounts != NULL) { \
		args->MPI_Ireduce_scatter.recvcounts__ref.val = *args->MPI_Ireduce_scatter.recvcounts; \
	} \
	if (args->MPI_Ireduce_scatter.request != NULL) { \
		args->MPI_Ireduce_scatter.request__ref.val = *args->MPI_Ireduce_scatter.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cart_map` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cart_map` function call.
 *
 * @struct args_MPI_Cart_map_t
 *
 * @note 
 *	int
 *	MPI_Cart_map (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int ndims (int)
 *			const int[] dims (const int[])
 *			const int[] periods (const int[])
 *			int * newrank (int *)
 *	)
 */
#if HAVE_MPI_Cart_map
struct args_MPI_Cart_map_t {
	MPI_Comm comm;
	int ndims;
	int(* dims);
	struct {
		int val;
	} dims__ref;
	int(* periods);
	struct {
		int val;
	} periods__ref;
	int * newrank;
	struct {
		int val;
	} newrank__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cart_map(activity) { \
	activity->mpi_args.MPI_Cart_map.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Cart_map.ndims = (int) ndims; \
	activity->mpi_args.MPI_Cart_map.dims = (int(*)) dims; \
	activity->mpi_args.MPI_Cart_map.periods = (int(*)) periods; \
	activity->mpi_args.MPI_Cart_map.newrank = (int *) newrank; \
};

#define GET_PTRS_VALUE_MPI_Cart_map(args) { \
	if (args->MPI_Cart_map.dims != NULL) { \
		args->MPI_Cart_map.dims__ref.val = *args->MPI_Cart_map.dims; \
	} \
	if (args->MPI_Cart_map.periods != NULL) { \
		args->MPI_Cart_map.periods__ref.val = *args->MPI_Cart_map.periods; \
	} \
	if (args->MPI_Cart_map.newrank != NULL) { \
		args->MPI_Cart_map.newrank__ref.val = *args->MPI_Cart_map.newrank; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Intercomm_merge` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Intercomm_merge` function call.
 *
 * @struct args_MPI_Intercomm_merge_t
 *
 * @note 
 *	int
 *	MPI_Intercomm_merge (
 *			MPI_Comm intercomm (struct mpi_communicator_t *)
 *			int high (int)
 *			MPI_Comm * newintracomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Intercomm_merge
struct args_MPI_Intercomm_merge_t {
	MPI_Comm intercomm;
	int high;
	MPI_Comm * newintracomm;
	struct {
		MPI_Comm val;
	} newintracomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Intercomm_merge(activity) { \
	activity->mpi_args.MPI_Intercomm_merge.intercomm = (MPI_Comm) intercomm; \
	activity->mpi_args.MPI_Intercomm_merge.high = (int) high; \
	activity->mpi_args.MPI_Intercomm_merge.newintracomm = (MPI_Comm *) newintracomm; \
};

#define GET_PTRS_VALUE_MPI_Intercomm_merge(args) { \
	if (args->MPI_Intercomm_merge.newintracomm != NULL) { \
		args->MPI_Intercomm_merge.newintracomm__ref.val = *args->MPI_Intercomm_merge.newintracomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_hindexed` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_hindexed` function call.
 *
 * @struct args_MPI_Type_create_hindexed_t
 *
 * @note 
 *	int
 *	MPI_Type_create_hindexed (
 *			int count (int)
 *			const int[] array_of_blocklengths (const int[])
 *			const MPI_Aint[] array_of_displacements (const long[])
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_hindexed
struct args_MPI_Type_create_hindexed_t {
	int count;
	int(* array_of_blocklengths);
	struct {
		int val;
	} array_of_blocklengths__ref;
	MPI_Aint(* array_of_displacements);
	struct {
		MPI_Aint val;
	} array_of_displacements__ref;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_hindexed(activity) { \
	activity->mpi_args.MPI_Type_create_hindexed.count = (int) count; \
	activity->mpi_args.MPI_Type_create_hindexed.array_of_blocklengths = (int(*)) array_of_blocklengths; \
	activity->mpi_args.MPI_Type_create_hindexed.array_of_displacements = (MPI_Aint(*)) array_of_displacements; \
	activity->mpi_args.MPI_Type_create_hindexed.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_create_hindexed.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_hindexed(args) { \
	if (args->MPI_Type_create_hindexed.array_of_blocklengths != NULL) { \
		args->MPI_Type_create_hindexed.array_of_blocklengths__ref.val = *args->MPI_Type_create_hindexed.array_of_blocklengths; \
	} \
	if (args->MPI_Type_create_hindexed.array_of_displacements != NULL) { \
		args->MPI_Type_create_hindexed.array_of_displacements__ref.val = *args->MPI_Type_create_hindexed.array_of_displacements; \
	} \
	if (args->MPI_Type_create_hindexed.newtype != NULL) { \
		args->MPI_Type_create_hindexed.newtype__ref.val = *args->MPI_Type_create_hindexed.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_get_nkeys` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_get_nkeys` function call.
 *
 * @struct args_MPI_Info_get_nkeys_t
 *
 * @note 
 *	int
 *	MPI_Info_get_nkeys (
 *			MPI_Info info (struct mpi_info_t *)
 *			int * nkeys (int *)
 *	)
 */
#if HAVE_MPI_Info_get_nkeys
struct args_MPI_Info_get_nkeys_t {
	MPI_Info info;
	int * nkeys;
	struct {
		int val;
	} nkeys__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_get_nkeys(activity) { \
	activity->mpi_args.MPI_Info_get_nkeys.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_get_nkeys.nkeys = (int *) nkeys; \
};

#define GET_PTRS_VALUE_MPI_Info_get_nkeys(args) { \
	if (args->MPI_Info_get_nkeys.nkeys != NULL) { \
		args->MPI_Info_get_nkeys.nkeys__ref.val = *args->MPI_Info_get_nkeys.nkeys; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read` function call.
 *
 * @struct args_MPI_File_read_t
 *
 * @note 
 *	int
 *	MPI_File_read (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read
struct args_MPI_File_read_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read(activity) { \
	activity->mpi_args.MPI_File_read.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read.count = (int) count; \
	activity->mpi_args.MPI_File_read.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_read.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read(args) { \
	if (args->MPI_File_read.status != NULL) { \
		args->MPI_File_read.status__ref.val = *args->MPI_File_read.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ineighbor_allgatherv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ineighbor_allgatherv` function call.
 *
 * @struct args_MPI_Ineighbor_allgatherv_t
 *
 * @note 
 *	int
 *	MPI_Ineighbor_allgatherv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ineighbor_allgatherv
struct args_MPI_Ineighbor_allgatherv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ineighbor_allgatherv(activity) { \
	activity->mpi_args.MPI_Ineighbor_allgatherv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ineighbor_allgatherv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ineighbor_allgatherv(args) { \
	if (args->MPI_Ineighbor_allgatherv.recvcounts != NULL) { \
		args->MPI_Ineighbor_allgatherv.recvcounts__ref.val = *args->MPI_Ineighbor_allgatherv.recvcounts; \
	} \
	if (args->MPI_Ineighbor_allgatherv.displs != NULL) { \
		args->MPI_Ineighbor_allgatherv.displs__ref.val = *args->MPI_Ineighbor_allgatherv.displs; \
	} \
	if (args->MPI_Ineighbor_allgatherv.request != NULL) { \
		args->MPI_Ineighbor_allgatherv.request__ref.val = *args->MPI_Ineighbor_allgatherv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Attr_put` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Attr_put` function call.
 *
 * @struct args_MPI_Attr_put_t
 *
 * @note 
 *	int
 *	MPI_Attr_put (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int keyval (int)
 *			void * attribute_val (void *)
 *	)
 */
#if HAVE_MPI_Attr_put
struct args_MPI_Attr_put_t {
	MPI_Comm comm;
	int keyval;
	void * attribute_val;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Attr_put(activity) { \
	activity->mpi_args.MPI_Attr_put.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Attr_put.keyval = (int) keyval; \
	activity->mpi_args.MPI_Attr_put.attribute_val = (void *) attribute_val; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_ordered_begin` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_ordered_begin` function call.
 *
 * @struct args_MPI_File_write_ordered_begin_t
 *
 * @note 
 *	int
 *	MPI_File_write_ordered_begin (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_File_write_ordered_begin
struct args_MPI_File_write_ordered_begin_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_ordered_begin(activity) { \
	activity->mpi_args.MPI_File_write_ordered_begin.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_ordered_begin.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_ordered_begin.count = (int) count; \
	activity->mpi_args.MPI_File_write_ordered_begin.datatype = (MPI_Datatype) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_set_elements_x` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_set_elements_x` function call.
 *
 * @struct args_MPI_Status_set_elements_x_t
 *
 * @note 
 *	int
 *	MPI_Status_set_elements_x (
 *			MPI_Status * status (struct opaque **)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Count count (long long)
 *	)
 */
#if HAVE_MPI_Status_set_elements_x
struct args_MPI_Status_set_elements_x_t {
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	MPI_Datatype datatype;
	MPI_Count count;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_set_elements_x(activity) { \
	activity->mpi_args.MPI_Status_set_elements_x.status = (MPI_Status *) status; \
	activity->mpi_args.MPI_Status_set_elements_x.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Status_set_elements_x.count = (MPI_Count) count; \
};

#define GET_PTRS_VALUE_MPI_Status_set_elements_x(args) { \
	if (args->MPI_Status_set_elements_x.status != NULL) { \
		args->MPI_Status_set_elements_x.status__ref.val = *args->MPI_Status_set_elements_x.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Compare_and_swap` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Compare_and_swap` function call.
 *
 * @struct args_MPI_Compare_and_swap_t
 *
 * @note 
 *	int
 *	MPI_Compare_and_swap (
 *			const void * origin_addr (const void *)
 *			const void * compare_addr (const void *)
 *			void * result_addr (void *)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Compare_and_swap
struct args_MPI_Compare_and_swap_t {
	void * origin_addr;
	void * compare_addr;
	void * result_addr;
	MPI_Datatype datatype;
	int target_rank;
	MPI_Aint target_disp;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Compare_and_swap(activity) { \
	activity->mpi_args.MPI_Compare_and_swap.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Compare_and_swap.compare_addr = (void *) compare_addr; \
	activity->mpi_args.MPI_Compare_and_swap.result_addr = (void *) result_addr; \
	activity->mpi_args.MPI_Compare_and_swap.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Compare_and_swap.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Compare_and_swap.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Compare_and_swap.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_f90_real` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_f90_real` function call.
 *
 * @struct args_MPI_Type_create_f90_real_t
 *
 * @note 
 *	int
 *	MPI_Type_create_f90_real (
 *			int p (int)
 *			int r (int)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_f90_real
struct args_MPI_Type_create_f90_real_t {
	int p;
	int r;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_f90_real(activity) { \
	activity->mpi_args.MPI_Type_create_f90_real.p = (int) p; \
	activity->mpi_args.MPI_Type_create_f90_real.r = (int) r; \
	activity->mpi_args.MPI_Type_create_f90_real.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_f90_real(args) { \
	if (args->MPI_Type_create_f90_real.newtype != NULL) { \
		args->MPI_Type_create_f90_real.newtype__ref.val = *args->MPI_Type_create_f90_real.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_delete_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_delete_attr` function call.
 *
 * @struct args_MPI_Type_delete_attr_t
 *
 * @note 
 *	int
 *	MPI_Type_delete_attr (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			int type_keyval (int)
 *	)
 */
#if HAVE_MPI_Type_delete_attr
struct args_MPI_Type_delete_attr_t {
	MPI_Datatype type;
	int type_keyval;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_delete_attr(activity) { \
	activity->mpi_args.MPI_Type_delete_attr.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_delete_attr.type_keyval = (int) type_keyval; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Probe` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Probe` function call.
 *
 * @struct args_MPI_Probe_t
 *
 * @note 
 *	int
 *	MPI_Probe (
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Probe
struct args_MPI_Probe_t {
	int source;
	int tag;
	MPI_Comm comm;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Probe(activity) { \
	activity->mpi_args.MPI_Probe.source = (int) source; \
	activity->mpi_args.MPI_Probe.tag = (int) tag; \
	activity->mpi_args.MPI_Probe.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Probe.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Probe(args) { \
	if (args->MPI_Probe.status != NULL) { \
		args->MPI_Probe.status__ref.val = *args->MPI_Probe.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_close` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_close` function call.
 *
 * @struct args_MPI_File_close_t
 *
 * @note 
 *	int
 *	MPI_File_close (
 *			MPI_File * fh (struct mpi_file_t **)
 *	)
 */
#if HAVE_MPI_File_close
struct args_MPI_File_close_t {
	MPI_File * fh;
	struct {
		MPI_File val;
	} fh__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_close(activity) { \
	activity->mpi_args.MPI_File_close.fh = (MPI_File *) fh; \
};

#define GET_PTRS_VALUE_MPI_File_close(args) { \
	if (args->MPI_File_close.fh != NULL) { \
		args->MPI_File_close.fh__ref.val = *args->MPI_File_close.fh; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Request_get_status` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Request_get_status` function call.
 *
 * @struct args_MPI_Request_get_status_t
 *
 * @note 
 *	int
 *	MPI_Request_get_status (
 *			MPI_Request request (struct mpi_request_t *)
 *			int * flag (int *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Request_get_status
struct args_MPI_Request_get_status_t {
	MPI_Request request;
	int * flag;
	struct {
		int val;
	} flag__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Request_get_status(activity) { \
	activity->mpi_args.MPI_Request_get_status.request = (MPI_Request) request; \
	activity->mpi_args.MPI_Request_get_status.flag = (int *) flag; \
	activity->mpi_args.MPI_Request_get_status.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Request_get_status(args) { \
	if (args->MPI_Request_get_status.flag != NULL) { \
		args->MPI_Request_get_status.flag__ref.val = *args->MPI_Request_get_status.flag; \
	} \
	if (args->MPI_Request_get_status.status != NULL) { \
		args->MPI_Request_get_status.status__ref.val = *args->MPI_Request_get_status.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_call_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_call_errhandler` function call.
 *
 * @struct args_MPI_Session_call_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Session_call_errhandler (
 *			MPI_Session session (struct mpi_instance_t *)
 *			int errorcode (int)
 *	)
 */
#if HAVE_MPI_Session_call_errhandler
struct args_MPI_Session_call_errhandler_t {
	MPI_Session session;
	int errorcode;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_call_errhandler(activity) { \
	activity->mpi_args.MPI_Session_call_errhandler.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_call_errhandler.errorcode = (int) errorcode; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Rget_accumulate` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Rget_accumulate` function call.
 *
 * @struct args_MPI_Rget_accumulate_t
 *
 * @note 
 *	int
 *	MPI_Rget_accumulate (
 *			const void * origin_addr (const void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			void * result_addr (void *)
 *			int result_count (int)
 *			MPI_Datatype result_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_count (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Rget_accumulate
struct args_MPI_Rget_accumulate_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	void * result_addr;
	int result_count;
	MPI_Datatype result_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_count;
	MPI_Datatype target_datatype;
	MPI_Op op;
	MPI_Win win;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Rget_accumulate(activity) { \
	activity->mpi_args.MPI_Rget_accumulate.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Rget_accumulate.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Rget_accumulate.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Rget_accumulate.result_addr = (void *) result_addr; \
	activity->mpi_args.MPI_Rget_accumulate.result_count = (int) result_count; \
	activity->mpi_args.MPI_Rget_accumulate.result_datatype = (MPI_Datatype) result_datatype; \
	activity->mpi_args.MPI_Rget_accumulate.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Rget_accumulate.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Rget_accumulate.target_count = (int) target_count; \
	activity->mpi_args.MPI_Rget_accumulate.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Rget_accumulate.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Rget_accumulate.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Rget_accumulate.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Rget_accumulate(args) { \
	if (args->MPI_Rget_accumulate.request != NULL) { \
		args->MPI_Rget_accumulate.request__ref.val = *args->MPI_Rget_accumulate.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iread_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iread_all` function call.
 *
 * @struct args_MPI_File_iread_all_t
 *
 * @note 
 *	int
 *	MPI_File_iread_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iread_all
struct args_MPI_File_iread_all_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iread_all(activity) { \
	activity->mpi_args.MPI_File_iread_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iread_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iread_all.count = (int) count; \
	activity->mpi_args.MPI_File_iread_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iread_all.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iread_all(args) { \
	if (args->MPI_File_iread_all.request != NULL) { \
		args->MPI_File_iread_all.request__ref.val = *args->MPI_File_iread_all.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Isendrecv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Isendrecv` function call.
 *
 * @struct args_MPI_Isendrecv_t
 *
 * @note 
 *	int
 *	MPI_Isendrecv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int sendtag (int)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int source (int)
 *			int recvtag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Isendrecv
struct args_MPI_Isendrecv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	int dest;
	int sendtag;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int source;
	int recvtag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Isendrecv(activity) { \
	activity->mpi_args.MPI_Isendrecv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Isendrecv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Isendrecv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Isendrecv.dest = (int) dest; \
	activity->mpi_args.MPI_Isendrecv.sendtag = (int) sendtag; \
	activity->mpi_args.MPI_Isendrecv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Isendrecv.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Isendrecv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Isendrecv.source = (int) source; \
	activity->mpi_args.MPI_Isendrecv.recvtag = (int) recvtag; \
	activity->mpi_args.MPI_Isendrecv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Isendrecv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Isendrecv(args) { \
	if (args->MPI_Isendrecv.request != NULL) { \
		args->MPI_Isendrecv.request__ref.val = *args->MPI_Isendrecv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Pack_external` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Pack_external` function call.
 *
 * @struct args_MPI_Pack_external_t
 *
 * @note 
 *	int
 *	MPI_Pack_external (
 *			const char[] datarep (const char[])
 *			const void * inbuf (const void *)
 *			int incount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			void * outbuf (void *)
 *			MPI_Aint outsize (long)
 *			MPI_Aint * position (long*)
 *	)
 */
#if HAVE_MPI_Pack_external
struct args_MPI_Pack_external_t {
	char(* datarep);
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} datarep__ref;
	void * inbuf;
	int incount;
	MPI_Datatype datatype;
	void * outbuf;
	MPI_Aint outsize;
	MPI_Aint * position;
	struct {
		MPI_Aint val;
	} position__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Pack_external(activity) { \
	activity->mpi_args.MPI_Pack_external.datarep = (char(*)) datarep; \
	activity->mpi_args.MPI_Pack_external.inbuf = (void *) inbuf; \
	activity->mpi_args.MPI_Pack_external.incount = (int) incount; \
	activity->mpi_args.MPI_Pack_external.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Pack_external.outbuf = (void *) outbuf; \
	activity->mpi_args.MPI_Pack_external.outsize = (MPI_Aint) outsize; \
	activity->mpi_args.MPI_Pack_external.position = (MPI_Aint *) position; \
};

#define GET_PTRS_VALUE_MPI_Pack_external(args) { \
	if (args->MPI_Pack_external.datarep != NULL) { \
		strncpy(args->MPI_Pack_external.datarep__ref.val, args->MPI_Pack_external.datarep, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Pack_external.position != NULL) { \
		args->MPI_Pack_external.position__ref.val = *args->MPI_Pack_external.position; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Pready_range` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Pready_range` function call.
 *
 * @struct args_MPI_Pready_range_t
 *
 * @note 
 *	int
 *	MPI_Pready_range (
 *			int partition_low (int)
 *			int partition_high (int)
 *			MPI_Request request (struct mpi_request_t *)
 *	)
 */
#if HAVE_MPI_Pready_range
struct args_MPI_Pready_range_t {
	int partition_low;
	int partition_high;
	MPI_Request request;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Pready_range(activity) { \
	activity->mpi_args.MPI_Pready_range.partition_low = (int) partition_low; \
	activity->mpi_args.MPI_Pready_range.partition_high = (int) partition_high; \
	activity->mpi_args.MPI_Pready_range.request = (MPI_Request) request; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_envelope` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_envelope` function call.
 *
 * @struct args_MPI_Type_get_envelope_t
 *
 * @note 
 *	int
 *	MPI_Type_get_envelope (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			int * num_integers (int *)
 *			int * num_addresses (int *)
 *			int * num_datatypes (int *)
 *			int * combiner (int *)
 *	)
 */
#if HAVE_MPI_Type_get_envelope
struct args_MPI_Type_get_envelope_t {
	MPI_Datatype type;
	int * num_integers;
	struct {
		int val;
	} num_integers__ref;
	int * num_addresses;
	struct {
		int val;
	} num_addresses__ref;
	int * num_datatypes;
	struct {
		int val;
	} num_datatypes__ref;
	int * combiner;
	struct {
		int val;
	} combiner__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_envelope(activity) { \
	activity->mpi_args.MPI_Type_get_envelope.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_get_envelope.num_integers = (int *) num_integers; \
	activity->mpi_args.MPI_Type_get_envelope.num_addresses = (int *) num_addresses; \
	activity->mpi_args.MPI_Type_get_envelope.num_datatypes = (int *) num_datatypes; \
	activity->mpi_args.MPI_Type_get_envelope.combiner = (int *) combiner; \
};

#define GET_PTRS_VALUE_MPI_Type_get_envelope(args) { \
	if (args->MPI_Type_get_envelope.num_integers != NULL) { \
		args->MPI_Type_get_envelope.num_integers__ref.val = *args->MPI_Type_get_envelope.num_integers; \
	} \
	if (args->MPI_Type_get_envelope.num_addresses != NULL) { \
		args->MPI_Type_get_envelope.num_addresses__ref.val = *args->MPI_Type_get_envelope.num_addresses; \
	} \
	if (args->MPI_Type_get_envelope.num_datatypes != NULL) { \
		args->MPI_Type_get_envelope.num_datatypes__ref.val = *args->MPI_Type_get_envelope.num_datatypes; \
	} \
	if (args->MPI_Type_get_envelope.combiner != NULL) { \
		args->MPI_Type_get_envelope.combiner__ref.val = *args->MPI_Type_get_envelope.combiner; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_create` function call.
 *
 * @struct args_MPI_Win_create_t
 *
 * @note 
 *	int
 *	MPI_Win_create (
 *			void * base (void *)
 *			MPI_Aint size (long)
 *			int disp_unit (int)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Win * win (struct mpi_win_t **)
 *	)
 */
#if HAVE_MPI_Win_create
struct args_MPI_Win_create_t {
	void * base;
	MPI_Aint size;
	int disp_unit;
	MPI_Info info;
	MPI_Comm comm;
	MPI_Win * win;
	struct {
		MPI_Win val;
	} win__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_create(activity) { \
	activity->mpi_args.MPI_Win_create.base = (void *) base; \
	activity->mpi_args.MPI_Win_create.size = (MPI_Aint) size; \
	activity->mpi_args.MPI_Win_create.disp_unit = (int) disp_unit; \
	activity->mpi_args.MPI_Win_create.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Win_create.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Win_create.win = (MPI_Win *) win; \
};

#define GET_PTRS_VALUE_MPI_Win_create(args) { \
	if (args->MPI_Win_create.win != NULL) { \
		args->MPI_Win_create.win__ref.val = *args->MPI_Win_create.win; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Isendrecv_replace` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Isendrecv_replace` function call.
 *
 * @struct args_MPI_Isendrecv_replace_t
 *
 * @note 
 *	int
 *	MPI_Isendrecv_replace (
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int sendtag (int)
 *			int source (int)
 *			int recvtag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Isendrecv_replace
struct args_MPI_Isendrecv_replace_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int sendtag;
	int source;
	int recvtag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Isendrecv_replace(activity) { \
	activity->mpi_args.MPI_Isendrecv_replace.buf = (void *) buf; \
	activity->mpi_args.MPI_Isendrecv_replace.count = (int) count; \
	activity->mpi_args.MPI_Isendrecv_replace.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Isendrecv_replace.dest = (int) dest; \
	activity->mpi_args.MPI_Isendrecv_replace.sendtag = (int) sendtag; \
	activity->mpi_args.MPI_Isendrecv_replace.source = (int) source; \
	activity->mpi_args.MPI_Isendrecv_replace.recvtag = (int) recvtag; \
	activity->mpi_args.MPI_Isendrecv_replace.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Isendrecv_replace.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Isendrecv_replace(args) { \
	if (args->MPI_Isendrecv_replace.request != NULL) { \
		args->MPI_Isendrecv_replace.request__ref.val = *args->MPI_Isendrecv_replace.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_set_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_set_errhandler` function call.
 *
 * @struct args_MPI_Win_set_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Win_set_errhandler (
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *	)
 */
#if HAVE_MPI_Win_set_errhandler
struct args_MPI_Win_set_errhandler_t {
	MPI_Win win;
	MPI_Errhandler errhandler;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_set_errhandler(activity) { \
	activity->mpi_args.MPI_Win_set_errhandler.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_set_errhandler.errhandler = (MPI_Errhandler) errhandler; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Fetch_and_op` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Fetch_and_op` function call.
 *
 * @struct args_MPI_Fetch_and_op_t
 *
 * @note 
 *	int
 *	MPI_Fetch_and_op (
 *			const void * origin_addr (const void *)
 *			void * result_addr (void *)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Fetch_and_op
struct args_MPI_Fetch_and_op_t {
	void * origin_addr;
	void * result_addr;
	MPI_Datatype datatype;
	int target_rank;
	MPI_Aint target_disp;
	MPI_Op op;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Fetch_and_op(activity) { \
	activity->mpi_args.MPI_Fetch_and_op.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Fetch_and_op.result_addr = (void *) result_addr; \
	activity->mpi_args.MPI_Fetch_and_op.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Fetch_and_op.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Fetch_and_op.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Fetch_and_op.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Fetch_and_op.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cartdim_get` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cartdim_get` function call.
 *
 * @struct args_MPI_Cartdim_get_t
 *
 * @note 
 *	int
 *	MPI_Cartdim_get (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * ndims (int *)
 *	)
 */
#if HAVE_MPI_Cartdim_get
struct args_MPI_Cartdim_get_t {
	MPI_Comm comm;
	int * ndims;
	struct {
		int val;
	} ndims__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cartdim_get(activity) { \
	activity->mpi_args.MPI_Cartdim_get.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Cartdim_get.ndims = (int *) ndims; \
};

#define GET_PTRS_VALUE_MPI_Cartdim_get(args) { \
	if (args->MPI_Cartdim_get.ndims != NULL) { \
		args->MPI_Cartdim_get.ndims__ref.val = *args->MPI_Cartdim_get.ndims; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Dist_graph_neighbors` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Dist_graph_neighbors` function call.
 *
 * @struct args_MPI_Dist_graph_neighbors_t
 *
 * @note 
 *	int
 *	MPI_Dist_graph_neighbors (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int maxindegree (int)
 *			int[] sources (int[])
 *			int[] sourceweights (int[])
 *			int maxoutdegree (int)
 *			int[] destinations (int[])
 *			int[] destweights (int[])
 *	)
 */
#if HAVE_MPI_Dist_graph_neighbors
struct args_MPI_Dist_graph_neighbors_t {
	MPI_Comm comm;
	int maxindegree;
	int(* sources);
	struct {
		int val;
	} sources__ref;
	int(* sourceweights);
	struct {
		int val;
	} sourceweights__ref;
	int maxoutdegree;
	int(* destinations);
	struct {
		int val;
	} destinations__ref;
	int(* destweights);
	struct {
		int val;
	} destweights__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Dist_graph_neighbors(activity) { \
	activity->mpi_args.MPI_Dist_graph_neighbors.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Dist_graph_neighbors.maxindegree = (int) maxindegree; \
	activity->mpi_args.MPI_Dist_graph_neighbors.sources = (int(*)) sources; \
	activity->mpi_args.MPI_Dist_graph_neighbors.sourceweights = (int(*)) sourceweights; \
	activity->mpi_args.MPI_Dist_graph_neighbors.maxoutdegree = (int) maxoutdegree; \
	activity->mpi_args.MPI_Dist_graph_neighbors.destinations = (int(*)) destinations; \
	activity->mpi_args.MPI_Dist_graph_neighbors.destweights = (int(*)) destweights; \
};

#define GET_PTRS_VALUE_MPI_Dist_graph_neighbors(args) { \
	if (args->MPI_Dist_graph_neighbors.sources != NULL) { \
		args->MPI_Dist_graph_neighbors.sources__ref.val = *args->MPI_Dist_graph_neighbors.sources; \
	} \
	if (args->MPI_Dist_graph_neighbors.sourceweights != NULL) { \
		args->MPI_Dist_graph_neighbors.sourceweights__ref.val = *args->MPI_Dist_graph_neighbors.sourceweights; \
	} \
	if (args->MPI_Dist_graph_neighbors.destinations != NULL) { \
		args->MPI_Dist_graph_neighbors.destinations__ref.val = *args->MPI_Dist_graph_neighbors.destinations; \
	} \
	if (args->MPI_Dist_graph_neighbors.destweights != NULL) { \
		args->MPI_Dist_graph_neighbors.destweights__ref.val = *args->MPI_Dist_graph_neighbors.destweights; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_seek` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_seek` function call.
 *
 * @struct args_MPI_File_seek_t
 *
 * @note 
 *	int
 *	MPI_File_seek (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			int whence (int)
 *	)
 */
#if HAVE_MPI_File_seek
struct args_MPI_File_seek_t {
	MPI_File fh;
	MPI_Offset offset;
	int whence;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_seek(activity) { \
	activity->mpi_args.MPI_File_seek.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_seek.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_seek.whence = (int) whence; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get` function call.
 *
 * @struct args_MPI_Get_t
 *
 * @note 
 *	int
 *	MPI_Get (
 *			void * origin_addr (void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_count (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Get
struct args_MPI_Get_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_count;
	MPI_Datatype target_datatype;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get(activity) { \
	activity->mpi_args.MPI_Get.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Get.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Get.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Get.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Get.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Get.target_count = (int) target_count; \
	activity->mpi_args.MPI_Get.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Get.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Pack_external_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Pack_external_size` function call.
 *
 * @struct args_MPI_Pack_external_size_t
 *
 * @note 
 *	int
 *	MPI_Pack_external_size (
 *			const char[] datarep (const char[])
 *			int incount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Aint * size (long*)
 *	)
 */
#if HAVE_MPI_Pack_external_size
struct args_MPI_Pack_external_size_t {
	char(* datarep);
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} datarep__ref;
	int incount;
	MPI_Datatype datatype;
	MPI_Aint * size;
	struct {
		MPI_Aint val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Pack_external_size(activity) { \
	activity->mpi_args.MPI_Pack_external_size.datarep = (char(*)) datarep; \
	activity->mpi_args.MPI_Pack_external_size.incount = (int) incount; \
	activity->mpi_args.MPI_Pack_external_size.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Pack_external_size.size = (MPI_Aint *) size; \
};

#define GET_PTRS_VALUE_MPI_Pack_external_size(args) { \
	if (args->MPI_Pack_external_size.datarep != NULL) { \
		strncpy(args->MPI_Pack_external_size.datarep__ref.val, args->MPI_Pack_external_size.datarep, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Pack_external_size.size != NULL) { \
		args->MPI_Pack_external_size.size__ref.val = *args->MPI_Pack_external_size.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_flush_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_flush_all` function call.
 *
 * @struct args_MPI_Win_flush_all_t
 *
 * @note 
 *	int
 *	MPI_Win_flush_all (
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_flush_all
struct args_MPI_Win_flush_all_t {
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_flush_all(activity) { \
	activity->mpi_args.MPI_Win_flush_all.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Rsend` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Rsend` function call.
 *
 * @struct args_MPI_Rsend_t
 *
 * @note 
 *	int
 *	MPI_Rsend (
 *			const void * ibuf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Rsend
struct args_MPI_Rsend_t {
	void * ibuf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Rsend(activity) { \
	activity->mpi_args.MPI_Rsend.ibuf = (void *) ibuf; \
	activity->mpi_args.MPI_Rsend.count = (int) count; \
	activity->mpi_args.MPI_Rsend.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Rsend.dest = (int) dest; \
	activity->mpi_args.MPI_Rsend.tag = (int) tag; \
	activity->mpi_args.MPI_Rsend.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_free` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_free` function call.
 *
 * @struct args_MPI_Win_free_t
 *
 * @note 
 *	int
 *	MPI_Win_free (
 *			MPI_Win * win (struct mpi_win_t **)
 *	)
 */
#if HAVE_MPI_Win_free
struct args_MPI_Win_free_t {
	MPI_Win * win;
	struct {
		MPI_Win val;
	} win__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_free(activity) { \
	activity->mpi_args.MPI_Win_free.win = (MPI_Win *) win; \
};

#define GET_PTRS_VALUE_MPI_Win_free(args) { \
	if (args->MPI_Win_free.win != NULL) { \
		args->MPI_Win_free.win__ref.val = *args->MPI_Win_free.win; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_f90_complex` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_f90_complex` function call.
 *
 * @struct args_MPI_Type_create_f90_complex_t
 *
 * @note 
 *	int
 *	MPI_Type_create_f90_complex (
 *			int p (int)
 *			int r (int)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_f90_complex
struct args_MPI_Type_create_f90_complex_t {
	int p;
	int r;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_f90_complex(activity) { \
	activity->mpi_args.MPI_Type_create_f90_complex.p = (int) p; \
	activity->mpi_args.MPI_Type_create_f90_complex.r = (int) r; \
	activity->mpi_args.MPI_Type_create_f90_complex.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_f90_complex(args) { \
	if (args->MPI_Type_create_f90_complex.newtype != NULL) { \
		args->MPI_Type_create_f90_complex.newtype__ref.val = *args->MPI_Type_create_f90_complex.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_alltoallw_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_alltoallw_init` function call.
 *
 * @struct args_MPI_Neighbor_alltoallw_init_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_alltoallw_init (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const MPI_Aint[] sdispls (const long[])
 *			const MPI_Datatype[] sendtypes (const struct mpi_datatype_t *[])
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const MPI_Aint[] rdispls (const long[])
 *			const MPI_Datatype[] recvtypes (const struct mpi_datatype_t *[])
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Neighbor_alltoallw_init
struct args_MPI_Neighbor_alltoallw_init_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	MPI_Aint(* sdispls);
	struct {
		MPI_Aint val;
	} sdispls__ref;
	MPI_Datatype(* sendtypes);
	struct {
		MPI_Datatype val;
	} sendtypes__ref;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	MPI_Aint(* rdispls);
	struct {
		MPI_Aint val;
	} rdispls__ref;
	MPI_Datatype(* recvtypes);
	struct {
		MPI_Datatype val;
	} recvtypes__ref;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_alltoallw_init(activity) { \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.sdispls = (MPI_Aint(*)) sdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.sendtypes = (MPI_Datatype(*)) sendtypes; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.rdispls = (MPI_Aint(*)) rdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.recvtypes = (MPI_Datatype(*)) recvtypes; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Neighbor_alltoallw_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_alltoallw_init(args) { \
	if (args->MPI_Neighbor_alltoallw_init.sendcounts != NULL) { \
		args->MPI_Neighbor_alltoallw_init.sendcounts__ref.val = *args->MPI_Neighbor_alltoallw_init.sendcounts; \
	} \
	if (args->MPI_Neighbor_alltoallw_init.sdispls != NULL) { \
		args->MPI_Neighbor_alltoallw_init.sdispls__ref.val = *args->MPI_Neighbor_alltoallw_init.sdispls; \
	} \
	if (args->MPI_Neighbor_alltoallw_init.sendtypes != NULL) { \
		args->MPI_Neighbor_alltoallw_init.sendtypes__ref.val = *args->MPI_Neighbor_alltoallw_init.sendtypes; \
	} \
	if (args->MPI_Neighbor_alltoallw_init.recvcounts != NULL) { \
		args->MPI_Neighbor_alltoallw_init.recvcounts__ref.val = *args->MPI_Neighbor_alltoallw_init.recvcounts; \
	} \
	if (args->MPI_Neighbor_alltoallw_init.rdispls != NULL) { \
		args->MPI_Neighbor_alltoallw_init.rdispls__ref.val = *args->MPI_Neighbor_alltoallw_init.rdispls; \
	} \
	if (args->MPI_Neighbor_alltoallw_init.recvtypes != NULL) { \
		args->MPI_Neighbor_alltoallw_init.recvtypes__ref.val = *args->MPI_Neighbor_alltoallw_init.recvtypes; \
	} \
	if (args->MPI_Neighbor_alltoallw_init.request != NULL) { \
		args->MPI_Neighbor_alltoallw_init.request__ref.val = *args->MPI_Neighbor_alltoallw_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Rget` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Rget` function call.
 *
 * @struct args_MPI_Rget_t
 *
 * @note 
 *	int
 *	MPI_Rget (
 *			void * origin_addr (void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_count (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Rget
struct args_MPI_Rget_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_count;
	MPI_Datatype target_datatype;
	MPI_Win win;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Rget(activity) { \
	activity->mpi_args.MPI_Rget.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Rget.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Rget.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Rget.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Rget.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Rget.target_count = (int) target_count; \
	activity->mpi_args.MPI_Rget.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Rget.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Rget.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Rget(args) { \
	if (args->MPI_Rget.request != NULL) { \
		args->MPI_Rget.request__ref.val = *args->MPI_Rget.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_create_keyval` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_create_keyval` function call.
 *
 * @struct args_MPI_Win_create_keyval_t
 *
 * @note 
 *	int
 *	MPI_Win_create_keyval (
 *			MPI_Win_copy_attr_function * win_copy_attr_fn (int (*)(struct mpi_win_t *, int, void *, void *, void *, int *))
 *			MPI_Win_delete_attr_function * win_delete_attr_fn (int (*)(struct mpi_win_t *, int, void *, void *))
 *			int * win_keyval (int *)
 *			void * extra_state (void *)
 *	)
 */
#if HAVE_MPI_Win_create_keyval
struct args_MPI_Win_create_keyval_t {
	MPI_Win_copy_attr_function * win_copy_attr_fn;
	MPI_Win_delete_attr_function * win_delete_attr_fn;
	int * win_keyval;
	struct {
		int val;
	} win_keyval__ref;
	void * extra_state;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_create_keyval(activity) { \
	activity->mpi_args.MPI_Win_create_keyval.win_copy_attr_fn = (MPI_Win_copy_attr_function *) win_copy_attr_fn; \
	activity->mpi_args.MPI_Win_create_keyval.win_delete_attr_fn = (MPI_Win_delete_attr_function *) win_delete_attr_fn; \
	activity->mpi_args.MPI_Win_create_keyval.win_keyval = (int *) win_keyval; \
	activity->mpi_args.MPI_Win_create_keyval.extra_state = (void *) extra_state; \
};

#define GET_PTRS_VALUE_MPI_Win_create_keyval(args) { \
	if (args->MPI_Win_create_keyval.win_keyval != NULL) { \
		args->MPI_Win_create_keyval.win_keyval__ref.val = *args->MPI_Win_create_keyval.win_keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Op_commutative` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Op_commutative` function call.
 *
 * @struct args_MPI_Op_commutative_t
 *
 * @note 
 *	int
 *	MPI_Op_commutative (
 *			MPI_Op op (struct mpi_op_t *)
 *			int * commute (int *)
 *	)
 */
#if HAVE_MPI_Op_commutative
struct args_MPI_Op_commutative_t {
	MPI_Op op;
	int * commute;
	struct {
		int val;
	} commute__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Op_commutative(activity) { \
	activity->mpi_args.MPI_Op_commutative.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Op_commutative.commute = (int *) commute; \
};

#define GET_PTRS_VALUE_MPI_Op_commutative(args) { \
	if (args->MPI_Op_commutative.commute != NULL) { \
		args->MPI_Op_commutative.commute__ref.val = *args->MPI_Op_commutative.commute; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_allgather` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_allgather` function call.
 *
 * @struct args_MPI_Neighbor_allgather_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_allgather (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Neighbor_allgather
struct args_MPI_Neighbor_allgather_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_allgather(activity) { \
	activity->mpi_args.MPI_Neighbor_allgather.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_allgather.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Neighbor_allgather.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_allgather.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_allgather.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Neighbor_allgather.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_allgather.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_call_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_call_errhandler` function call.
 *
 * @struct args_MPI_Comm_call_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Comm_call_errhandler (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int errorcode (int)
 *	)
 */
#if HAVE_MPI_Comm_call_errhandler
struct args_MPI_Comm_call_errhandler_t {
	MPI_Comm comm;
	int errorcode;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_call_errhandler(activity) { \
	activity->mpi_args.MPI_Comm_call_errhandler.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_call_errhandler.errorcode = (int) errorcode; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Scatter_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Scatter_init` function call.
 *
 * @struct args_MPI_Scatter_init_t
 *
 * @note 
 *	int
 *	MPI_Scatter_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Scatter_init
struct args_MPI_Scatter_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Scatter_init(activity) { \
	activity->mpi_args.MPI_Scatter_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Scatter_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Scatter_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Scatter_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Scatter_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Scatter_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Scatter_init.root = (int) root; \
	activity->mpi_args.MPI_Scatter_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Scatter_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Scatter_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Scatter_init(args) { \
	if (args->MPI_Scatter_init.request != NULL) { \
		args->MPI_Scatter_init.request__ref.val = *args->MPI_Scatter_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_get_string` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_get_string` function call.
 *
 * @struct args_MPI_Info_get_string_t
 *
 * @note 
 *	int
 *	MPI_Info_get_string (
 *			MPI_Info info (struct mpi_info_t *)
 *			const char * key (const char *)
 *			int * buflen (int *)
 *			char * value (char *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Info_get_string
struct args_MPI_Info_get_string_t {
	MPI_Info info;
	char * key;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} key__ref;
	int * buflen;
	struct {
		int val;
	} buflen__ref;
	char * value;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} value__ref;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_get_string(activity) { \
	activity->mpi_args.MPI_Info_get_string.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_get_string.key = (char *) key; \
	activity->mpi_args.MPI_Info_get_string.buflen = (int *) buflen; \
	activity->mpi_args.MPI_Info_get_string.value = (char *) value; \
	activity->mpi_args.MPI_Info_get_string.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Info_get_string(args) { \
	if (args->MPI_Info_get_string.key != NULL) { \
		strncpy(args->MPI_Info_get_string.key__ref.val, args->MPI_Info_get_string.key, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Info_get_string.buflen != NULL) { \
		args->MPI_Info_get_string.buflen__ref.val = *args->MPI_Info_get_string.buflen; \
	} \
	if (args->MPI_Info_get_string.value != NULL) { \
		strncpy(args->MPI_Info_get_string.value__ref.val, args->MPI_Info_get_string.value, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Info_get_string.flag != NULL) { \
		args->MPI_Info_get_string.flag__ref.val = *args->MPI_Info_get_string.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Mrecv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Mrecv` function call.
 *
 * @struct args_MPI_Mrecv_t
 *
 * @note 
 *	int
 *	MPI_Mrecv (
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			MPI_Message * message (struct mpi_message_t **)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Mrecv
struct args_MPI_Mrecv_t {
	void * buf;
	int count;
	MPI_Datatype type;
	MPI_Message * message;
	struct {
		MPI_Message val;
	} message__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Mrecv(activity) { \
	activity->mpi_args.MPI_Mrecv.buf = (void *) buf; \
	activity->mpi_args.MPI_Mrecv.count = (int) count; \
	activity->mpi_args.MPI_Mrecv.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Mrecv.message = (MPI_Message *) message; \
	activity->mpi_args.MPI_Mrecv.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Mrecv(args) { \
	if (args->MPI_Mrecv.message != NULL) { \
		args->MPI_Mrecv.message__ref.val = *args->MPI_Mrecv.message; \
	} \
	if (args->MPI_Mrecv.status != NULL) { \
		args->MPI_Mrecv.status__ref.val = *args->MPI_Mrecv.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Open_port` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Open_port` function call.
 *
 * @struct args_MPI_Open_port_t
 *
 * @note 
 *	int
 *	MPI_Open_port (
 *			MPI_Info info (struct mpi_info_t *)
 *			char * port_name (char *)
 *	)
 */
#if HAVE_MPI_Open_port
struct args_MPI_Open_port_t {
	MPI_Info info;
	char * port_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} port_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Open_port(activity) { \
	activity->mpi_args.MPI_Open_port.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Open_port.port_name = (char *) port_name; \
};

#define GET_PTRS_VALUE_MPI_Open_port(args) { \
	if (args->MPI_Open_port.port_name != NULL) { \
		strncpy(args->MPI_Open_port.port_name__ref.val, args->MPI_Open_port.port_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cart_get` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cart_get` function call.
 *
 * @struct args_MPI_Cart_get_t
 *
 * @note 
 *	int
 *	MPI_Cart_get (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int maxdims (int)
 *			int[] dims (int[])
 *			int[] periods (int[])
 *			int[] coords (int[])
 *	)
 */
#if HAVE_MPI_Cart_get
struct args_MPI_Cart_get_t {
	MPI_Comm comm;
	int maxdims;
	int(* dims);
	struct {
		int val;
	} dims__ref;
	int(* periods);
	struct {
		int val;
	} periods__ref;
	int(* coords);
	struct {
		int val;
	} coords__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cart_get(activity) { \
	activity->mpi_args.MPI_Cart_get.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Cart_get.maxdims = (int) maxdims; \
	activity->mpi_args.MPI_Cart_get.dims = (int(*)) dims; \
	activity->mpi_args.MPI_Cart_get.periods = (int(*)) periods; \
	activity->mpi_args.MPI_Cart_get.coords = (int(*)) coords; \
};

#define GET_PTRS_VALUE_MPI_Cart_get(args) { \
	if (args->MPI_Cart_get.dims != NULL) { \
		args->MPI_Cart_get.dims__ref.val = *args->MPI_Cart_get.dims; \
	} \
	if (args->MPI_Cart_get.periods != NULL) { \
		args->MPI_Cart_get.periods__ref.val = *args->MPI_Cart_get.periods; \
	} \
	if (args->MPI_Cart_get.coords != NULL) { \
		args->MPI_Cart_get.coords__ref.val = *args->MPI_Cart_get.coords; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Lookup_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Lookup_name` function call.
 *
 * @struct args_MPI_Lookup_name_t
 *
 * @note 
 *	int
 *	MPI_Lookup_name (
 *			const char * service_name (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *			char * port_name (char *)
 *	)
 */
#if HAVE_MPI_Lookup_name
struct args_MPI_Lookup_name_t {
	char * service_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} service_name__ref;
	MPI_Info info;
	char * port_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} port_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Lookup_name(activity) { \
	activity->mpi_args.MPI_Lookup_name.service_name = (char *) service_name; \
	activity->mpi_args.MPI_Lookup_name.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Lookup_name.port_name = (char *) port_name; \
};

#define GET_PTRS_VALUE_MPI_Lookup_name(args) { \
	if (args->MPI_Lookup_name.service_name != NULL) { \
		strncpy(args->MPI_Lookup_name.service_name__ref.val, args->MPI_Lookup_name.service_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Lookup_name.port_name != NULL) { \
		strncpy(args->MPI_Lookup_name.port_name__ref.val, args->MPI_Lookup_name.port_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_extent` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_extent` function call.
 *
 * @struct args_MPI_Type_get_extent_t
 *
 * @note 
 *	int
 *	MPI_Type_get_extent (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			MPI_Aint * lb (long*)
 *			MPI_Aint * extent (long*)
 *	)
 */
#if HAVE_MPI_Type_get_extent
struct args_MPI_Type_get_extent_t {
	MPI_Datatype type;
	MPI_Aint * lb;
	struct {
		MPI_Aint val;
	} lb__ref;
	MPI_Aint * extent;
	struct {
		MPI_Aint val;
	} extent__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_extent(activity) { \
	activity->mpi_args.MPI_Type_get_extent.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_get_extent.lb = (MPI_Aint *) lb; \
	activity->mpi_args.MPI_Type_get_extent.extent = (MPI_Aint *) extent; \
};

#define GET_PTRS_VALUE_MPI_Type_get_extent(args) { \
	if (args->MPI_Type_get_extent.lb != NULL) { \
		args->MPI_Type_get_extent.lb__ref.val = *args->MPI_Type_get_extent.lb; \
	} \
	if (args->MPI_Type_get_extent.extent != NULL) { \
		args->MPI_Type_get_extent.extent__ref.val = *args->MPI_Type_get_extent.extent; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_spawn` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_spawn` function call.
 *
 * @struct args_MPI_Comm_spawn_t
 *
 * @note 
 *	int
 *	MPI_Comm_spawn (
 *			const char * command (const char *)
 *			char *[] argv (char *[])
 *			int maxprocs (int)
 *			MPI_Info info (struct mpi_info_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Comm * intercomm (struct mpi_communicator_t **)
 *			int[] array_of_errcodes (int[])
 *	)
 */
#if HAVE_MPI_Comm_spawn
struct args_MPI_Comm_spawn_t {
	char * command;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} command__ref;
	char *(* argv);
	struct {
		void* ptr1;
		char val[MPI_STRING_SIZE_MAX];
	} argv__ref;
	int maxprocs;
	MPI_Info info;
	int root;
	MPI_Comm comm;
	MPI_Comm * intercomm;
	struct {
		MPI_Comm val;
	} intercomm__ref;
	int(* array_of_errcodes);
	struct {
		int val;
	} array_of_errcodes__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_spawn(activity) { \
	activity->mpi_args.MPI_Comm_spawn.command = (char *) command; \
	activity->mpi_args.MPI_Comm_spawn.argv = (char *(*)) argv; \
	activity->mpi_args.MPI_Comm_spawn.maxprocs = (int) maxprocs; \
	activity->mpi_args.MPI_Comm_spawn.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Comm_spawn.root = (int) root; \
	activity->mpi_args.MPI_Comm_spawn.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_spawn.intercomm = (MPI_Comm *) intercomm; \
	activity->mpi_args.MPI_Comm_spawn.array_of_errcodes = (int(*)) array_of_errcodes; \
};

#define GET_PTRS_VALUE_MPI_Comm_spawn(args) { \
	if (args->MPI_Comm_spawn.command != NULL) { \
		strncpy(args->MPI_Comm_spawn.command__ref.val, args->MPI_Comm_spawn.command, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Comm_spawn.argv != NULL) { \
		args->MPI_Comm_spawn.argv__ref.ptr1 = *args->MPI_Comm_spawn.argv; \
		if (args->MPI_Comm_spawn.argv__ref.ptr1 != NULL) { \
			strncpy(args->MPI_Comm_spawn.argv__ref.val, args->MPI_Comm_spawn.argv__ref.ptr1, MPI_STRING_SIZE_MAX-1); \
		} \
	} \
	if (args->MPI_Comm_spawn.intercomm != NULL) { \
		args->MPI_Comm_spawn.intercomm__ref.val = *args->MPI_Comm_spawn.intercomm; \
	} \
	if (args->MPI_Comm_spawn.array_of_errcodes != NULL) { \
		args->MPI_Comm_spawn.array_of_errcodes__ref.val = *args->MPI_Comm_spawn.array_of_errcodes; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Unpublish_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Unpublish_name` function call.
 *
 * @struct args_MPI_Unpublish_name_t
 *
 * @note 
 *	int
 *	MPI_Unpublish_name (
 *			const char * service_name (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *			const char * port_name (const char *)
 *	)
 */
#if HAVE_MPI_Unpublish_name
struct args_MPI_Unpublish_name_t {
	char * service_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} service_name__ref;
	MPI_Info info;
	char * port_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} port_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Unpublish_name(activity) { \
	activity->mpi_args.MPI_Unpublish_name.service_name = (char *) service_name; \
	activity->mpi_args.MPI_Unpublish_name.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Unpublish_name.port_name = (char *) port_name; \
};

#define GET_PTRS_VALUE_MPI_Unpublish_name(args) { \
	if (args->MPI_Unpublish_name.service_name != NULL) { \
		strncpy(args->MPI_Unpublish_name.service_name__ref.val, args->MPI_Unpublish_name.service_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Unpublish_name.port_name != NULL) { \
		strncpy(args->MPI_Unpublish_name.port_name__ref.val, args->MPI_Unpublish_name.port_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Grequest_complete` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Grequest_complete` function call.
 *
 * @struct args_MPI_Grequest_complete_t
 *
 * @note 
 *	int
 *	MPI_Grequest_complete (
 *			MPI_Request request (struct mpi_request_t *)
 *	)
 */
#if HAVE_MPI_Grequest_complete
struct args_MPI_Grequest_complete_t {
	MPI_Request request;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Grequest_complete(activity) { \
	activity->mpi_args.MPI_Grequest_complete.request = (MPI_Request) request; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_group` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_group` function call.
 *
 * @struct args_MPI_File_get_group_t
 *
 * @note 
 *	int
 *	MPI_File_get_group (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Group * group (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_File_get_group
struct args_MPI_File_get_group_t {
	MPI_File fh;
	MPI_Group * group;
	struct {
		MPI_Group val;
	} group__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_group(activity) { \
	activity->mpi_args.MPI_File_get_group.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_group.group = (MPI_Group *) group; \
};

#define GET_PTRS_VALUE_MPI_File_get_group(args) { \
	if (args->MPI_File_get_group.group != NULL) { \
		args->MPI_File_get_group.group__ref.val = *args->MPI_File_get_group.group; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iread_at_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iread_at_all` function call.
 *
 * @struct args_MPI_File_iread_at_all_t
 *
 * @note 
 *	int
 *	MPI_File_iread_at_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iread_at_all
struct args_MPI_File_iread_at_all_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iread_at_all(activity) { \
	activity->mpi_args.MPI_File_iread_at_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iread_at_all.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_iread_at_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iread_at_all.count = (int) count; \
	activity->mpi_args.MPI_File_iread_at_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iread_at_all.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iread_at_all(args) { \
	if (args->MPI_File_iread_at_all.request != NULL) { \
		args->MPI_File_iread_at_all.request__ref.val = *args->MPI_File_iread_at_all.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Graphdims_get` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Graphdims_get` function call.
 *
 * @struct args_MPI_Graphdims_get_t
 *
 * @note 
 *	int
 *	MPI_Graphdims_get (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * nnodes (int *)
 *			int * nedges (int *)
 *	)
 */
#if HAVE_MPI_Graphdims_get
struct args_MPI_Graphdims_get_t {
	MPI_Comm comm;
	int * nnodes;
	struct {
		int val;
	} nnodes__ref;
	int * nedges;
	struct {
		int val;
	} nedges__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Graphdims_get(activity) { \
	activity->mpi_args.MPI_Graphdims_get.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Graphdims_get.nnodes = (int *) nnodes; \
	activity->mpi_args.MPI_Graphdims_get.nedges = (int *) nedges; \
};

#define GET_PTRS_VALUE_MPI_Graphdims_get(args) { \
	if (args->MPI_Graphdims_get.nnodes != NULL) { \
		args->MPI_Graphdims_get.nnodes__ref.val = *args->MPI_Graphdims_get.nnodes; \
	} \
	if (args->MPI_Graphdims_get.nedges != NULL) { \
		args->MPI_Graphdims_get.nedges__ref.val = *args->MPI_Graphdims_get.nedges; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iread_shared` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iread_shared` function call.
 *
 * @struct args_MPI_File_iread_shared_t
 *
 * @note 
 *	int
 *	MPI_File_iread_shared (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iread_shared
struct args_MPI_File_iread_shared_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iread_shared(activity) { \
	activity->mpi_args.MPI_File_iread_shared.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iread_shared.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iread_shared.count = (int) count; \
	activity->mpi_args.MPI_File_iread_shared.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iread_shared.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iread_shared(args) { \
	if (args->MPI_File_iread_shared.request != NULL) { \
		args->MPI_File_iread_shared.request__ref.val = *args->MPI_File_iread_shared.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_idup_with_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_idup_with_info` function call.
 *
 * @struct args_MPI_Comm_idup_with_info_t
 *
 * @note 
 *	int
 *	MPI_Comm_idup_with_info (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Comm_idup_with_info
struct args_MPI_Comm_idup_with_info_t {
	MPI_Comm comm;
	MPI_Info info;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_idup_with_info(activity) { \
	activity->mpi_args.MPI_Comm_idup_with_info.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_idup_with_info.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Comm_idup_with_info.newcomm = (MPI_Comm *) newcomm; \
	activity->mpi_args.MPI_Comm_idup_with_info.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Comm_idup_with_info(args) { \
	if (args->MPI_Comm_idup_with_info.newcomm != NULL) { \
		args->MPI_Comm_idup_with_info.newcomm__ref.val = *args->MPI_Comm_idup_with_info.newcomm; \
	} \
	if (args->MPI_Comm_idup_with_info.request != NULL) { \
		args->MPI_Comm_idup_with_info.request__ref.val = *args->MPI_Comm_idup_with_info.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_version` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_version` function call.
 *
 * @struct args_MPI_Get_version_t
 *
 * @note 
 *	int
 *	MPI_Get_version (
 *			int * version (int *)
 *			int * subversion (int *)
 *	)
 */
#if HAVE_MPI_Get_version
struct args_MPI_Get_version_t {
	int * version;
	struct {
		int val;
	} version__ref;
	int * subversion;
	struct {
		int val;
	} subversion__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_version(activity) { \
	activity->mpi_args.MPI_Get_version.version = (int *) version; \
	activity->mpi_args.MPI_Get_version.subversion = (int *) subversion; \
};

#define GET_PTRS_VALUE_MPI_Get_version(args) { \
	if (args->MPI_Get_version.version != NULL) { \
		args->MPI_Get_version.version__ref.val = *args->MPI_Get_version.version; \
	} \
	if (args->MPI_Get_version.subversion != NULL) { \
		args->MPI_Get_version.subversion__ref.val = *args->MPI_Get_version.subversion; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_lock_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_lock_all` function call.
 *
 * @struct args_MPI_Win_lock_all_t
 *
 * @note 
 *	int
 *	MPI_Win_lock_all (
 *			int mpi_assert (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_lock_all
struct args_MPI_Win_lock_all_t {
	int mpi_assert;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_lock_all(activity) { \
	activity->mpi_args.MPI_Win_lock_all.mpi_assert = (int) mpi_assert; \
	activity->mpi_args.MPI_Win_lock_all.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Intercomm_create_from_groups` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Intercomm_create_from_groups` function call.
 *
 * @struct args_MPI_Intercomm_create_from_groups_t
 *
 * @note 
 *	int
 *	MPI_Intercomm_create_from_groups (
 *			MPI_Group local_group (struct mpi_group_t *)
 *			int local_leader (int)
 *			MPI_Group remote_group (struct mpi_group_t *)
 *			int remote_leader (int)
 *			const char * tag (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *			MPI_Comm * newintercomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Intercomm_create_from_groups
struct args_MPI_Intercomm_create_from_groups_t {
	MPI_Group local_group;
	int local_leader;
	MPI_Group remote_group;
	int remote_leader;
	char * tag;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} tag__ref;
	MPI_Info info;
	MPI_Errhandler errhandler;
	MPI_Comm * newintercomm;
	struct {
		MPI_Comm val;
	} newintercomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Intercomm_create_from_groups(activity) { \
	activity->mpi_args.MPI_Intercomm_create_from_groups.local_group = (MPI_Group) local_group; \
	activity->mpi_args.MPI_Intercomm_create_from_groups.local_leader = (int) local_leader; \
	activity->mpi_args.MPI_Intercomm_create_from_groups.remote_group = (MPI_Group) remote_group; \
	activity->mpi_args.MPI_Intercomm_create_from_groups.remote_leader = (int) remote_leader; \
	activity->mpi_args.MPI_Intercomm_create_from_groups.tag = (char *) tag; \
	activity->mpi_args.MPI_Intercomm_create_from_groups.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Intercomm_create_from_groups.errhandler = (MPI_Errhandler) errhandler; \
	activity->mpi_args.MPI_Intercomm_create_from_groups.newintercomm = (MPI_Comm *) newintercomm; \
};

#define GET_PTRS_VALUE_MPI_Intercomm_create_from_groups(args) { \
	if (args->MPI_Intercomm_create_from_groups.tag != NULL) { \
		strncpy(args->MPI_Intercomm_create_from_groups.tag__ref.val, args->MPI_Intercomm_create_from_groups.tag, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Intercomm_create_from_groups.newintercomm != NULL) { \
		args->MPI_Intercomm_create_from_groups.newintercomm__ref.val = *args->MPI_Intercomm_create_from_groups.newintercomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_alltoallv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_alltoallv_init` function call.
 *
 * @struct args_MPI_Neighbor_alltoallv_init_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_alltoallv_init (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Neighbor_alltoallv_init
struct args_MPI_Neighbor_alltoallv_init_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_alltoallv_init(activity) { \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Neighbor_alltoallv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_alltoallv_init(args) { \
	if (args->MPI_Neighbor_alltoallv_init.sendcounts != NULL) { \
		args->MPI_Neighbor_alltoallv_init.sendcounts__ref.val = *args->MPI_Neighbor_alltoallv_init.sendcounts; \
	} \
	if (args->MPI_Neighbor_alltoallv_init.sdispls != NULL) { \
		args->MPI_Neighbor_alltoallv_init.sdispls__ref.val = *args->MPI_Neighbor_alltoallv_init.sdispls; \
	} \
	if (args->MPI_Neighbor_alltoallv_init.recvcounts != NULL) { \
		args->MPI_Neighbor_alltoallv_init.recvcounts__ref.val = *args->MPI_Neighbor_alltoallv_init.recvcounts; \
	} \
	if (args->MPI_Neighbor_alltoallv_init.rdispls != NULL) { \
		args->MPI_Neighbor_alltoallv_init.rdispls__ref.val = *args->MPI_Neighbor_alltoallv_init.rdispls; \
	} \
	if (args->MPI_Neighbor_alltoallv_init.request != NULL) { \
		args->MPI_Neighbor_alltoallv_init.request__ref.val = *args->MPI_Neighbor_alltoallv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_darray` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_darray` function call.
 *
 * @struct args_MPI_Type_create_darray_t
 *
 * @note 
 *	int
 *	MPI_Type_create_darray (
 *			int size (int)
 *			int rank (int)
 *			int ndims (int)
 *			const int[] gsize_array (const int[])
 *			const int[] distrib_array (const int[])
 *			const int[] darg_array (const int[])
 *			const int[] psize_array (const int[])
 *			int order (int)
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_darray
struct args_MPI_Type_create_darray_t {
	int size;
	int rank;
	int ndims;
	int(* gsize_array);
	struct {
		int val;
	} gsize_array__ref;
	int(* distrib_array);
	struct {
		int val;
	} distrib_array__ref;
	int(* darg_array);
	struct {
		int val;
	} darg_array__ref;
	int(* psize_array);
	struct {
		int val;
	} psize_array__ref;
	int order;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_darray(activity) { \
	activity->mpi_args.MPI_Type_create_darray.size = (int) size; \
	activity->mpi_args.MPI_Type_create_darray.rank = (int) rank; \
	activity->mpi_args.MPI_Type_create_darray.ndims = (int) ndims; \
	activity->mpi_args.MPI_Type_create_darray.gsize_array = (int(*)) gsize_array; \
	activity->mpi_args.MPI_Type_create_darray.distrib_array = (int(*)) distrib_array; \
	activity->mpi_args.MPI_Type_create_darray.darg_array = (int(*)) darg_array; \
	activity->mpi_args.MPI_Type_create_darray.psize_array = (int(*)) psize_array; \
	activity->mpi_args.MPI_Type_create_darray.order = (int) order; \
	activity->mpi_args.MPI_Type_create_darray.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_create_darray.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_darray(args) { \
	if (args->MPI_Type_create_darray.gsize_array != NULL) { \
		args->MPI_Type_create_darray.gsize_array__ref.val = *args->MPI_Type_create_darray.gsize_array; \
	} \
	if (args->MPI_Type_create_darray.distrib_array != NULL) { \
		args->MPI_Type_create_darray.distrib_array__ref.val = *args->MPI_Type_create_darray.distrib_array; \
	} \
	if (args->MPI_Type_create_darray.darg_array != NULL) { \
		args->MPI_Type_create_darray.darg_array__ref.val = *args->MPI_Type_create_darray.darg_array; \
	} \
	if (args->MPI_Type_create_darray.psize_array != NULL) { \
		args->MPI_Type_create_darray.psize_array__ref.val = *args->MPI_Type_create_darray.psize_array; \
	} \
	if (args->MPI_Type_create_darray.newtype != NULL) { \
		args->MPI_Type_create_darray.newtype__ref.val = *args->MPI_Type_create_darray.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_position_shared` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_position_shared` function call.
 *
 * @struct args_MPI_File_get_position_shared_t
 *
 * @note 
 *	int
 *	MPI_File_get_position_shared (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset * offset (long long*)
 *	)
 */
#if HAVE_MPI_File_get_position_shared
struct args_MPI_File_get_position_shared_t {
	MPI_File fh;
	MPI_Offset * offset;
	struct {
		MPI_Offset val;
	} offset__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_position_shared(activity) { \
	activity->mpi_args.MPI_File_get_position_shared.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_position_shared.offset = (MPI_Offset *) offset; \
};

#define GET_PTRS_VALUE_MPI_File_get_position_shared(args) { \
	if (args->MPI_File_get_position_shared.offset != NULL) { \
		args->MPI_File_get_position_shared.offset__ref.val = *args->MPI_File_get_position_shared.offset; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_get_group` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_get_group` function call.
 *
 * @struct args_MPI_Win_get_group_t
 *
 * @note 
 *	int
 *	MPI_Win_get_group (
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Group * group (struct mpi_group_t **)
 *	)
 */
#if HAVE_MPI_Win_get_group
struct args_MPI_Win_get_group_t {
	MPI_Win win;
	MPI_Group * group;
	struct {
		MPI_Group val;
	} group__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_get_group(activity) { \
	activity->mpi_args.MPI_Win_get_group.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_get_group.group = (MPI_Group *) group; \
};

#define GET_PTRS_VALUE_MPI_Win_get_group(args) { \
	if (args->MPI_Win_get_group.group != NULL) { \
		args->MPI_Win_get_group.group__ref.val = *args->MPI_Win_get_group.group; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Error_class` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Error_class` function call.
 *
 * @struct args_MPI_Error_class_t
 *
 * @note 
 *	int
 *	MPI_Error_class (
 *			int errorcode (int)
 *			int * errorclass (int *)
 *	)
 */
#if HAVE_MPI_Error_class
struct args_MPI_Error_class_t {
	int errorcode;
	int * errorclass;
	struct {
		int val;
	} errorclass__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Error_class(activity) { \
	activity->mpi_args.MPI_Error_class.errorcode = (int) errorcode; \
	activity->mpi_args.MPI_Error_class.errorclass = (int *) errorclass; \
};

#define GET_PTRS_VALUE_MPI_Error_class(args) { \
	if (args->MPI_Error_class.errorclass != NULL) { \
		args->MPI_Error_class.errorclass__ref.val = *args->MPI_Error_class.errorclass; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_get_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_get_attr` function call.
 *
 * @struct args_MPI_Win_get_attr_t
 *
 * @note 
 *	int
 *	MPI_Win_get_attr (
 *			MPI_Win win (struct mpi_win_t *)
 *			int win_keyval (int)
 *			void * attribute_val (void *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Win_get_attr
struct args_MPI_Win_get_attr_t {
	MPI_Win win;
	int win_keyval;
	void * attribute_val;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_get_attr(activity) { \
	activity->mpi_args.MPI_Win_get_attr.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_get_attr.win_keyval = (int) win_keyval; \
	activity->mpi_args.MPI_Win_get_attr.attribute_val = (void *) attribute_val; \
	activity->mpi_args.MPI_Win_get_attr.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Win_get_attr(args) { \
	if (args->MPI_Win_get_attr.flag != NULL) { \
		args->MPI_Win_get_attr.flag__ref.val = *args->MPI_Win_get_attr.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Reduce_local` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Reduce_local` function call.
 *
 * @struct args_MPI_Reduce_local_t
 *
 * @note 
 *	int
 *	MPI_Reduce_local (
 *			const void * inbuf (const void *)
 *			void * inoutbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *	)
 */
#if HAVE_MPI_Reduce_local
struct args_MPI_Reduce_local_t {
	void * inbuf;
	void * inoutbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Reduce_local(activity) { \
	activity->mpi_args.MPI_Reduce_local.inbuf = (void *) inbuf; \
	activity->mpi_args.MPI_Reduce_local.inoutbuf = (void *) inoutbuf; \
	activity->mpi_args.MPI_Reduce_local.count = (int) count; \
	activity->mpi_args.MPI_Reduce_local.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Reduce_local.op = (MPI_Op) op; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ireduce_scatter_block` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ireduce_scatter_block` function call.
 *
 * @struct args_MPI_Ireduce_scatter_block_t
 *
 * @note 
 *	int
 *	MPI_Ireduce_scatter_block (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ireduce_scatter_block
struct args_MPI_Ireduce_scatter_block_t {
	void * sendbuf;
	void * recvbuf;
	int recvcount;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ireduce_scatter_block(activity) { \
	activity->mpi_args.MPI_Ireduce_scatter_block.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ireduce_scatter_block.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ireduce_scatter_block.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Ireduce_scatter_block.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Ireduce_scatter_block.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Ireduce_scatter_block.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ireduce_scatter_block.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ireduce_scatter_block(args) { \
	if (args->MPI_Ireduce_scatter_block.request != NULL) { \
		args->MPI_Ireduce_scatter_block.request__ref.val = *args->MPI_Ireduce_scatter_block.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_set_cancelled` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_set_cancelled` function call.
 *
 * @struct args_MPI_Status_set_cancelled_t
 *
 * @note 
 *	int
 *	MPI_Status_set_cancelled (
 *			MPI_Status * status (struct opaque **)
 *			int flag (int)
 *	)
 */
#if HAVE_MPI_Status_set_cancelled
struct args_MPI_Status_set_cancelled_t {
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int flag;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_set_cancelled(activity) { \
	activity->mpi_args.MPI_Status_set_cancelled.status = (MPI_Status *) status; \
	activity->mpi_args.MPI_Status_set_cancelled.flag = (int) flag; \
};

#define GET_PTRS_VALUE_MPI_Status_set_cancelled(args) { \
	if (args->MPI_Status_set_cancelled.status != NULL) { \
		args->MPI_Status_set_cancelled.status__ref.val = *args->MPI_Status_set_cancelled.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_test` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_test` function call.
 *
 * @struct args_MPI_Win_test_t
 *
 * @note 
 *	int
 *	MPI_Win_test (
 *			MPI_Win win (struct mpi_win_t *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Win_test
struct args_MPI_Win_test_t {
	MPI_Win win;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_test(activity) { \
	activity->mpi_args.MPI_Win_test.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_test.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Win_test(args) { \
	if (args->MPI_Win_test.flag != NULL) { \
		args->MPI_Win_test.flag__ref.val = *args->MPI_Win_test.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Test_cancelled` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Test_cancelled` function call.
 *
 * @struct args_MPI_Test_cancelled_t
 *
 * @note 
 *	int
 *	MPI_Test_cancelled (
 *			const MPI_Status * status (const struct opaque * *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Test_cancelled
struct args_MPI_Test_cancelled_t {
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Test_cancelled(activity) { \
	activity->mpi_args.MPI_Test_cancelled.status = (MPI_Status *) status; \
	activity->mpi_args.MPI_Test_cancelled.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Test_cancelled(args) { \
	if (args->MPI_Test_cancelled.status != NULL) { \
		args->MPI_Test_cancelled.status__ref.val = *args->MPI_Test_cancelled.status; \
	} \
	if (args->MPI_Test_cancelled.flag != NULL) { \
		args->MPI_Test_cancelled.flag__ref.val = *args->MPI_Test_cancelled.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_seek_shared` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_seek_shared` function call.
 *
 * @struct args_MPI_File_seek_shared_t
 *
 * @note 
 *	int
 *	MPI_File_seek_shared (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			int whence (int)
 *	)
 */
#if HAVE_MPI_File_seek_shared
struct args_MPI_File_seek_shared_t {
	MPI_File fh;
	MPI_Offset offset;
	int whence;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_seek_shared(activity) { \
	activity->mpi_args.MPI_File_seek_shared.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_seek_shared.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_seek_shared.whence = (int) whence; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Error_string` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Error_string` function call.
 *
 * @struct args_MPI_Error_string_t
 *
 * @note 
 *	int
 *	MPI_Error_string (
 *			int errorcode (int)
 *			char * string (char *)
 *			int * resultlen (int *)
 *	)
 */
#if HAVE_MPI_Error_string
struct args_MPI_Error_string_t {
	int errorcode;
	char * string;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} string__ref;
	int * resultlen;
	struct {
		int val;
	} resultlen__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Error_string(activity) { \
	activity->mpi_args.MPI_Error_string.errorcode = (int) errorcode; \
	activity->mpi_args.MPI_Error_string.string = (char *) string; \
	activity->mpi_args.MPI_Error_string.resultlen = (int *) resultlen; \
};

#define GET_PTRS_VALUE_MPI_Error_string(args) { \
	if (args->MPI_Error_string.string != NULL) { \
		strncpy(args->MPI_Error_string.string__ref.val, args->MPI_Error_string.string, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Error_string.resultlen != NULL) { \
		args->MPI_Error_string.resultlen__ref.val = *args->MPI_Error_string.resultlen; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Graph_neighbors_count` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Graph_neighbors_count` function call.
 *
 * @struct args_MPI_Graph_neighbors_count_t
 *
 * @note 
 *	int
 *	MPI_Graph_neighbors_count (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int rank (int)
 *			int * nneighbors (int *)
 *	)
 */
#if HAVE_MPI_Graph_neighbors_count
struct args_MPI_Graph_neighbors_count_t {
	MPI_Comm comm;
	int rank;
	int * nneighbors;
	struct {
		int val;
	} nneighbors__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Graph_neighbors_count(activity) { \
	activity->mpi_args.MPI_Graph_neighbors_count.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Graph_neighbors_count.rank = (int) rank; \
	activity->mpi_args.MPI_Graph_neighbors_count.nneighbors = (int *) nneighbors; \
};

#define GET_PTRS_VALUE_MPI_Graph_neighbors_count(args) { \
	if (args->MPI_Graph_neighbors_count.nneighbors != NULL) { \
		args->MPI_Graph_neighbors_count.nneighbors__ref.val = *args->MPI_Graph_neighbors_count.nneighbors; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_create_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_create_errhandler` function call.
 *
 * @struct args_MPI_Session_create_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Session_create_errhandler (
 *			MPI_Session_errhandler_function * session_errhandler_fn (void (*)(struct mpi_instance_t * *, int *, ...))
 *			MPI_Errhandler * errhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_Session_create_errhandler
struct args_MPI_Session_create_errhandler_t {
	MPI_Session_errhandler_function * session_errhandler_fn;
	MPI_Errhandler * errhandler;
	struct {
		MPI_Errhandler val;
	} errhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_create_errhandler(activity) { \
	activity->mpi_args.MPI_Session_create_errhandler.session_errhandler_fn = (MPI_Session_errhandler_function *) session_errhandler_fn; \
	activity->mpi_args.MPI_Session_create_errhandler.errhandler = (MPI_Errhandler *) errhandler; \
};

#define GET_PTRS_VALUE_MPI_Session_create_errhandler(args) { \
	if (args->MPI_Session_create_errhandler.errhandler != NULL) { \
		args->MPI_Session_create_errhandler.errhandler__ref.val = *args->MPI_Session_create_errhandler.errhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_unlock` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_unlock` function call.
 *
 * @struct args_MPI_Win_unlock_t
 *
 * @note 
 *	int
 *	MPI_Win_unlock (
 *			int rank (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_unlock
struct args_MPI_Win_unlock_t {
	int rank;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_unlock(activity) { \
	activity->mpi_args.MPI_Win_unlock.rank = (int) rank; \
	activity->mpi_args.MPI_Win_unlock.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iscatter` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iscatter` function call.
 *
 * @struct args_MPI_Iscatter_t
 *
 * @note 
 *	int
 *	MPI_Iscatter (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Iscatter
struct args_MPI_Iscatter_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iscatter(activity) { \
	activity->mpi_args.MPI_Iscatter.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Iscatter.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Iscatter.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Iscatter.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Iscatter.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Iscatter.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Iscatter.root = (int) root; \
	activity->mpi_args.MPI_Iscatter.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iscatter.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Iscatter(args) { \
	if (args->MPI_Iscatter.request != NULL) { \
		args->MPI_Iscatter.request__ref.val = *args->MPI_Iscatter.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_all` function call.
 *
 * @struct args_MPI_File_read_all_t
 *
 * @note 
 *	int
 *	MPI_File_read_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_all
struct args_MPI_File_read_all_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_all(activity) { \
	activity->mpi_args.MPI_File_read_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_all.count = (int) count; \
	activity->mpi_args.MPI_File_read_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_read_all.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_all(args) { \
	if (args->MPI_File_read_all.status != NULL) { \
		args->MPI_File_read_all.status__ref.val = *args->MPI_File_read_all.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_set_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_set_errhandler` function call.
 *
 * @struct args_MPI_File_set_errhandler_t
 *
 * @note 
 *	int
 *	MPI_File_set_errhandler (
 *			MPI_File file (struct mpi_file_t *)
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *	)
 */
#if HAVE_MPI_File_set_errhandler
struct args_MPI_File_set_errhandler_t {
	MPI_File file;
	MPI_Errhandler errhandler;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_set_errhandler(activity) { \
	activity->mpi_args.MPI_File_set_errhandler.file = (MPI_File) file; \
	activity->mpi_args.MPI_File_set_errhandler.errhandler = (MPI_Errhandler) errhandler; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_set_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_set_attr` function call.
 *
 * @struct args_MPI_Type_set_attr_t
 *
 * @note 
 *	int
 *	MPI_Type_set_attr (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			int type_keyval (int)
 *			void * attr_val (void *)
 *	)
 */
#if HAVE_MPI_Type_set_attr
struct args_MPI_Type_set_attr_t {
	MPI_Datatype type;
	int type_keyval;
	void * attr_val;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_set_attr(activity) { \
	activity->mpi_args.MPI_Type_set_attr.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_set_attr.type_keyval = (int) type_keyval; \
	activity->mpi_args.MPI_Type_set_attr.attr_val = (void *) attr_val; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_errhandler` function call.
 *
 * @struct args_MPI_File_get_errhandler_t
 *
 * @note 
 *	int
 *	MPI_File_get_errhandler (
 *			MPI_File file (struct mpi_file_t *)
 *			MPI_Errhandler * errhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_File_get_errhandler
struct args_MPI_File_get_errhandler_t {
	MPI_File file;
	MPI_Errhandler * errhandler;
	struct {
		MPI_Errhandler val;
	} errhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_errhandler(activity) { \
	activity->mpi_args.MPI_File_get_errhandler.file = (MPI_File) file; \
	activity->mpi_args.MPI_File_get_errhandler.errhandler = (MPI_Errhandler *) errhandler; \
};

#define GET_PTRS_VALUE_MPI_File_get_errhandler(args) { \
	if (args->MPI_File_get_errhandler.errhandler != NULL) { \
		args->MPI_File_get_errhandler.errhandler__ref.val = *args->MPI_File_get_errhandler.errhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_finalize` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_finalize` function call.
 *
 * @struct args_MPI_Session_finalize_t
 *
 * @note 
 *	int
 *	MPI_Session_finalize (
 *			MPI_Session * session (struct mpi_instance_t **)
 *	)
 */
#if HAVE_MPI_Session_finalize
struct args_MPI_Session_finalize_t {
	MPI_Session * session;
	struct {
		MPI_Session val;
	} session__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_finalize(activity) { \
	activity->mpi_args.MPI_Session_finalize.session = (MPI_Session *) session; \
};

#define GET_PTRS_VALUE_MPI_Session_finalize(args) { \
	if (args->MPI_Session_finalize.session != NULL) { \
		args->MPI_Session_finalize.session__ref.val = *args->MPI_Session_finalize.session; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_free_keyval` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_free_keyval` function call.
 *
 * @struct args_MPI_Comm_free_keyval_t
 *
 * @note 
 *	int
 *	MPI_Comm_free_keyval (
 *			int * comm_keyval (int *)
 *	)
 */
#if HAVE_MPI_Comm_free_keyval
struct args_MPI_Comm_free_keyval_t {
	int * comm_keyval;
	struct {
		int val;
	} comm_keyval__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_free_keyval(activity) { \
	activity->mpi_args.MPI_Comm_free_keyval.comm_keyval = (int *) comm_keyval; \
};

#define GET_PTRS_VALUE_MPI_Comm_free_keyval(args) { \
	if (args->MPI_Comm_free_keyval.comm_keyval != NULL) { \
		args->MPI_Comm_free_keyval.comm_keyval__ref.val = *args->MPI_Comm_free_keyval.comm_keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iwrite_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iwrite_all` function call.
 *
 * @struct args_MPI_File_iwrite_all_t
 *
 * @note 
 *	int
 *	MPI_File_iwrite_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iwrite_all
struct args_MPI_File_iwrite_all_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iwrite_all(activity) { \
	activity->mpi_args.MPI_File_iwrite_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iwrite_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iwrite_all.count = (int) count; \
	activity->mpi_args.MPI_File_iwrite_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iwrite_all.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iwrite_all(args) { \
	if (args->MPI_File_iwrite_all.request != NULL) { \
		args->MPI_File_iwrite_all.request__ref.val = *args->MPI_File_iwrite_all.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Free_mem` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Free_mem` function call.
 *
 * @struct args_MPI_Free_mem_t
 *
 * @note 
 *	int
 *	MPI_Free_mem (
 *			void * base (void *)
 *	)
 */
#if HAVE_MPI_Free_mem
struct args_MPI_Free_mem_t {
	void * base;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Free_mem(activity) { \
	activity->mpi_args.MPI_Free_mem.base = (void *) base; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_set_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_set_info` function call.
 *
 * @struct args_MPI_Win_set_info_t
 *
 * @note 
 *	int
 *	MPI_Win_set_info (
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *	)
 */
#if HAVE_MPI_Win_set_info
struct args_MPI_Win_set_info_t {
	MPI_Win win;
	MPI_Info info;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_set_info(activity) { \
	activity->mpi_args.MPI_Win_set_info.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_set_info.info = (MPI_Info) info; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Alltoallv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Alltoallv_init` function call.
 *
 * @struct args_MPI_Alltoallv_init_t
 *
 * @note 
 *	int
 *	MPI_Alltoallv_init (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Alltoallv_init
struct args_MPI_Alltoallv_init_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Alltoallv_init(activity) { \
	activity->mpi_args.MPI_Alltoallv_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Alltoallv_init.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Alltoallv_init.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Alltoallv_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Alltoallv_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Alltoallv_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Alltoallv_init.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Alltoallv_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Alltoallv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Alltoallv_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Alltoallv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Alltoallv_init(args) { \
	if (args->MPI_Alltoallv_init.sendcounts != NULL) { \
		args->MPI_Alltoallv_init.sendcounts__ref.val = *args->MPI_Alltoallv_init.sendcounts; \
	} \
	if (args->MPI_Alltoallv_init.sdispls != NULL) { \
		args->MPI_Alltoallv_init.sdispls__ref.val = *args->MPI_Alltoallv_init.sdispls; \
	} \
	if (args->MPI_Alltoallv_init.recvcounts != NULL) { \
		args->MPI_Alltoallv_init.recvcounts__ref.val = *args->MPI_Alltoallv_init.recvcounts; \
	} \
	if (args->MPI_Alltoallv_init.rdispls != NULL) { \
		args->MPI_Alltoallv_init.rdispls__ref.val = *args->MPI_Alltoallv_init.rdispls; \
	} \
	if (args->MPI_Alltoallv_init.request != NULL) { \
		args->MPI_Alltoallv_init.request__ref.val = *args->MPI_Alltoallv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_attach` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_attach` function call.
 *
 * @struct args_MPI_Win_attach_t
 *
 * @note 
 *	int
 *	MPI_Win_attach (
 *			MPI_Win win (struct mpi_win_t *)
 *			void * base (void *)
 *			MPI_Aint size (long)
 *	)
 */
#if HAVE_MPI_Win_attach
struct args_MPI_Win_attach_t {
	MPI_Win win;
	void * base;
	MPI_Aint size;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_attach(activity) { \
	activity->mpi_args.MPI_Win_attach.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_attach.base = (void *) base; \
	activity->mpi_args.MPI_Win_attach.size = (MPI_Aint) size; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_position` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_position` function call.
 *
 * @struct args_MPI_File_get_position_t
 *
 * @note 
 *	int
 *	MPI_File_get_position (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset * offset (long long*)
 *	)
 */
#if HAVE_MPI_File_get_position
struct args_MPI_File_get_position_t {
	MPI_File fh;
	MPI_Offset * offset;
	struct {
		MPI_Offset val;
	} offset__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_position(activity) { \
	activity->mpi_args.MPI_File_get_position.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_position.offset = (MPI_Offset *) offset; \
};

#define GET_PTRS_VALUE_MPI_File_get_position(args) { \
	if (args->MPI_File_get_position.offset != NULL) { \
		args->MPI_File_get_position.offset__ref.val = *args->MPI_File_get_position.offset; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Accumulate` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Accumulate` function call.
 *
 * @struct args_MPI_Accumulate_t
 *
 * @note 
 *	int
 *	MPI_Accumulate (
 *			const void * origin_addr (const void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_count (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Accumulate
struct args_MPI_Accumulate_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_count;
	MPI_Datatype target_datatype;
	MPI_Op op;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Accumulate(activity) { \
	activity->mpi_args.MPI_Accumulate.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Accumulate.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Accumulate.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Accumulate.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Accumulate.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Accumulate.target_count = (int) target_count; \
	activity->mpi_args.MPI_Accumulate.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Accumulate.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Accumulate.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_shared` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_shared` function call.
 *
 * @struct args_MPI_File_write_shared_t
 *
 * @note 
 *	int
 *	MPI_File_write_shared (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_shared
struct args_MPI_File_write_shared_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_shared(activity) { \
	activity->mpi_args.MPI_File_write_shared.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_shared.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_shared.count = (int) count; \
	activity->mpi_args.MPI_File_write_shared.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_write_shared.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_shared(args) { \
	if (args->MPI_File_write_shared.status != NULL) { \
		args->MPI_File_write_shared.status__ref.val = *args->MPI_File_write_shared.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_create_dynamic` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_create_dynamic` function call.
 *
 * @struct args_MPI_Win_create_dynamic_t
 *
 * @note 
 *	int
 *	MPI_Win_create_dynamic (
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Win * win (struct mpi_win_t **)
 *	)
 */
#if HAVE_MPI_Win_create_dynamic
struct args_MPI_Win_create_dynamic_t {
	MPI_Info info;
	MPI_Comm comm;
	MPI_Win * win;
	struct {
		MPI_Win val;
	} win__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_create_dynamic(activity) { \
	activity->mpi_args.MPI_Win_create_dynamic.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Win_create_dynamic.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Win_create_dynamic.win = (MPI_Win *) win; \
};

#define GET_PTRS_VALUE_MPI_Win_create_dynamic(args) { \
	if (args->MPI_Win_create_dynamic.win != NULL) { \
		args->MPI_Win_create_dynamic.win__ref.val = *args->MPI_Win_create_dynamic.win; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_alltoallw` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_alltoallw` function call.
 *
 * @struct args_MPI_Neighbor_alltoallw_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_alltoallw (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const MPI_Aint[] sdispls (const long[])
 *			const MPI_Datatype[] sendtypes (const struct mpi_datatype_t *[])
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const MPI_Aint[] rdispls (const long[])
 *			const MPI_Datatype[] recvtypes (const struct mpi_datatype_t *[])
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Neighbor_alltoallw
struct args_MPI_Neighbor_alltoallw_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	MPI_Aint(* sdispls);
	struct {
		MPI_Aint val;
	} sdispls__ref;
	MPI_Datatype(* sendtypes);
	struct {
		MPI_Datatype val;
	} sendtypes__ref;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	MPI_Aint(* rdispls);
	struct {
		MPI_Aint val;
	} rdispls__ref;
	MPI_Datatype(* recvtypes);
	struct {
		MPI_Datatype val;
	} recvtypes__ref;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_alltoallw(activity) { \
	activity->mpi_args.MPI_Neighbor_alltoallw.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallw.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallw.sdispls = (MPI_Aint(*)) sdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallw.sendtypes = (MPI_Datatype(*)) sendtypes; \
	activity->mpi_args.MPI_Neighbor_alltoallw.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_alltoallw.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Neighbor_alltoallw.rdispls = (MPI_Aint(*)) rdispls; \
	activity->mpi_args.MPI_Neighbor_alltoallw.recvtypes = (MPI_Datatype(*)) recvtypes; \
	activity->mpi_args.MPI_Neighbor_alltoallw.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_alltoallw(args) { \
	if (args->MPI_Neighbor_alltoallw.sendcounts != NULL) { \
		args->MPI_Neighbor_alltoallw.sendcounts__ref.val = *args->MPI_Neighbor_alltoallw.sendcounts; \
	} \
	if (args->MPI_Neighbor_alltoallw.sdispls != NULL) { \
		args->MPI_Neighbor_alltoallw.sdispls__ref.val = *args->MPI_Neighbor_alltoallw.sdispls; \
	} \
	if (args->MPI_Neighbor_alltoallw.sendtypes != NULL) { \
		args->MPI_Neighbor_alltoallw.sendtypes__ref.val = *args->MPI_Neighbor_alltoallw.sendtypes; \
	} \
	if (args->MPI_Neighbor_alltoallw.recvcounts != NULL) { \
		args->MPI_Neighbor_alltoallw.recvcounts__ref.val = *args->MPI_Neighbor_alltoallw.recvcounts; \
	} \
	if (args->MPI_Neighbor_alltoallw.rdispls != NULL) { \
		args->MPI_Neighbor_alltoallw.rdispls__ref.val = *args->MPI_Neighbor_alltoallw.rdispls; \
	} \
	if (args->MPI_Neighbor_alltoallw.recvtypes != NULL) { \
		args->MPI_Neighbor_alltoallw.recvtypes__ref.val = *args->MPI_Neighbor_alltoallw.recvtypes; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iexscan` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iexscan` function call.
 *
 * @struct args_MPI_Iexscan_t
 *
 * @note 
 *	int
 *	MPI_Iexscan (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Iexscan
struct args_MPI_Iexscan_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iexscan(activity) { \
	activity->mpi_args.MPI_Iexscan.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Iexscan.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Iexscan.count = (int) count; \
	activity->mpi_args.MPI_Iexscan.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Iexscan.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Iexscan.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iexscan.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Iexscan(args) { \
	if (args->MPI_Iexscan.request != NULL) { \
		args->MPI_Iexscan.request__ref.val = *args->MPI_Iexscan.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Graph_map` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Graph_map` function call.
 *
 * @struct args_MPI_Graph_map_t
 *
 * @note 
 *	int
 *	MPI_Graph_map (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int nnodes (int)
 *			const int[] index (const int[])
 *			const int[] edges (const int[])
 *			int * newrank (int *)
 *	)
 */
#if HAVE_MPI_Graph_map
struct args_MPI_Graph_map_t {
	MPI_Comm comm;
	int nnodes;
	int(* index);
	struct {
		int val;
	} index__ref;
	int(* edges);
	struct {
		int val;
	} edges__ref;
	int * newrank;
	struct {
		int val;
	} newrank__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Graph_map(activity) { \
	activity->mpi_args.MPI_Graph_map.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Graph_map.nnodes = (int) nnodes; \
	activity->mpi_args.MPI_Graph_map.index = (int(*)) index; \
	activity->mpi_args.MPI_Graph_map.edges = (int(*)) edges; \
	activity->mpi_args.MPI_Graph_map.newrank = (int *) newrank; \
};

#define GET_PTRS_VALUE_MPI_Graph_map(args) { \
	if (args->MPI_Graph_map.index != NULL) { \
		args->MPI_Graph_map.index__ref.val = *args->MPI_Graph_map.index; \
	} \
	if (args->MPI_Graph_map.edges != NULL) { \
		args->MPI_Graph_map.edges__ref.val = *args->MPI_Graph_map.edges; \
	} \
	if (args->MPI_Graph_map.newrank != NULL) { \
		args->MPI_Graph_map.newrank__ref.val = *args->MPI_Graph_map.newrank; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Recv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Recv_init` function call.
 *
 * @struct args_MPI_Recv_init_t
 *
 * @note 
 *	int
 *	MPI_Recv_init (
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Recv_init
struct args_MPI_Recv_init_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int source;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Recv_init(activity) { \
	activity->mpi_args.MPI_Recv_init.buf = (void *) buf; \
	activity->mpi_args.MPI_Recv_init.count = (int) count; \
	activity->mpi_args.MPI_Recv_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Recv_init.source = (int) source; \
	activity->mpi_args.MPI_Recv_init.tag = (int) tag; \
	activity->mpi_args.MPI_Recv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Recv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Recv_init(args) { \
	if (args->MPI_Recv_init.request != NULL) { \
		args->MPI_Recv_init.request__ref.val = *args->MPI_Recv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_subarray` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_subarray` function call.
 *
 * @struct args_MPI_Type_create_subarray_t
 *
 * @note 
 *	int
 *	MPI_Type_create_subarray (
 *			int ndims (int)
 *			const int[] size_array (const int[])
 *			const int[] subsize_array (const int[])
 *			const int[] start_array (const int[])
 *			int order (int)
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_subarray
struct args_MPI_Type_create_subarray_t {
	int ndims;
	int(* size_array);
	struct {
		int val;
	} size_array__ref;
	int(* subsize_array);
	struct {
		int val;
	} subsize_array__ref;
	int(* start_array);
	struct {
		int val;
	} start_array__ref;
	int order;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_subarray(activity) { \
	activity->mpi_args.MPI_Type_create_subarray.ndims = (int) ndims; \
	activity->mpi_args.MPI_Type_create_subarray.size_array = (int(*)) size_array; \
	activity->mpi_args.MPI_Type_create_subarray.subsize_array = (int(*)) subsize_array; \
	activity->mpi_args.MPI_Type_create_subarray.start_array = (int(*)) start_array; \
	activity->mpi_args.MPI_Type_create_subarray.order = (int) order; \
	activity->mpi_args.MPI_Type_create_subarray.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_create_subarray.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_subarray(args) { \
	if (args->MPI_Type_create_subarray.size_array != NULL) { \
		args->MPI_Type_create_subarray.size_array__ref.val = *args->MPI_Type_create_subarray.size_array; \
	} \
	if (args->MPI_Type_create_subarray.subsize_array != NULL) { \
		args->MPI_Type_create_subarray.subsize_array__ref.val = *args->MPI_Type_create_subarray.subsize_array; \
	} \
	if (args->MPI_Type_create_subarray.start_array != NULL) { \
		args->MPI_Type_create_subarray.start_array__ref.val = *args->MPI_Type_create_subarray.start_array; \
	} \
	if (args->MPI_Type_create_subarray.newtype != NULL) { \
		args->MPI_Type_create_subarray.newtype__ref.val = *args->MPI_Type_create_subarray.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_create_group` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_create_group` function call.
 *
 * @struct args_MPI_Comm_create_group_t
 *
 * @note 
 *	int
 *	MPI_Comm_create_group (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Group group (struct mpi_group_t *)
 *			int tag (int)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_create_group
struct args_MPI_Comm_create_group_t {
	MPI_Comm comm;
	MPI_Group group;
	int tag;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_create_group(activity) { \
	activity->mpi_args.MPI_Comm_create_group.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_create_group.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Comm_create_group.tag = (int) tag; \
	activity->mpi_args.MPI_Comm_create_group.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_create_group(args) { \
	if (args->MPI_Comm_create_group.newcomm != NULL) { \
		args->MPI_Comm_create_group.newcomm__ref.val = *args->MPI_Comm_create_group.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Allgather_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Allgather_init` function call.
 *
 * @struct args_MPI_Allgather_init_t
 *
 * @note 
 *	int
 *	MPI_Allgather_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Allgather_init
struct args_MPI_Allgather_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Allgather_init(activity) { \
	activity->mpi_args.MPI_Allgather_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Allgather_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Allgather_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Allgather_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Allgather_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Allgather_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Allgather_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Allgather_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Allgather_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Allgather_init(args) { \
	if (args->MPI_Allgather_init.request != NULL) { \
		args->MPI_Allgather_init.request__ref.val = *args->MPI_Allgather_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Reduce_scatter_block_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Reduce_scatter_block_init` function call.
 *
 * @struct args_MPI_Reduce_scatter_block_init_t
 *
 * @note 
 *	int
 *	MPI_Reduce_scatter_block_init (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Reduce_scatter_block_init
struct args_MPI_Reduce_scatter_block_init_t {
	void * sendbuf;
	void * recvbuf;
	int recvcount;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Reduce_scatter_block_init(activity) { \
	activity->mpi_args.MPI_Reduce_scatter_block_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Reduce_scatter_block_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Reduce_scatter_block_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Reduce_scatter_block_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Reduce_scatter_block_init.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Reduce_scatter_block_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Reduce_scatter_block_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Reduce_scatter_block_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Reduce_scatter_block_init(args) { \
	if (args->MPI_Reduce_scatter_block_init.request != NULL) { \
		args->MPI_Reduce_scatter_block_init.request__ref.val = *args->MPI_Reduce_scatter_block_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_match_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_match_size` function call.
 *
 * @struct args_MPI_Type_match_size_t
 *
 * @note 
 *	int
 *	MPI_Type_match_size (
 *			int typeclass (int)
 *			int size (int)
 *			MPI_Datatype * type (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_match_size
struct args_MPI_Type_match_size_t {
	int typeclass;
	int size;
	MPI_Datatype * type;
	struct {
		MPI_Datatype val;
	} type__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_match_size(activity) { \
	activity->mpi_args.MPI_Type_match_size.typeclass = (int) typeclass; \
	activity->mpi_args.MPI_Type_match_size.size = (int) size; \
	activity->mpi_args.MPI_Type_match_size.type = (MPI_Datatype *) type; \
};

#define GET_PTRS_VALUE_MPI_Type_match_size(args) { \
	if (args->MPI_Type_match_size.type != NULL) { \
		args->MPI_Type_match_size.type__ref.val = *args->MPI_Type_match_size.type; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_true_extent` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_true_extent` function call.
 *
 * @struct args_MPI_Type_get_true_extent_t
 *
 * @note 
 *	int
 *	MPI_Type_get_true_extent (
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Aint * true_lb (long*)
 *			MPI_Aint * true_extent (long*)
 *	)
 */
#if HAVE_MPI_Type_get_true_extent
struct args_MPI_Type_get_true_extent_t {
	MPI_Datatype datatype;
	MPI_Aint * true_lb;
	struct {
		MPI_Aint val;
	} true_lb__ref;
	MPI_Aint * true_extent;
	struct {
		MPI_Aint val;
	} true_extent__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_true_extent(activity) { \
	activity->mpi_args.MPI_Type_get_true_extent.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Type_get_true_extent.true_lb = (MPI_Aint *) true_lb; \
	activity->mpi_args.MPI_Type_get_true_extent.true_extent = (MPI_Aint *) true_extent; \
};

#define GET_PTRS_VALUE_MPI_Type_get_true_extent(args) { \
	if (args->MPI_Type_get_true_extent.true_lb != NULL) { \
		args->MPI_Type_get_true_extent.true_lb__ref.val = *args->MPI_Type_get_true_extent.true_lb; \
	} \
	if (args->MPI_Type_get_true_extent.true_extent != NULL) { \
		args->MPI_Type_get_true_extent.true_extent__ref.val = *args->MPI_Type_get_true_extent.true_extent; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Alltoall_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Alltoall_init` function call.
 *
 * @struct args_MPI_Alltoall_init_t
 *
 * @note 
 *	int
 *	MPI_Alltoall_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Alltoall_init
struct args_MPI_Alltoall_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Alltoall_init(activity) { \
	activity->mpi_args.MPI_Alltoall_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Alltoall_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Alltoall_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Alltoall_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Alltoall_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Alltoall_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Alltoall_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Alltoall_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Alltoall_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Alltoall_init(args) { \
	if (args->MPI_Alltoall_init.request != NULL) { \
		args->MPI_Alltoall_init.request__ref.val = *args->MPI_Alltoall_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Send_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Send_init` function call.
 *
 * @struct args_MPI_Send_init_t
 *
 * @note 
 *	int
 *	MPI_Send_init (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Send_init
struct args_MPI_Send_init_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Send_init(activity) { \
	activity->mpi_args.MPI_Send_init.buf = (void *) buf; \
	activity->mpi_args.MPI_Send_init.count = (int) count; \
	activity->mpi_args.MPI_Send_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Send_init.dest = (int) dest; \
	activity->mpi_args.MPI_Send_init.tag = (int) tag; \
	activity->mpi_args.MPI_Send_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Send_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Send_init(args) { \
	if (args->MPI_Send_init.request != NULL) { \
		args->MPI_Send_init.request__ref.val = *args->MPI_Send_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_allgather_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_allgather_init` function call.
 *
 * @struct args_MPI_Neighbor_allgather_init_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_allgather_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Neighbor_allgather_init
struct args_MPI_Neighbor_allgather_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_allgather_init(activity) { \
	activity->mpi_args.MPI_Neighbor_allgather_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_allgather_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Neighbor_allgather_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_allgather_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_allgather_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Neighbor_allgather_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_allgather_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Neighbor_allgather_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Neighbor_allgather_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_allgather_init(args) { \
	if (args->MPI_Neighbor_allgather_init.request != NULL) { \
		args->MPI_Neighbor_allgather_init.request__ref.val = *args->MPI_Neighbor_allgather_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ibcast` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ibcast` function call.
 *
 * @struct args_MPI_Ibcast_t
 *
 * @note 
 *	int
 *	MPI_Ibcast (
 *			void * buffer (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ibcast
struct args_MPI_Ibcast_t {
	void * buffer;
	int count;
	MPI_Datatype datatype;
	int root;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ibcast(activity) { \
	activity->mpi_args.MPI_Ibcast.buffer = (void *) buffer; \
	activity->mpi_args.MPI_Ibcast.count = (int) count; \
	activity->mpi_args.MPI_Ibcast.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Ibcast.root = (int) root; \
	activity->mpi_args.MPI_Ibcast.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ibcast.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ibcast(args) { \
	if (args->MPI_Ibcast.request != NULL) { \
		args->MPI_Ibcast.request__ref.val = *args->MPI_Ibcast.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iread` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iread` function call.
 *
 * @struct args_MPI_File_iread_t
 *
 * @note 
 *	int
 *	MPI_File_iread (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iread
struct args_MPI_File_iread_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iread(activity) { \
	activity->mpi_args.MPI_File_iread.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iread.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iread.count = (int) count; \
	activity->mpi_args.MPI_File_iread.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iread.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iread(args) { \
	if (args->MPI_File_iread.request != NULL) { \
		args->MPI_File_iread.request__ref.val = *args->MPI_File_iread.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_alltoall_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_alltoall_init` function call.
 *
 * @struct args_MPI_Neighbor_alltoall_init_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_alltoall_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Neighbor_alltoall_init
struct args_MPI_Neighbor_alltoall_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_alltoall_init(activity) { \
	activity->mpi_args.MPI_Neighbor_alltoall_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Neighbor_alltoall_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_alltoall_init(args) { \
	if (args->MPI_Neighbor_alltoall_init.request != NULL) { \
		args->MPI_Neighbor_alltoall_init.request__ref.val = *args->MPI_Neighbor_alltoall_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cart_rank` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cart_rank` function call.
 *
 * @struct args_MPI_Cart_rank_t
 *
 * @note 
 *	int
 *	MPI_Cart_rank (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			const int[] coords (const int[])
 *			int * rank (int *)
 *	)
 */
#if HAVE_MPI_Cart_rank
struct args_MPI_Cart_rank_t {
	MPI_Comm comm;
	int(* coords);
	struct {
		int val;
	} coords__ref;
	int * rank;
	struct {
		int val;
	} rank__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cart_rank(activity) { \
	activity->mpi_args.MPI_Cart_rank.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Cart_rank.coords = (int(*)) coords; \
	activity->mpi_args.MPI_Cart_rank.rank = (int *) rank; \
};

#define GET_PTRS_VALUE_MPI_Cart_rank(args) { \
	if (args->MPI_Cart_rank.coords != NULL) { \
		args->MPI_Cart_rank.coords__ref.val = *args->MPI_Cart_rank.coords; \
	} \
	if (args->MPI_Cart_rank.rank != NULL) { \
		args->MPI_Cart_rank.rank__ref.val = *args->MPI_Cart_rank.rank; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Publish_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Publish_name` function call.
 *
 * @struct args_MPI_Publish_name_t
 *
 * @note 
 *	int
 *	MPI_Publish_name (
 *			const char * service_name (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *			const char * port_name (const char *)
 *	)
 */
#if HAVE_MPI_Publish_name
struct args_MPI_Publish_name_t {
	char * service_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} service_name__ref;
	MPI_Info info;
	char * port_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} port_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Publish_name(activity) { \
	activity->mpi_args.MPI_Publish_name.service_name = (char *) service_name; \
	activity->mpi_args.MPI_Publish_name.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Publish_name.port_name = (char *) port_name; \
};

#define GET_PTRS_VALUE_MPI_Publish_name(args) { \
	if (args->MPI_Publish_name.service_name != NULL) { \
		strncpy(args->MPI_Publish_name.service_name__ref.val, args->MPI_Publish_name.service_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Publish_name.port_name != NULL) { \
		strncpy(args->MPI_Publish_name.port_name__ref.val, args->MPI_Publish_name.port_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_set_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_set_attr` function call.
 *
 * @struct args_MPI_Win_set_attr_t
 *
 * @note 
 *	int
 *	MPI_Win_set_attr (
 *			MPI_Win win (struct mpi_win_t *)
 *			int win_keyval (int)
 *			void * attribute_val (void *)
 *	)
 */
#if HAVE_MPI_Win_set_attr
struct args_MPI_Win_set_attr_t {
	MPI_Win win;
	int win_keyval;
	void * attribute_val;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_set_attr(activity) { \
	activity->mpi_args.MPI_Win_set_attr.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_set_attr.win_keyval = (int) win_keyval; \
	activity->mpi_args.MPI_Win_set_attr.attribute_val = (void *) attribute_val; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_sync` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_sync` function call.
 *
 * @struct args_MPI_Win_sync_t
 *
 * @note 
 *	int
 *	MPI_Win_sync (
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_sync
struct args_MPI_Win_sync_t {
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_sync(activity) { \
	activity->mpi_args.MPI_Win_sync.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_free_keyval` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_free_keyval` function call.
 *
 * @struct args_MPI_Type_free_keyval_t
 *
 * @note 
 *	int
 *	MPI_Type_free_keyval (
 *			int * type_keyval (int *)
 *	)
 */
#if HAVE_MPI_Type_free_keyval
struct args_MPI_Type_free_keyval_t {
	int * type_keyval;
	struct {
		int val;
	} type_keyval__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_free_keyval(activity) { \
	activity->mpi_args.MPI_Type_free_keyval.type_keyval = (int *) type_keyval; \
};

#define GET_PTRS_VALUE_MPI_Type_free_keyval(args) { \
	if (args->MPI_Type_free_keyval.type_keyval != NULL) { \
		args->MPI_Type_free_keyval.type_keyval__ref.val = *args->MPI_Type_free_keyval.type_keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write` function call.
 *
 * @struct args_MPI_File_write_t
 *
 * @note 
 *	int
 *	MPI_File_write (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write
struct args_MPI_File_write_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write(activity) { \
	activity->mpi_args.MPI_File_write.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write.count = (int) count; \
	activity->mpi_args.MPI_File_write.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_write.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write(args) { \
	if (args->MPI_File_write.status != NULL) { \
		args->MPI_File_write.status__ref.val = *args->MPI_File_write.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Register_datarep` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Register_datarep` function call.
 *
 * @struct args_MPI_Register_datarep_t
 *
 * @note 
 *	int
 *	MPI_Register_datarep (
 *			const char * datarep (const char *)
 *			MPI_Datarep_conversion_function * read_conversion_fn (int (*)(void *, struct mpi_datatype_t *, int, void *, long long, void *))
 *			MPI_Datarep_conversion_function * write_conversion_fn (int (*)(void *, struct mpi_datatype_t *, int, void *, long long, void *))
 *			MPI_Datarep_extent_function * dtype_file_extent_fn (int (*)(struct mpi_datatype_t *, long *, void *))
 *			void * extra_state (void *)
 *	)
 */
#if HAVE_MPI_Register_datarep
struct args_MPI_Register_datarep_t {
	char * datarep;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} datarep__ref;
	MPI_Datarep_conversion_function * read_conversion_fn;
	MPI_Datarep_conversion_function * write_conversion_fn;
	MPI_Datarep_extent_function * dtype_file_extent_fn;
	void * extra_state;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Register_datarep(activity) { \
	activity->mpi_args.MPI_Register_datarep.datarep = (char *) datarep; \
	activity->mpi_args.MPI_Register_datarep.read_conversion_fn = (MPI_Datarep_conversion_function *) read_conversion_fn; \
	activity->mpi_args.MPI_Register_datarep.write_conversion_fn = (MPI_Datarep_conversion_function *) write_conversion_fn; \
	activity->mpi_args.MPI_Register_datarep.dtype_file_extent_fn = (MPI_Datarep_extent_function *) dtype_file_extent_fn; \
	activity->mpi_args.MPI_Register_datarep.extra_state = (void *) extra_state; \
};

#define GET_PTRS_VALUE_MPI_Register_datarep(args) { \
	if (args->MPI_Register_datarep.datarep != NULL) { \
		strncpy(args->MPI_Register_datarep.datarep__ref.val, args->MPI_Register_datarep.datarep, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ineighbor_alltoall` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ineighbor_alltoall` function call.
 *
 * @struct args_MPI_Ineighbor_alltoall_t
 *
 * @note 
 *	int
 *	MPI_Ineighbor_alltoall (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ineighbor_alltoall
struct args_MPI_Ineighbor_alltoall_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ineighbor_alltoall(activity) { \
	activity->mpi_args.MPI_Ineighbor_alltoall.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ineighbor_alltoall.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Ineighbor_alltoall.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Ineighbor_alltoall.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ineighbor_alltoall.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Ineighbor_alltoall.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Ineighbor_alltoall.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ineighbor_alltoall.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ineighbor_alltoall(args) { \
	if (args->MPI_Ineighbor_alltoall.request != NULL) { \
		args->MPI_Ineighbor_alltoall.request__ref.val = *args->MPI_Ineighbor_alltoall.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_preallocate` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_preallocate` function call.
 *
 * @struct args_MPI_File_preallocate_t
 *
 * @note 
 *	int
 *	MPI_File_preallocate (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset size (long long)
 *	)
 */
#if HAVE_MPI_File_preallocate
struct args_MPI_File_preallocate_t {
	MPI_File fh;
	MPI_Offset size;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_preallocate(activity) { \
	activity->mpi_args.MPI_File_preallocate.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_preallocate.size = (MPI_Offset) size; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iallgatherv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iallgatherv` function call.
 *
 * @struct args_MPI_Iallgatherv_t
 *
 * @note 
 *	int
 *	MPI_Iallgatherv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Iallgatherv
struct args_MPI_Iallgatherv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iallgatherv(activity) { \
	activity->mpi_args.MPI_Iallgatherv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Iallgatherv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Iallgatherv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Iallgatherv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Iallgatherv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Iallgatherv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Iallgatherv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Iallgatherv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iallgatherv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Iallgatherv(args) { \
	if (args->MPI_Iallgatherv.recvcounts != NULL) { \
		args->MPI_Iallgatherv.recvcounts__ref.val = *args->MPI_Iallgatherv.recvcounts; \
	} \
	if (args->MPI_Iallgatherv.displs != NULL) { \
		args->MPI_Iallgatherv.displs__ref.val = *args->MPI_Iallgatherv.displs; \
	} \
	if (args->MPI_Iallgatherv.request != NULL) { \
		args->MPI_Iallgatherv.request__ref.val = *args->MPI_Iallgatherv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_allgatherv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_allgatherv_init` function call.
 *
 * @struct args_MPI_Neighbor_allgatherv_init_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_allgatherv_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Neighbor_allgatherv_init
struct args_MPI_Neighbor_allgatherv_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_allgatherv_init(activity) { \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Neighbor_allgatherv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_allgatherv_init(args) { \
	if (args->MPI_Neighbor_allgatherv_init.recvcounts != NULL) { \
		args->MPI_Neighbor_allgatherv_init.recvcounts__ref.val = *args->MPI_Neighbor_allgatherv_init.recvcounts; \
	} \
	if (args->MPI_Neighbor_allgatherv_init.displs != NULL) { \
		args->MPI_Neighbor_allgatherv_init.displs__ref.val = *args->MPI_Neighbor_allgatherv_init.displs; \
	} \
	if (args->MPI_Neighbor_allgatherv_init.request != NULL) { \
		args->MPI_Neighbor_allgatherv_init.request__ref.val = *args->MPI_Neighbor_allgatherv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iprobe` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iprobe` function call.
 *
 * @struct args_MPI_Iprobe_t
 *
 * @note 
 *	int
 *	MPI_Iprobe (
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int * flag (int *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Iprobe
struct args_MPI_Iprobe_t {
	int source;
	int tag;
	MPI_Comm comm;
	int * flag;
	struct {
		int val;
	} flag__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iprobe(activity) { \
	activity->mpi_args.MPI_Iprobe.source = (int) source; \
	activity->mpi_args.MPI_Iprobe.tag = (int) tag; \
	activity->mpi_args.MPI_Iprobe.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iprobe.flag = (int *) flag; \
	activity->mpi_args.MPI_Iprobe.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Iprobe(args) { \
	if (args->MPI_Iprobe.flag != NULL) { \
		args->MPI_Iprobe.flag__ref.val = *args->MPI_Iprobe.flag; \
	} \
	if (args->MPI_Iprobe.status != NULL) { \
		args->MPI_Iprobe.status__ref.val = *args->MPI_Iprobe.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_true_extent_x` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_true_extent_x` function call.
 *
 * @struct args_MPI_Type_get_true_extent_x_t
 *
 * @note 
 *	int
 *	MPI_Type_get_true_extent_x (
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Count * true_lb (long long*)
 *			MPI_Count * true_extent (long long*)
 *	)
 */
#if HAVE_MPI_Type_get_true_extent_x
struct args_MPI_Type_get_true_extent_x_t {
	MPI_Datatype datatype;
	MPI_Count * true_lb;
	struct {
		MPI_Count val;
	} true_lb__ref;
	MPI_Count * true_extent;
	struct {
		MPI_Count val;
	} true_extent__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_true_extent_x(activity) { \
	activity->mpi_args.MPI_Type_get_true_extent_x.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Type_get_true_extent_x.true_lb = (MPI_Count *) true_lb; \
	activity->mpi_args.MPI_Type_get_true_extent_x.true_extent = (MPI_Count *) true_extent; \
};

#define GET_PTRS_VALUE_MPI_Type_get_true_extent_x(args) { \
	if (args->MPI_Type_get_true_extent_x.true_lb != NULL) { \
		args->MPI_Type_get_true_extent_x.true_lb__ref.val = *args->MPI_Type_get_true_extent_x.true_lb; \
	} \
	if (args->MPI_Type_get_true_extent_x.true_extent != NULL) { \
		args->MPI_Type_get_true_extent_x.true_extent__ref.val = *args->MPI_Type_get_true_extent_x.true_extent; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_complete` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_complete` function call.
 *
 * @struct args_MPI_Win_complete_t
 *
 * @note 
 *	int
 *	MPI_Win_complete (
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_complete
struct args_MPI_Win_complete_t {
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_complete(activity) { \
	activity->mpi_args.MPI_Win_complete.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_set_atomicity` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_set_atomicity` function call.
 *
 * @struct args_MPI_File_set_atomicity_t
 *
 * @note 
 *	int
 *	MPI_File_set_atomicity (
 *			MPI_File fh (struct mpi_file_t *)
 *			int flag (int)
 *	)
 */
#if HAVE_MPI_File_set_atomicity
struct args_MPI_File_set_atomicity_t {
	MPI_File fh;
	int flag;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_set_atomicity(activity) { \
	activity->mpi_args.MPI_File_set_atomicity.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_set_atomicity.flag = (int) flag; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Unpack_external` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Unpack_external` function call.
 *
 * @struct args_MPI_Unpack_external_t
 *
 * @note 
 *	int
 *	MPI_Unpack_external (
 *			const char[] datarep (const char[])
 *			const void * inbuf (const void *)
 *			MPI_Aint insize (long)
 *			MPI_Aint * position (long*)
 *			void * outbuf (void *)
 *			int outcount (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_Unpack_external
struct args_MPI_Unpack_external_t {
	char(* datarep);
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} datarep__ref;
	void * inbuf;
	MPI_Aint insize;
	MPI_Aint * position;
	struct {
		MPI_Aint val;
	} position__ref;
	void * outbuf;
	int outcount;
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Unpack_external(activity) { \
	activity->mpi_args.MPI_Unpack_external.datarep = (char(*)) datarep; \
	activity->mpi_args.MPI_Unpack_external.inbuf = (void *) inbuf; \
	activity->mpi_args.MPI_Unpack_external.insize = (MPI_Aint) insize; \
	activity->mpi_args.MPI_Unpack_external.position = (MPI_Aint *) position; \
	activity->mpi_args.MPI_Unpack_external.outbuf = (void *) outbuf; \
	activity->mpi_args.MPI_Unpack_external.outcount = (int) outcount; \
	activity->mpi_args.MPI_Unpack_external.datatype = (MPI_Datatype) datatype; \
};

#define GET_PTRS_VALUE_MPI_Unpack_external(args) { \
	if (args->MPI_Unpack_external.datarep != NULL) { \
		strncpy(args->MPI_Unpack_external.datarep__ref.val, args->MPI_Unpack_external.datarep, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Unpack_external.position != NULL) { \
		args->MPI_Unpack_external.position__ref.val = *args->MPI_Unpack_external.position; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Mprobe` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Mprobe` function call.
 *
 * @struct args_MPI_Mprobe_t
 *
 * @note 
 *	int
 *	MPI_Mprobe (
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Message * message (struct mpi_message_t **)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Mprobe
struct args_MPI_Mprobe_t {
	int source;
	int tag;
	MPI_Comm comm;
	MPI_Message * message;
	struct {
		MPI_Message val;
	} message__ref;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Mprobe(activity) { \
	activity->mpi_args.MPI_Mprobe.source = (int) source; \
	activity->mpi_args.MPI_Mprobe.tag = (int) tag; \
	activity->mpi_args.MPI_Mprobe.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Mprobe.message = (MPI_Message *) message; \
	activity->mpi_args.MPI_Mprobe.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_Mprobe(args) { \
	if (args->MPI_Mprobe.message != NULL) { \
		args->MPI_Mprobe.message__ref.val = *args->MPI_Mprobe.message; \
	} \
	if (args->MPI_Mprobe.status != NULL) { \
		args->MPI_Mprobe.status__ref.val = *args->MPI_Mprobe.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Add_error_code` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Add_error_code` function call.
 *
 * @struct args_MPI_Add_error_code_t
 *
 * @note 
 *	int
 *	MPI_Add_error_code (
 *			int errorclass (int)
 *			int * errorcode (int *)
 *	)
 */
#if HAVE_MPI_Add_error_code
struct args_MPI_Add_error_code_t {
	int errorclass;
	int * errorcode;
	struct {
		int val;
	} errorcode__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Add_error_code(activity) { \
	activity->mpi_args.MPI_Add_error_code.errorclass = (int) errorclass; \
	activity->mpi_args.MPI_Add_error_code.errorcode = (int *) errorcode; \
};

#define GET_PTRS_VALUE_MPI_Add_error_code(args) { \
	if (args->MPI_Add_error_code.errorcode != NULL) { \
		args->MPI_Add_error_code.errorcode__ref.val = *args->MPI_Add_error_code.errorcode; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_delete_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_delete_attr` function call.
 *
 * @struct args_MPI_Win_delete_attr_t
 *
 * @note 
 *	int
 *	MPI_Win_delete_attr (
 *			MPI_Win win (struct mpi_win_t *)
 *			int win_keyval (int)
 *	)
 */
#if HAVE_MPI_Win_delete_attr
struct args_MPI_Win_delete_attr_t {
	MPI_Win win;
	int win_keyval;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_delete_attr(activity) { \
	activity->mpi_args.MPI_Win_delete_attr.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_delete_attr.win_keyval = (int) win_keyval; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_at_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_at_all` function call.
 *
 * @struct args_MPI_File_read_at_all_t
 *
 * @note 
 *	int
 *	MPI_File_read_at_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_at_all
struct args_MPI_File_read_at_all_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_at_all(activity) { \
	activity->mpi_args.MPI_File_read_at_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_at_all.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_read_at_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_at_all.count = (int) count; \
	activity->mpi_args.MPI_File_read_at_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_read_at_all.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_at_all(args) { \
	if (args->MPI_File_read_at_all.status != NULL) { \
		args->MPI_File_read_at_all.status__ref.val = *args->MPI_File_read_at_all.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Pready` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Pready` function call.
 *
 * @struct args_MPI_Pready_t
 *
 * @note 
 *	int
 *	MPI_Pready (
 *			int partitions (int)
 *			MPI_Request request (struct mpi_request_t *)
 *	)
 */
#if HAVE_MPI_Pready
struct args_MPI_Pready_t {
	int partitions;
	MPI_Request request;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Pready(activity) { \
	activity->mpi_args.MPI_Pready.partitions = (int) partitions; \
	activity->mpi_args.MPI_Pready.request = (MPI_Request) request; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iscatterv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iscatterv` function call.
 *
 * @struct args_MPI_Iscatterv_t
 *
 * @note 
 *	int
 *	MPI_Iscatterv (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Iscatterv
struct args_MPI_Iscatterv_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iscatterv(activity) { \
	activity->mpi_args.MPI_Iscatterv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Iscatterv.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Iscatterv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Iscatterv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Iscatterv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Iscatterv.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Iscatterv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Iscatterv.root = (int) root; \
	activity->mpi_args.MPI_Iscatterv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iscatterv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Iscatterv(args) { \
	if (args->MPI_Iscatterv.sendcounts != NULL) { \
		args->MPI_Iscatterv.sendcounts__ref.val = *args->MPI_Iscatterv.sendcounts; \
	} \
	if (args->MPI_Iscatterv.displs != NULL) { \
		args->MPI_Iscatterv.displs__ref.val = *args->MPI_Iscatterv.displs; \
	} \
	if (args->MPI_Iscatterv.request != NULL) { \
		args->MPI_Iscatterv.request__ref.val = *args->MPI_Iscatterv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_detach` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_detach` function call.
 *
 * @struct args_MPI_Win_detach_t
 *
 * @note 
 *	int
 *	MPI_Win_detach (
 *			MPI_Win win (struct mpi_win_t *)
 *			const void * base (const void *)
 *	)
 */
#if HAVE_MPI_Win_detach
struct args_MPI_Win_detach_t {
	MPI_Win win;
	void * base;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_detach(activity) { \
	activity->mpi_args.MPI_Win_detach.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_detach.base = (void *) base; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_call_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_call_errhandler` function call.
 *
 * @struct args_MPI_File_call_errhandler_t
 *
 * @note 
 *	int
 *	MPI_File_call_errhandler (
 *			MPI_File fh (struct mpi_file_t *)
 *			int errorcode (int)
 *	)
 */
#if HAVE_MPI_File_call_errhandler
struct args_MPI_File_call_errhandler_t {
	MPI_File fh;
	int errorcode;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_call_errhandler(activity) { \
	activity->mpi_args.MPI_File_call_errhandler.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_call_errhandler.errorcode = (int) errorcode; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iallreduce` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iallreduce` function call.
 *
 * @struct args_MPI_Iallreduce_t
 *
 * @note 
 *	int
 *	MPI_Iallreduce (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Iallreduce
struct args_MPI_Iallreduce_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iallreduce(activity) { \
	activity->mpi_args.MPI_Iallreduce.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Iallreduce.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Iallreduce.count = (int) count; \
	activity->mpi_args.MPI_Iallreduce.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Iallreduce.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Iallreduce.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iallreduce.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Iallreduce(args) { \
	if (args->MPI_Iallreduce.request != NULL) { \
		args->MPI_Iallreduce.request__ref.val = *args->MPI_Iallreduce.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_processor_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_processor_name` function call.
 *
 * @struct args_MPI_Get_processor_name_t
 *
 * @note 
 *	int
 *	MPI_Get_processor_name (
 *			char * name (char *)
 *			int * resultlen (int *)
 *	)
 */
#if HAVE_MPI_Get_processor_name
struct args_MPI_Get_processor_name_t {
	char * name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} name__ref;
	int * resultlen;
	struct {
		int val;
	} resultlen__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_processor_name(activity) { \
	activity->mpi_args.MPI_Get_processor_name.name = (char *) name; \
	activity->mpi_args.MPI_Get_processor_name.resultlen = (int *) resultlen; \
};

#define GET_PTRS_VALUE_MPI_Get_processor_name(args) { \
	if (args->MPI_Get_processor_name.name != NULL) { \
		strncpy(args->MPI_Get_processor_name.name__ref.val, args->MPI_Get_processor_name.name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Get_processor_name.resultlen != NULL) { \
		args->MPI_Get_processor_name.resultlen__ref.val = *args->MPI_Get_processor_name.resultlen; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Start` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Start` function call.
 *
 * @struct args_MPI_Start_t
 *
 * @note 
 *	int
 *	MPI_Start (
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Start
struct args_MPI_Start_t {
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Start(activity) { \
	activity->mpi_args.MPI_Start.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Start(args) { \
	if (args->MPI_Start.request != NULL) { \
		args->MPI_Start.request__ref.val = *args->MPI_Start.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_type_extent` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_type_extent` function call.
 *
 * @struct args_MPI_File_get_type_extent_t
 *
 * @note 
 *	int
 *	MPI_File_get_type_extent (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Aint * extent (long*)
 *	)
 */
#if HAVE_MPI_File_get_type_extent
struct args_MPI_File_get_type_extent_t {
	MPI_File fh;
	MPI_Datatype datatype;
	MPI_Aint * extent;
	struct {
		MPI_Aint val;
	} extent__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_type_extent(activity) { \
	activity->mpi_args.MPI_File_get_type_extent.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_type_extent.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_get_type_extent.extent = (MPI_Aint *) extent; \
};

#define GET_PTRS_VALUE_MPI_File_get_type_extent(args) { \
	if (args->MPI_File_get_type_extent.extent != NULL) { \
		args->MPI_File_get_type_extent.extent__ref.val = *args->MPI_File_get_type_extent.extent; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_shared` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_shared` function call.
 *
 * @struct args_MPI_File_read_shared_t
 *
 * @note 
 *	int
 *	MPI_File_read_shared (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_read_shared
struct args_MPI_File_read_shared_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_shared(activity) { \
	activity->mpi_args.MPI_File_read_shared.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_shared.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_shared.count = (int) count; \
	activity->mpi_args.MPI_File_read_shared.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_read_shared.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_read_shared(args) { \
	if (args->MPI_File_read_shared.status != NULL) { \
		args->MPI_File_read_shared.status__ref.val = *args->MPI_File_read_shared.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_open` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_open` function call.
 *
 * @struct args_MPI_File_open_t
 *
 * @note 
 *	int
 *	MPI_File_open (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			const char * filename (const char *)
 *			int amode (int)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_File * fh (struct mpi_file_t **)
 *	)
 */
#if HAVE_MPI_File_open
struct args_MPI_File_open_t {
	MPI_Comm comm;
	char * filename;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} filename__ref;
	int amode;
	MPI_Info info;
	MPI_File * fh;
	struct {
		MPI_File val;
	} fh__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_open(activity) { \
	activity->mpi_args.MPI_File_open.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_File_open.filename = (char *) filename; \
	activity->mpi_args.MPI_File_open.amode = (int) amode; \
	activity->mpi_args.MPI_File_open.info = (MPI_Info) info; \
	activity->mpi_args.MPI_File_open.fh = (MPI_File *) fh; \
};

#define GET_PTRS_VALUE_MPI_File_open(args) { \
	if (args->MPI_File_open.filename != NULL) { \
		strncpy(args->MPI_File_open.filename__ref.val, args->MPI_File_open.filename, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_File_open.fh != NULL) { \
		args->MPI_File_open.fh__ref.val = *args->MPI_File_open.fh; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_amode` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_amode` function call.
 *
 * @struct args_MPI_File_get_amode_t
 *
 * @note 
 *	int
 *	MPI_File_get_amode (
 *			MPI_File fh (struct mpi_file_t *)
 *			int * amode (int *)
 *	)
 */
#if HAVE_MPI_File_get_amode
struct args_MPI_File_get_amode_t {
	MPI_File fh;
	int * amode;
	struct {
		int val;
	} amode__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_amode(activity) { \
	activity->mpi_args.MPI_File_get_amode.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_amode.amode = (int *) amode; \
};

#define GET_PTRS_VALUE_MPI_File_get_amode(args) { \
	if (args->MPI_File_get_amode.amode != NULL) { \
		args->MPI_File_get_amode.amode__ref.val = *args->MPI_File_get_amode.amode; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_hindexed_block` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_hindexed_block` function call.
 *
 * @struct args_MPI_Type_create_hindexed_block_t
 *
 * @note 
 *	int
 *	MPI_Type_create_hindexed_block (
 *			int count (int)
 *			int blocklength (int)
 *			const MPI_Aint[] array_of_displacements (const long[])
 *			MPI_Datatype oldtype (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_create_hindexed_block
struct args_MPI_Type_create_hindexed_block_t {
	int count;
	int blocklength;
	MPI_Aint(* array_of_displacements);
	struct {
		MPI_Aint val;
	} array_of_displacements__ref;
	MPI_Datatype oldtype;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_hindexed_block(activity) { \
	activity->mpi_args.MPI_Type_create_hindexed_block.count = (int) count; \
	activity->mpi_args.MPI_Type_create_hindexed_block.blocklength = (int) blocklength; \
	activity->mpi_args.MPI_Type_create_hindexed_block.array_of_displacements = (MPI_Aint(*)) array_of_displacements; \
	activity->mpi_args.MPI_Type_create_hindexed_block.oldtype = (MPI_Datatype) oldtype; \
	activity->mpi_args.MPI_Type_create_hindexed_block.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_create_hindexed_block(args) { \
	if (args->MPI_Type_create_hindexed_block.array_of_displacements != NULL) { \
		args->MPI_Type_create_hindexed_block.array_of_displacements__ref.val = *args->MPI_Type_create_hindexed_block.array_of_displacements; \
	} \
	if (args->MPI_Type_create_hindexed_block.newtype != NULL) { \
		args->MPI_Type_create_hindexed_block.newtype__ref.val = *args->MPI_Type_create_hindexed_block.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cart_coords` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cart_coords` function call.
 *
 * @struct args_MPI_Cart_coords_t
 *
 * @note 
 *	int
 *	MPI_Cart_coords (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int rank (int)
 *			int maxdims (int)
 *			int[] coords (int[])
 *	)
 */
#if HAVE_MPI_Cart_coords
struct args_MPI_Cart_coords_t {
	MPI_Comm comm;
	int rank;
	int maxdims;
	int(* coords);
	struct {
		int val;
	} coords__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cart_coords(activity) { \
	activity->mpi_args.MPI_Cart_coords.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Cart_coords.rank = (int) rank; \
	activity->mpi_args.MPI_Cart_coords.maxdims = (int) maxdims; \
	activity->mpi_args.MPI_Cart_coords.coords = (int(*)) coords; \
};

#define GET_PTRS_VALUE_MPI_Cart_coords(args) { \
	if (args->MPI_Cart_coords.coords != NULL) { \
		args->MPI_Cart_coords.coords__ref.val = *args->MPI_Cart_coords.coords; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Issend` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Issend` function call.
 *
 * @struct args_MPI_Issend_t
 *
 * @note 
 *	int
 *	MPI_Issend (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Issend
struct args_MPI_Issend_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Issend(activity) { \
	activity->mpi_args.MPI_Issend.buf = (void *) buf; \
	activity->mpi_args.MPI_Issend.count = (int) count; \
	activity->mpi_args.MPI_Issend.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Issend.dest = (int) dest; \
	activity->mpi_args.MPI_Issend.tag = (int) tag; \
	activity->mpi_args.MPI_Issend.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Issend.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Issend(args) { \
	if (args->MPI_Issend.request != NULL) { \
		args->MPI_Issend.request__ref.val = *args->MPI_Issend.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Graph_get` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Graph_get` function call.
 *
 * @struct args_MPI_Graph_get_t
 *
 * @note 
 *	int
 *	MPI_Graph_get (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int maxindex (int)
 *			int maxedges (int)
 *			int[] index (int[])
 *			int[] edges (int[])
 *	)
 */
#if HAVE_MPI_Graph_get
struct args_MPI_Graph_get_t {
	MPI_Comm comm;
	int maxindex;
	int maxedges;
	int(* index);
	struct {
		int val;
	} index__ref;
	int(* edges);
	struct {
		int val;
	} edges__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Graph_get(activity) { \
	activity->mpi_args.MPI_Graph_get.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Graph_get.maxindex = (int) maxindex; \
	activity->mpi_args.MPI_Graph_get.maxedges = (int) maxedges; \
	activity->mpi_args.MPI_Graph_get.index = (int(*)) index; \
	activity->mpi_args.MPI_Graph_get.edges = (int(*)) edges; \
};

#define GET_PTRS_VALUE_MPI_Graph_get(args) { \
	if (args->MPI_Graph_get.index != NULL) { \
		args->MPI_Graph_get.index__ref.val = *args->MPI_Graph_get.index; \
	} \
	if (args->MPI_Graph_get.edges != NULL) { \
		args->MPI_Graph_get.edges__ref.val = *args->MPI_Graph_get.edges; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_free_keyval` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_free_keyval` function call.
 *
 * @struct args_MPI_Win_free_keyval_t
 *
 * @note 
 *	int
 *	MPI_Win_free_keyval (
 *			int * win_keyval (int *)
 *	)
 */
#if HAVE_MPI_Win_free_keyval
struct args_MPI_Win_free_keyval_t {
	int * win_keyval;
	struct {
		int val;
	} win_keyval__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_free_keyval(activity) { \
	activity->mpi_args.MPI_Win_free_keyval.win_keyval = (int *) win_keyval; \
};

#define GET_PTRS_VALUE_MPI_Win_free_keyval(args) { \
	if (args->MPI_Win_free_keyval.win_keyval != NULL) { \
		args->MPI_Win_free_keyval.win_keyval__ref.val = *args->MPI_Win_free_keyval.win_keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ineighbor_alltoallw` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ineighbor_alltoallw` function call.
 *
 * @struct args_MPI_Ineighbor_alltoallw_t
 *
 * @note 
 *	int
 *	MPI_Ineighbor_alltoallw (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const MPI_Aint[] sdispls (const long[])
 *			const MPI_Datatype[] sendtypes (const struct mpi_datatype_t *[])
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const MPI_Aint[] rdispls (const long[])
 *			const MPI_Datatype[] recvtypes (const struct mpi_datatype_t *[])
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ineighbor_alltoallw
struct args_MPI_Ineighbor_alltoallw_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	MPI_Aint(* sdispls);
	struct {
		MPI_Aint val;
	} sdispls__ref;
	MPI_Datatype(* sendtypes);
	struct {
		MPI_Datatype val;
	} sendtypes__ref;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	MPI_Aint(* rdispls);
	struct {
		MPI_Aint val;
	} rdispls__ref;
	MPI_Datatype(* recvtypes);
	struct {
		MPI_Datatype val;
	} recvtypes__ref;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ineighbor_alltoallw(activity) { \
	activity->mpi_args.MPI_Ineighbor_alltoallw.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.sdispls = (MPI_Aint(*)) sdispls; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.sendtypes = (MPI_Datatype(*)) sendtypes; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.rdispls = (MPI_Aint(*)) rdispls; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.recvtypes = (MPI_Datatype(*)) recvtypes; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ineighbor_alltoallw.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ineighbor_alltoallw(args) { \
	if (args->MPI_Ineighbor_alltoallw.sendcounts != NULL) { \
		args->MPI_Ineighbor_alltoallw.sendcounts__ref.val = *args->MPI_Ineighbor_alltoallw.sendcounts; \
	} \
	if (args->MPI_Ineighbor_alltoallw.sdispls != NULL) { \
		args->MPI_Ineighbor_alltoallw.sdispls__ref.val = *args->MPI_Ineighbor_alltoallw.sdispls; \
	} \
	if (args->MPI_Ineighbor_alltoallw.sendtypes != NULL) { \
		args->MPI_Ineighbor_alltoallw.sendtypes__ref.val = *args->MPI_Ineighbor_alltoallw.sendtypes; \
	} \
	if (args->MPI_Ineighbor_alltoallw.recvcounts != NULL) { \
		args->MPI_Ineighbor_alltoallw.recvcounts__ref.val = *args->MPI_Ineighbor_alltoallw.recvcounts; \
	} \
	if (args->MPI_Ineighbor_alltoallw.rdispls != NULL) { \
		args->MPI_Ineighbor_alltoallw.rdispls__ref.val = *args->MPI_Ineighbor_alltoallw.rdispls; \
	} \
	if (args->MPI_Ineighbor_alltoallw.recvtypes != NULL) { \
		args->MPI_Ineighbor_alltoallw.recvtypes__ref.val = *args->MPI_Ineighbor_alltoallw.recvtypes; \
	} \
	if (args->MPI_Ineighbor_alltoallw.request != NULL) { \
		args->MPI_Ineighbor_alltoallw.request__ref.val = *args->MPI_Ineighbor_alltoallw.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_set_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_set_info` function call.
 *
 * @struct args_MPI_File_set_info_t
 *
 * @note 
 *	int
 *	MPI_File_set_info (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *	)
 */
#if HAVE_MPI_File_set_info
struct args_MPI_File_set_info_t {
	MPI_File fh;
	MPI_Info info;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_set_info(activity) { \
	activity->mpi_args.MPI_File_set_info.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_set_info.info = (MPI_Info) info; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iread_at` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iread_at` function call.
 *
 * @struct args_MPI_File_iread_at_t
 *
 * @note 
 *	int
 *	MPI_File_iread_at (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iread_at
struct args_MPI_File_iread_at_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iread_at(activity) { \
	activity->mpi_args.MPI_File_iread_at.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iread_at.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_iread_at.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iread_at.count = (int) count; \
	activity->mpi_args.MPI_File_iread_at.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iread_at.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iread_at(args) { \
	if (args->MPI_File_iread_at.request != NULL) { \
		args->MPI_File_iread_at.request__ref.val = *args->MPI_File_iread_at.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Attr_delete` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Attr_delete` function call.
 *
 * @struct args_MPI_Attr_delete_t
 *
 * @note 
 *	int
 *	MPI_Attr_delete (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int keyval (int)
 *	)
 */
#if HAVE_MPI_Attr_delete
struct args_MPI_Attr_delete_t {
	MPI_Comm comm;
	int keyval;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Attr_delete(activity) { \
	activity->mpi_args.MPI_Attr_delete.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Attr_delete.keyval = (int) keyval; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_get_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_get_info` function call.
 *
 * @struct args_MPI_Session_get_info_t
 *
 * @note 
 *	int
 *	MPI_Session_get_info (
 *			MPI_Session session (struct mpi_instance_t *)
 *			MPI_Info * info_used (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Session_get_info
struct args_MPI_Session_get_info_t {
	MPI_Session session;
	MPI_Info * info_used;
	struct {
		MPI_Info val;
	} info_used__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_get_info(activity) { \
	activity->mpi_args.MPI_Session_get_info.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_get_info.info_used = (MPI_Info *) info_used; \
};

#define GET_PTRS_VALUE_MPI_Session_get_info(args) { \
	if (args->MPI_Session_get_info.info_used != NULL) { \
		args->MPI_Session_get_info.info_used__ref.val = *args->MPI_Session_get_info.info_used; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_get_nth_pset` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_get_nth_pset` function call.
 *
 * @struct args_MPI_Session_get_nth_pset_t
 *
 * @note 
 *	int
 *	MPI_Session_get_nth_pset (
 *			MPI_Session session (struct mpi_instance_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			int n (int)
 *			int * len (int *)
 *			char * pset_name (char *)
 *	)
 */
#if HAVE_MPI_Session_get_nth_pset
struct args_MPI_Session_get_nth_pset_t {
	MPI_Session session;
	MPI_Info info;
	int n;
	int * len;
	struct {
		int val;
	} len__ref;
	char * pset_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} pset_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_get_nth_pset(activity) { \
	activity->mpi_args.MPI_Session_get_nth_pset.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_get_nth_pset.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Session_get_nth_pset.n = (int) n; \
	activity->mpi_args.MPI_Session_get_nth_pset.len = (int *) len; \
	activity->mpi_args.MPI_Session_get_nth_pset.pset_name = (char *) pset_name; \
};

#define GET_PTRS_VALUE_MPI_Session_get_nth_pset(args) { \
	if (args->MPI_Session_get_nth_pset.len != NULL) { \
		args->MPI_Session_get_nth_pset.len__ref.val = *args->MPI_Session_get_nth_pset.len; \
	} \
	if (args->MPI_Session_get_nth_pset.pset_name != NULL) { \
		strncpy(args->MPI_Session_get_nth_pset.pset_name__ref.val, args->MPI_Session_get_nth_pset.pset_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_create_keyval` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_create_keyval` function call.
 *
 * @struct args_MPI_Type_create_keyval_t
 *
 * @note 
 *	int
 *	MPI_Type_create_keyval (
 *			MPI_Type_copy_attr_function * type_copy_attr_fn (int (*)(struct mpi_datatype_t *, int, void *, void *, void *, int *))
 *			MPI_Type_delete_attr_function * type_delete_attr_fn (int (*)(struct mpi_datatype_t *, int, void *, void *))
 *			int * type_keyval (int *)
 *			void * extra_state (void *)
 *	)
 */
#if HAVE_MPI_Type_create_keyval
struct args_MPI_Type_create_keyval_t {
	MPI_Type_copy_attr_function * type_copy_attr_fn;
	MPI_Type_delete_attr_function * type_delete_attr_fn;
	int * type_keyval;
	struct {
		int val;
	} type_keyval__ref;
	void * extra_state;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_create_keyval(activity) { \
	activity->mpi_args.MPI_Type_create_keyval.type_copy_attr_fn = (MPI_Type_copy_attr_function *) type_copy_attr_fn; \
	activity->mpi_args.MPI_Type_create_keyval.type_delete_attr_fn = (MPI_Type_delete_attr_function *) type_delete_attr_fn; \
	activity->mpi_args.MPI_Type_create_keyval.type_keyval = (int *) type_keyval; \
	activity->mpi_args.MPI_Type_create_keyval.extra_state = (void *) extra_state; \
};

#define GET_PTRS_VALUE_MPI_Type_create_keyval(args) { \
	if (args->MPI_Type_create_keyval.type_keyval != NULL) { \
		args->MPI_Type_create_keyval.type_keyval__ref.val = *args->MPI_Type_create_keyval.type_keyval; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Attr_get` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Attr_get` function call.
 *
 * @struct args_MPI_Attr_get_t
 *
 * @note 
 *	int
 *	MPI_Attr_get (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int keyval (int)
 *			void * attribute_val (void *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Attr_get
struct args_MPI_Attr_get_t {
	MPI_Comm comm;
	int keyval;
	void * attribute_val;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Attr_get(activity) { \
	activity->mpi_args.MPI_Attr_get.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Attr_get.keyval = (int) keyval; \
	activity->mpi_args.MPI_Attr_get.attribute_val = (void *) attribute_val; \
	activity->mpi_args.MPI_Attr_get.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Attr_get(args) { \
	if (args->MPI_Attr_get.flag != NULL) { \
		args->MPI_Attr_get.flag__ref.val = *args->MPI_Attr_get.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Add_error_string` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Add_error_string` function call.
 *
 * @struct args_MPI_Add_error_string_t
 *
 * @note 
 *	int
 *	MPI_Add_error_string (
 *			int errorcode (int)
 *			const char * string (const char *)
 *	)
 */
#if HAVE_MPI_Add_error_string
struct args_MPI_Add_error_string_t {
	int errorcode;
	char * string;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} string__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Add_error_string(activity) { \
	activity->mpi_args.MPI_Add_error_string.errorcode = (int) errorcode; \
	activity->mpi_args.MPI_Add_error_string.string = (char *) string; \
};

#define GET_PTRS_VALUE_MPI_Add_error_string(args) { \
	if (args->MPI_Add_error_string.string != NULL) { \
		strncpy(args->MPI_Add_error_string.string__ref.val, args->MPI_Add_error_string.string, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ineighbor_alltoallv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ineighbor_alltoallv` function call.
 *
 * @struct args_MPI_Ineighbor_alltoallv_t
 *
 * @note 
 *	int
 *	MPI_Ineighbor_alltoallv (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ineighbor_alltoallv
struct args_MPI_Ineighbor_alltoallv_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ineighbor_alltoallv(activity) { \
	activity->mpi_args.MPI_Ineighbor_alltoallv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ineighbor_alltoallv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ineighbor_alltoallv(args) { \
	if (args->MPI_Ineighbor_alltoallv.sendcounts != NULL) { \
		args->MPI_Ineighbor_alltoallv.sendcounts__ref.val = *args->MPI_Ineighbor_alltoallv.sendcounts; \
	} \
	if (args->MPI_Ineighbor_alltoallv.sdispls != NULL) { \
		args->MPI_Ineighbor_alltoallv.sdispls__ref.val = *args->MPI_Ineighbor_alltoallv.sdispls; \
	} \
	if (args->MPI_Ineighbor_alltoallv.recvcounts != NULL) { \
		args->MPI_Ineighbor_alltoallv.recvcounts__ref.val = *args->MPI_Ineighbor_alltoallv.recvcounts; \
	} \
	if (args->MPI_Ineighbor_alltoallv.rdispls != NULL) { \
		args->MPI_Ineighbor_alltoallv.rdispls__ref.val = *args->MPI_Ineighbor_alltoallv.rdispls; \
	} \
	if (args->MPI_Ineighbor_alltoallv.request != NULL) { \
		args->MPI_Ineighbor_alltoallv.request__ref.val = *args->MPI_Ineighbor_alltoallv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Imrecv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Imrecv` function call.
 *
 * @struct args_MPI_Imrecv_t
 *
 * @note 
 *	int
 *	MPI_Imrecv (
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			MPI_Message * message (struct mpi_message_t **)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Imrecv
struct args_MPI_Imrecv_t {
	void * buf;
	int count;
	MPI_Datatype type;
	MPI_Message * message;
	struct {
		MPI_Message val;
	} message__ref;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Imrecv(activity) { \
	activity->mpi_args.MPI_Imrecv.buf = (void *) buf; \
	activity->mpi_args.MPI_Imrecv.count = (int) count; \
	activity->mpi_args.MPI_Imrecv.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Imrecv.message = (MPI_Message *) message; \
	activity->mpi_args.MPI_Imrecv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Imrecv(args) { \
	if (args->MPI_Imrecv.message != NULL) { \
		args->MPI_Imrecv.message__ref.val = *args->MPI_Imrecv.message; \
	} \
	if (args->MPI_Imrecv.request != NULL) { \
		args->MPI_Imrecv.request__ref.val = *args->MPI_Imrecv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Alltoallw` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Alltoallw` function call.
 *
 * @struct args_MPI_Alltoallw_t
 *
 * @note 
 *	int
 *	MPI_Alltoallw (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			const MPI_Datatype[] sendtypes (const struct mpi_datatype_t *[])
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			const MPI_Datatype[] recvtypes (const struct mpi_datatype_t *[])
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Alltoallw
struct args_MPI_Alltoallw_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype(* sendtypes);
	struct {
		MPI_Datatype val;
	} sendtypes__ref;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype(* recvtypes);
	struct {
		MPI_Datatype val;
	} recvtypes__ref;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Alltoallw(activity) { \
	activity->mpi_args.MPI_Alltoallw.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Alltoallw.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Alltoallw.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Alltoallw.sendtypes = (MPI_Datatype(*)) sendtypes; \
	activity->mpi_args.MPI_Alltoallw.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Alltoallw.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Alltoallw.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Alltoallw.recvtypes = (MPI_Datatype(*)) recvtypes; \
	activity->mpi_args.MPI_Alltoallw.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Alltoallw(args) { \
	if (args->MPI_Alltoallw.sendcounts != NULL) { \
		args->MPI_Alltoallw.sendcounts__ref.val = *args->MPI_Alltoallw.sendcounts; \
	} \
	if (args->MPI_Alltoallw.sdispls != NULL) { \
		args->MPI_Alltoallw.sdispls__ref.val = *args->MPI_Alltoallw.sdispls; \
	} \
	if (args->MPI_Alltoallw.sendtypes != NULL) { \
		args->MPI_Alltoallw.sendtypes__ref.val = *args->MPI_Alltoallw.sendtypes; \
	} \
	if (args->MPI_Alltoallw.recvcounts != NULL) { \
		args->MPI_Alltoallw.recvcounts__ref.val = *args->MPI_Alltoallw.recvcounts; \
	} \
	if (args->MPI_Alltoallw.rdispls != NULL) { \
		args->MPI_Alltoallw.rdispls__ref.val = *args->MPI_Alltoallw.rdispls; \
	} \
	if (args->MPI_Alltoallw.recvtypes != NULL) { \
		args->MPI_Alltoallw.recvtypes__ref.val = *args->MPI_Alltoallw.recvtypes; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Bcast_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Bcast_init` function call.
 *
 * @struct args_MPI_Bcast_init_t
 *
 * @note 
 *	int
 *	MPI_Bcast_init (
 *			void * buffer (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Bcast_init
struct args_MPI_Bcast_init_t {
	void * buffer;
	int count;
	MPI_Datatype datatype;
	int root;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Bcast_init(activity) { \
	activity->mpi_args.MPI_Bcast_init.buffer = (void *) buffer; \
	activity->mpi_args.MPI_Bcast_init.count = (int) count; \
	activity->mpi_args.MPI_Bcast_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Bcast_init.root = (int) root; \
	activity->mpi_args.MPI_Bcast_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Bcast_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Bcast_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Bcast_init(args) { \
	if (args->MPI_Bcast_init.request != NULL) { \
		args->MPI_Bcast_init.request__ref.val = *args->MPI_Bcast_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ibarrier` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ibarrier` function call.
 *
 * @struct args_MPI_Ibarrier_t
 *
 * @note 
 *	int
 *	MPI_Ibarrier (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ibarrier
struct args_MPI_Ibarrier_t {
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ibarrier(activity) { \
	activity->mpi_args.MPI_Ibarrier.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ibarrier.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ibarrier(args) { \
	if (args->MPI_Ibarrier.request != NULL) { \
		args->MPI_Ibarrier.request__ref.val = *args->MPI_Ibarrier.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iwrite_at_all` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iwrite_at_all` function call.
 *
 * @struct args_MPI_File_iwrite_at_all_t
 *
 * @note 
 *	int
 *	MPI_File_iwrite_at_all (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iwrite_at_all
struct args_MPI_File_iwrite_at_all_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iwrite_at_all(activity) { \
	activity->mpi_args.MPI_File_iwrite_at_all.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iwrite_at_all.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_iwrite_at_all.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iwrite_at_all.count = (int) count; \
	activity->mpi_args.MPI_File_iwrite_at_all.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iwrite_at_all.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iwrite_at_all(args) { \
	if (args->MPI_File_iwrite_at_all.request != NULL) { \
		args->MPI_File_iwrite_at_all.request__ref.val = *args->MPI_File_iwrite_at_all.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_size` function call.
 *
 * @struct args_MPI_File_get_size_t
 *
 * @note 
 *	int
 *	MPI_File_get_size (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset * size (long long*)
 *	)
 */
#if HAVE_MPI_File_get_size
struct args_MPI_File_get_size_t {
	MPI_File fh;
	MPI_Offset * size;
	struct {
		MPI_Offset val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_size(activity) { \
	activity->mpi_args.MPI_File_get_size.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_size.size = (MPI_Offset *) size; \
};

#define GET_PTRS_VALUE_MPI_File_get_size(args) { \
	if (args->MPI_File_get_size.size != NULL) { \
		args->MPI_File_get_size.size__ref.val = *args->MPI_File_get_size.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Barrier_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Barrier_init` function call.
 *
 * @struct args_MPI_Barrier_init_t
 *
 * @note 
 *	int
 *	MPI_Barrier_init (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Barrier_init
struct args_MPI_Barrier_init_t {
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Barrier_init(activity) { \
	activity->mpi_args.MPI_Barrier_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Barrier_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Barrier_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Barrier_init(args) { \
	if (args->MPI_Barrier_init.request != NULL) { \
		args->MPI_Barrier_init.request__ref.val = *args->MPI_Barrier_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_view` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_view` function call.
 *
 * @struct args_MPI_File_get_view_t
 *
 * @note 
 *	int
 *	MPI_File_get_view (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset * disp (long long*)
 *			MPI_Datatype * etype (struct mpi_datatype_t **)
 *			MPI_Datatype * filetype (struct mpi_datatype_t **)
 *			char * datarep (char *)
 *	)
 */
#if HAVE_MPI_File_get_view
struct args_MPI_File_get_view_t {
	MPI_File fh;
	MPI_Offset * disp;
	struct {
		MPI_Offset val;
	} disp__ref;
	MPI_Datatype * etype;
	struct {
		MPI_Datatype val;
	} etype__ref;
	MPI_Datatype * filetype;
	struct {
		MPI_Datatype val;
	} filetype__ref;
	char * datarep;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} datarep__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_view(activity) { \
	activity->mpi_args.MPI_File_get_view.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_view.disp = (MPI_Offset *) disp; \
	activity->mpi_args.MPI_File_get_view.etype = (MPI_Datatype *) etype; \
	activity->mpi_args.MPI_File_get_view.filetype = (MPI_Datatype *) filetype; \
	activity->mpi_args.MPI_File_get_view.datarep = (char *) datarep; \
};

#define GET_PTRS_VALUE_MPI_File_get_view(args) { \
	if (args->MPI_File_get_view.disp != NULL) { \
		args->MPI_File_get_view.disp__ref.val = *args->MPI_File_get_view.disp; \
	} \
	if (args->MPI_File_get_view.etype != NULL) { \
		args->MPI_File_get_view.etype__ref.val = *args->MPI_File_get_view.etype; \
	} \
	if (args->MPI_File_get_view.filetype != NULL) { \
		args->MPI_File_get_view.filetype__ref.val = *args->MPI_File_get_view.filetype; \
	} \
	if (args->MPI_File_get_view.datarep != NULL) { \
		strncpy(args->MPI_File_get_view.datarep__ref.val, args->MPI_File_get_view.datarep, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_allocate_shared` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_allocate_shared` function call.
 *
 * @struct args_MPI_Win_allocate_shared_t
 *
 * @note 
 *	int
 *	MPI_Win_allocate_shared (
 *			MPI_Aint size (long)
 *			int disp_unit (int)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			void * baseptr (void *)
 *			MPI_Win * win (struct mpi_win_t **)
 *	)
 */
#if HAVE_MPI_Win_allocate_shared
struct args_MPI_Win_allocate_shared_t {
	MPI_Aint size;
	int disp_unit;
	MPI_Info info;
	MPI_Comm comm;
	void * baseptr;
	MPI_Win * win;
	struct {
		MPI_Win val;
	} win__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_allocate_shared(activity) { \
	activity->mpi_args.MPI_Win_allocate_shared.size = (MPI_Aint) size; \
	activity->mpi_args.MPI_Win_allocate_shared.disp_unit = (int) disp_unit; \
	activity->mpi_args.MPI_Win_allocate_shared.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Win_allocate_shared.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Win_allocate_shared.baseptr = (void *) baseptr; \
	activity->mpi_args.MPI_Win_allocate_shared.win = (MPI_Win *) win; \
};

#define GET_PTRS_VALUE_MPI_Win_allocate_shared(args) { \
	if (args->MPI_Win_allocate_shared.win != NULL) { \
		args->MPI_Win_allocate_shared.win__ref.val = *args->MPI_Win_allocate_shared.win; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Close_port` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Close_port` function call.
 *
 * @struct args_MPI_Close_port_t
 *
 * @note 
 *	int
 *	MPI_Close_port (
 *			const char * port_name (const char *)
 *	)
 */
#if HAVE_MPI_Close_port
struct args_MPI_Close_port_t {
	char * port_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} port_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Close_port(activity) { \
	activity->mpi_args.MPI_Close_port.port_name = (char *) port_name; \
};

#define GET_PTRS_VALUE_MPI_Close_port(args) { \
	if (args->MPI_Close_port.port_name != NULL) { \
		strncpy(args->MPI_Close_port.port_name__ref.val, args->MPI_Close_port.port_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Finalized` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Finalized` function call.
 *
 * @struct args_MPI_Finalized_t
 *
 * @note 
 *	int
 *	MPI_Finalized (
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Finalized
struct args_MPI_Finalized_t {
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Finalized(activity) { \
	activity->mpi_args.MPI_Finalized.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Finalized(args) { \
	if (args->MPI_Finalized.flag != NULL) { \
		args->MPI_Finalized.flag__ref.val = *args->MPI_Finalized.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_dup` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_dup` function call.
 *
 * @struct args_MPI_Info_dup_t
 *
 * @note 
 *	int
 *	MPI_Info_dup (
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Info * newinfo (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Info_dup
struct args_MPI_Info_dup_t {
	MPI_Info info;
	MPI_Info * newinfo;
	struct {
		MPI_Info val;
	} newinfo__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_dup(activity) { \
	activity->mpi_args.MPI_Info_dup.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_dup.newinfo = (MPI_Info *) newinfo; \
};

#define GET_PTRS_VALUE_MPI_Info_dup(args) { \
	if (args->MPI_Info_dup.newinfo != NULL) { \
		args->MPI_Info_dup.newinfo__ref.val = *args->MPI_Info_dup.newinfo; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_get` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_get` function call.
 *
 * @struct args_MPI_Info_get_t
 *
 * @note 
 *	int
 *	MPI_Info_get (
 *			MPI_Info info (struct mpi_info_t *)
 *			const char * key (const char *)
 *			int valuelen (int)
 *			char * value (char *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Info_get
struct args_MPI_Info_get_t {
	MPI_Info info;
	char * key;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} key__ref;
	int valuelen;
	char * value;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} value__ref;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_get(activity) { \
	activity->mpi_args.MPI_Info_get.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_get.key = (char *) key; \
	activity->mpi_args.MPI_Info_get.valuelen = (int) valuelen; \
	activity->mpi_args.MPI_Info_get.value = (char *) value; \
	activity->mpi_args.MPI_Info_get.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Info_get(args) { \
	if (args->MPI_Info_get.key != NULL) { \
		strncpy(args->MPI_Info_get.key__ref.val, args->MPI_Info_get.key, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Info_get.value != NULL) { \
		strncpy(args->MPI_Info_get.value__ref.val, args->MPI_Info_get.value, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Info_get.flag != NULL) { \
		args->MPI_Info_get.flag__ref.val = *args->MPI_Info_get.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Get_library_version` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Get_library_version` function call.
 *
 * @struct args_MPI_Get_library_version_t
 *
 * @note 
 *	int
 *	MPI_Get_library_version (
 *			char * version (char *)
 *			int * resultlen (int *)
 *	)
 */
#if HAVE_MPI_Get_library_version
struct args_MPI_Get_library_version_t {
	char * version;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} version__ref;
	int * resultlen;
	struct {
		int val;
	} resultlen__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Get_library_version(activity) { \
	activity->mpi_args.MPI_Get_library_version.version = (char *) version; \
	activity->mpi_args.MPI_Get_library_version.resultlen = (int *) resultlen; \
};

#define GET_PTRS_VALUE_MPI_Get_library_version(args) { \
	if (args->MPI_Get_library_version.version != NULL) { \
		strncpy(args->MPI_Get_library_version.version__ref.val, args->MPI_Get_library_version.version, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Get_library_version.resultlen != NULL) { \
		args->MPI_Get_library_version.resultlen__ref.val = *args->MPI_Get_library_version.resultlen; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_create` function call.
 *
 * @struct args_MPI_Info_create_t
 *
 * @note 
 *	int
 *	MPI_Info_create (
 *			MPI_Info * info (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Info_create
struct args_MPI_Info_create_t {
	MPI_Info * info;
	struct {
		MPI_Info val;
	} info__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_create(activity) { \
	activity->mpi_args.MPI_Info_create.info = (MPI_Info *) info; \
};

#define GET_PTRS_VALUE_MPI_Info_create(args) { \
	if (args->MPI_Info_create.info != NULL) { \
		args->MPI_Info_create.info__ref.val = *args->MPI_Info_create.info; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_fence` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_fence` function call.
 *
 * @struct args_MPI_Win_fence_t
 *
 * @note 
 *	int
 *	MPI_Win_fence (
 *			int mpi_assert (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_fence
struct args_MPI_Win_fence_t {
	int mpi_assert;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_fence(activity) { \
	activity->mpi_args.MPI_Win_fence.mpi_assert = (int) mpi_assert; \
	activity->mpi_args.MPI_Win_fence.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iallgather` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iallgather` function call.
 *
 * @struct args_MPI_Iallgather_t
 *
 * @note 
 *	int
 *	MPI_Iallgather (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Iallgather
struct args_MPI_Iallgather_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iallgather(activity) { \
	activity->mpi_args.MPI_Iallgather.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Iallgather.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Iallgather.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Iallgather.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Iallgather.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Iallgather.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Iallgather.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iallgather.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Iallgather(args) { \
	if (args->MPI_Iallgather.request != NULL) { \
		args->MPI_Iallgather.request__ref.val = *args->MPI_Iallgather.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_spawn_multiple` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_spawn_multiple` function call.
 *
 * @struct args_MPI_Comm_spawn_multiple_t
 *
 * @note 
 *	int
 *	MPI_Comm_spawn_multiple (
 *			int count (int)
 *			char *[] array_of_commands (char *[])
 *			char **[] array_of_argv (char **[])
 *			const int[] array_of_maxprocs (const int[])
 *			const MPI_Info[] array_of_info (const struct mpi_info_t *[])
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Comm * intercomm (struct mpi_communicator_t **)
 *			int[] array_of_errcodes (int[])
 *	)
 */
#if HAVE_MPI_Comm_spawn_multiple
struct args_MPI_Comm_spawn_multiple_t {
	int count;
	char *(* array_of_commands);
	struct {
		void* ptr1;
		char val[MPI_STRING_SIZE_MAX];
	} array_of_commands__ref;
	char **(* array_of_argv);
	struct {
		void* ptr1;
		void* ptr2;
		char val[MPI_STRING_SIZE_MAX];
	} array_of_argv__ref;
	int(* array_of_maxprocs);
	struct {
		int val;
	} array_of_maxprocs__ref;
	MPI_Info(* array_of_info);
	struct {
		MPI_Info val;
	} array_of_info__ref;
	int root;
	MPI_Comm comm;
	MPI_Comm * intercomm;
	struct {
		MPI_Comm val;
	} intercomm__ref;
	int(* array_of_errcodes);
	struct {
		int val;
	} array_of_errcodes__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_spawn_multiple(activity) { \
	activity->mpi_args.MPI_Comm_spawn_multiple.count = (int) count; \
	activity->mpi_args.MPI_Comm_spawn_multiple.array_of_commands = (char *(*)) array_of_commands; \
	activity->mpi_args.MPI_Comm_spawn_multiple.array_of_argv = (char **(*)) array_of_argv; \
	activity->mpi_args.MPI_Comm_spawn_multiple.array_of_maxprocs = (int(*)) array_of_maxprocs; \
	activity->mpi_args.MPI_Comm_spawn_multiple.array_of_info = (MPI_Info(*)) array_of_info; \
	activity->mpi_args.MPI_Comm_spawn_multiple.root = (int) root; \
	activity->mpi_args.MPI_Comm_spawn_multiple.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_spawn_multiple.intercomm = (MPI_Comm *) intercomm; \
	activity->mpi_args.MPI_Comm_spawn_multiple.array_of_errcodes = (int(*)) array_of_errcodes; \
};

#define GET_PTRS_VALUE_MPI_Comm_spawn_multiple(args) { \
	if (args->MPI_Comm_spawn_multiple.array_of_commands != NULL) { \
		args->MPI_Comm_spawn_multiple.array_of_commands__ref.ptr1 = *args->MPI_Comm_spawn_multiple.array_of_commands; \
		if (args->MPI_Comm_spawn_multiple.array_of_commands__ref.ptr1 != NULL) { \
			strncpy(args->MPI_Comm_spawn_multiple.array_of_commands__ref.val, args->MPI_Comm_spawn_multiple.array_of_commands__ref.ptr1, MPI_STRING_SIZE_MAX-1); \
		} \
	} \
	if (args->MPI_Comm_spawn_multiple.array_of_argv != NULL) { \
		args->MPI_Comm_spawn_multiple.array_of_argv__ref.ptr1 = *args->MPI_Comm_spawn_multiple.array_of_argv; \
		if (args->MPI_Comm_spawn_multiple.array_of_argv__ref.ptr1 != NULL) { \
			args->MPI_Comm_spawn_multiple.array_of_argv__ref.ptr2 = **args->MPI_Comm_spawn_multiple.array_of_argv; \
			if (args->MPI_Comm_spawn_multiple.array_of_argv__ref.ptr2 != NULL) { \
				strncpy(args->MPI_Comm_spawn_multiple.array_of_argv__ref.val, args->MPI_Comm_spawn_multiple.array_of_argv__ref.ptr2, MPI_STRING_SIZE_MAX-1); \
			} \
		} \
	} \
	if (args->MPI_Comm_spawn_multiple.array_of_maxprocs != NULL) { \
		args->MPI_Comm_spawn_multiple.array_of_maxprocs__ref.val = *args->MPI_Comm_spawn_multiple.array_of_maxprocs; \
	} \
	if (args->MPI_Comm_spawn_multiple.array_of_info != NULL) { \
		args->MPI_Comm_spawn_multiple.array_of_info__ref.val = *args->MPI_Comm_spawn_multiple.array_of_info; \
	} \
	if (args->MPI_Comm_spawn_multiple.intercomm != NULL) { \
		args->MPI_Comm_spawn_multiple.intercomm__ref.val = *args->MPI_Comm_spawn_multiple.intercomm; \
	} \
	if (args->MPI_Comm_spawn_multiple.array_of_errcodes != NULL) { \
		args->MPI_Comm_spawn_multiple.array_of_errcodes__ref.val = *args->MPI_Comm_spawn_multiple.array_of_errcodes; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Precv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Precv_init` function call.
 *
 * @struct args_MPI_Precv_init_t
 *
 * @note 
 *	int
 *	MPI_Precv_init (
 *			void * buf (void *)
 *			int partitions (int)
 *			MPI_Count count (long long)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int source (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Precv_init
struct args_MPI_Precv_init_t {
	void * buf;
	int partitions;
	MPI_Count count;
	MPI_Datatype datatype;
	int source;
	int tag;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Precv_init(activity) { \
	activity->mpi_args.MPI_Precv_init.buf = (void *) buf; \
	activity->mpi_args.MPI_Precv_init.partitions = (int) partitions; \
	activity->mpi_args.MPI_Precv_init.count = (MPI_Count) count; \
	activity->mpi_args.MPI_Precv_init.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Precv_init.source = (int) source; \
	activity->mpi_args.MPI_Precv_init.tag = (int) tag; \
	activity->mpi_args.MPI_Precv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Precv_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Precv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Precv_init(args) { \
	if (args->MPI_Precv_init.request != NULL) { \
		args->MPI_Precv_init.request__ref.val = *args->MPI_Precv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_set_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_set_errhandler` function call.
 *
 * @struct args_MPI_Comm_set_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Comm_set_errhandler (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *	)
 */
#if HAVE_MPI_Comm_set_errhandler
struct args_MPI_Comm_set_errhandler_t {
	MPI_Comm comm;
	MPI_Errhandler errhandler;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_set_errhandler(activity) { \
	activity->mpi_args.MPI_Comm_set_errhandler.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_set_errhandler.errhandler = (MPI_Errhandler) errhandler; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_set_view` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_set_view` function call.
 *
 * @struct args_MPI_File_set_view_t
 *
 * @note 
 *	int
 *	MPI_File_set_view (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset disp (long long)
 *			MPI_Datatype etype (struct mpi_datatype_t *)
 *			MPI_Datatype filetype (struct mpi_datatype_t *)
 *			const char * datarep (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *	)
 */
#if HAVE_MPI_File_set_view
struct args_MPI_File_set_view_t {
	MPI_File fh;
	MPI_Offset disp;
	MPI_Datatype etype;
	MPI_Datatype filetype;
	char * datarep;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} datarep__ref;
	MPI_Info info;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_set_view(activity) { \
	activity->mpi_args.MPI_File_set_view.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_set_view.disp = (MPI_Offset) disp; \
	activity->mpi_args.MPI_File_set_view.etype = (MPI_Datatype) etype; \
	activity->mpi_args.MPI_File_set_view.filetype = (MPI_Datatype) filetype; \
	activity->mpi_args.MPI_File_set_view.datarep = (char *) datarep; \
	activity->mpi_args.MPI_File_set_view.info = (MPI_Info) info; \
};

#define GET_PTRS_VALUE_MPI_File_set_view(args) { \
	if (args->MPI_File_set_view.datarep != NULL) { \
		strncpy(args->MPI_File_set_view.datarep__ref.val, args->MPI_File_set_view.datarep, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Bsend` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Bsend` function call.
 *
 * @struct args_MPI_Bsend_t
 *
 * @note 
 *	int
 *	MPI_Bsend (
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			int dest (int)
 *			int tag (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Bsend
struct args_MPI_Bsend_t {
	void * buf;
	int count;
	MPI_Datatype datatype;
	int dest;
	int tag;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Bsend(activity) { \
	activity->mpi_args.MPI_Bsend.buf = (void *) buf; \
	activity->mpi_args.MPI_Bsend.count = (int) count; \
	activity->mpi_args.MPI_Bsend.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Bsend.dest = (int) dest; \
	activity->mpi_args.MPI_Bsend.tag = (int) tag; \
	activity->mpi_args.MPI_Bsend.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_size` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_size` function call.
 *
 * @struct args_MPI_Type_size_t
 *
 * @note 
 *	int
 *	MPI_Type_size (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			int * size (int *)
 *	)
 */
#if HAVE_MPI_Type_size
struct args_MPI_Type_size_t {
	MPI_Datatype type;
	int * size;
	struct {
		int val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_size(activity) { \
	activity->mpi_args.MPI_Type_size.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_size.size = (int *) size; \
};

#define GET_PTRS_VALUE_MPI_Type_size(args) { \
	if (args->MPI_Type_size.size != NULL) { \
		args->MPI_Type_size.size__ref.val = *args->MPI_Type_size.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_attr` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_attr` function call.
 *
 * @struct args_MPI_Type_get_attr_t
 *
 * @note 
 *	int
 *	MPI_Type_get_attr (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			int type_keyval (int)
 *			void * attribute_val (void *)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Type_get_attr
struct args_MPI_Type_get_attr_t {
	MPI_Datatype type;
	int type_keyval;
	void * attribute_val;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_attr(activity) { \
	activity->mpi_args.MPI_Type_get_attr.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_get_attr.type_keyval = (int) type_keyval; \
	activity->mpi_args.MPI_Type_get_attr.attribute_val = (void *) attribute_val; \
	activity->mpi_args.MPI_Type_get_attr.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Type_get_attr(args) { \
	if (args->MPI_Type_get_attr.flag != NULL) { \
		args->MPI_Type_get_attr.flag__ref.val = *args->MPI_Type_get_attr.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_write_ordered` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_write_ordered` function call.
 *
 * @struct args_MPI_File_write_ordered_t
 *
 * @note 
 *	int
 *	MPI_File_write_ordered (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Status * status (struct opaque **)
 *	)
 */
#if HAVE_MPI_File_write_ordered
struct args_MPI_File_write_ordered_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Status * status;
	struct {
		MPI_Status val;
	} status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_write_ordered(activity) { \
	activity->mpi_args.MPI_File_write_ordered.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_write_ordered.buf = (void *) buf; \
	activity->mpi_args.MPI_File_write_ordered.count = (int) count; \
	activity->mpi_args.MPI_File_write_ordered.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_write_ordered.status = (MPI_Status *) status; \
};

#define GET_PTRS_VALUE_MPI_File_write_ordered(args) { \
	if (args->MPI_File_write_ordered.status != NULL) { \
		args->MPI_File_write_ordered.status__ref.val = *args->MPI_File_write_ordered.status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_get_info` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_get_info` function call.
 *
 * @struct args_MPI_File_get_info_t
 *
 * @note 
 *	int
 *	MPI_File_get_info (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Info * info_used (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_File_get_info
struct args_MPI_File_get_info_t {
	MPI_File fh;
	MPI_Info * info_used;
	struct {
		MPI_Info val;
	} info_used__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_get_info(activity) { \
	activity->mpi_args.MPI_File_get_info.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_get_info.info_used = (MPI_Info *) info_used; \
};

#define GET_PTRS_VALUE_MPI_File_get_info(args) { \
	if (args->MPI_File_get_info.info_used != NULL) { \
		args->MPI_File_get_info.info_used__ref.val = *args->MPI_File_get_info.info_used; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Graph_neighbors` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Graph_neighbors` function call.
 *
 * @struct args_MPI_Graph_neighbors_t
 *
 * @note 
 *	int
 *	MPI_Graph_neighbors (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			int rank (int)
 *			int maxneighbors (int)
 *			int[] neighbors (int[])
 *	)
 */
#if HAVE_MPI_Graph_neighbors
struct args_MPI_Graph_neighbors_t {
	MPI_Comm comm;
	int rank;
	int maxneighbors;
	int(* neighbors);
	struct {
		int val;
	} neighbors__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Graph_neighbors(activity) { \
	activity->mpi_args.MPI_Graph_neighbors.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Graph_neighbors.rank = (int) rank; \
	activity->mpi_args.MPI_Graph_neighbors.maxneighbors = (int) maxneighbors; \
	activity->mpi_args.MPI_Graph_neighbors.neighbors = (int(*)) neighbors; \
};

#define GET_PTRS_VALUE_MPI_Graph_neighbors(args) { \
	if (args->MPI_Graph_neighbors.neighbors != NULL) { \
		args->MPI_Graph_neighbors.neighbors__ref.val = *args->MPI_Graph_neighbors.neighbors; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Igatherv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Igatherv` function call.
 *
 * @struct args_MPI_Igatherv_t
 *
 * @note 
 *	int
 *	MPI_Igatherv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Igatherv
struct args_MPI_Igatherv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Igatherv(activity) { \
	activity->mpi_args.MPI_Igatherv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Igatherv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Igatherv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Igatherv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Igatherv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Igatherv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Igatherv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Igatherv.root = (int) root; \
	activity->mpi_args.MPI_Igatherv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Igatherv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Igatherv(args) { \
	if (args->MPI_Igatherv.recvcounts != NULL) { \
		args->MPI_Igatherv.recvcounts__ref.val = *args->MPI_Igatherv.recvcounts; \
	} \
	if (args->MPI_Igatherv.displs != NULL) { \
		args->MPI_Igatherv.displs__ref.val = *args->MPI_Igatherv.displs; \
	} \
	if (args->MPI_Igatherv.request != NULL) { \
		args->MPI_Igatherv.request__ref.val = *args->MPI_Igatherv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_delete` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_delete` function call.
 *
 * @struct args_MPI_Info_delete_t
 *
 * @note 
 *	int
 *	MPI_Info_delete (
 *			MPI_Info info (struct mpi_info_t *)
 *			const char * key (const char *)
 *	)
 */
#if HAVE_MPI_Info_delete
struct args_MPI_Info_delete_t {
	MPI_Info info;
	char * key;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} key__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_delete(activity) { \
	activity->mpi_args.MPI_Info_delete.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Info_delete.key = (char *) key; \
};

#define GET_PTRS_VALUE_MPI_Info_delete(args) { \
	if (args->MPI_Info_delete.key != NULL) { \
		strncpy(args->MPI_Info_delete.key__ref.val, args->MPI_Info_delete.key, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Alloc_mem` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Alloc_mem` function call.
 *
 * @struct args_MPI_Alloc_mem_t
 *
 * @note 
 *	int
 *	MPI_Alloc_mem (
 *			MPI_Aint size (long)
 *			MPI_Info info (struct mpi_info_t *)
 *			void * baseptr (void *)
 *	)
 */
#if HAVE_MPI_Alloc_mem
struct args_MPI_Alloc_mem_t {
	MPI_Aint size;
	MPI_Info info;
	void * baseptr;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Alloc_mem(activity) { \
	activity->mpi_args.MPI_Alloc_mem.size = (MPI_Aint) size; \
	activity->mpi_args.MPI_Alloc_mem.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Alloc_mem.baseptr = (void *) baseptr; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_get_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_get_errhandler` function call.
 *
 * @struct args_MPI_Comm_get_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Comm_get_errhandler (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Errhandler * erhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_Comm_get_errhandler
struct args_MPI_Comm_get_errhandler_t {
	MPI_Comm comm;
	MPI_Errhandler * erhandler;
	struct {
		MPI_Errhandler val;
	} erhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_get_errhandler(activity) { \
	activity->mpi_args.MPI_Comm_get_errhandler.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_get_errhandler.erhandler = (MPI_Errhandler *) erhandler; \
};

#define GET_PTRS_VALUE_MPI_Comm_get_errhandler(args) { \
	if (args->MPI_Comm_get_errhandler.erhandler != NULL) { \
		args->MPI_Comm_get_errhandler.erhandler__ref.val = *args->MPI_Comm_get_errhandler.erhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_init` function call.
 *
 * @struct args_MPI_Session_init_t
 *
 * @note 
 *	int
 *	MPI_Session_init (
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *			MPI_Session * session (struct mpi_instance_t **)
 *	)
 */
#if HAVE_MPI_Session_init
struct args_MPI_Session_init_t {
	MPI_Info info;
	MPI_Errhandler errhandler;
	MPI_Session * session;
	struct {
		MPI_Session val;
	} session__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_init(activity) { \
	activity->mpi_args.MPI_Session_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Session_init.errhandler = (MPI_Errhandler) errhandler; \
	activity->mpi_args.MPI_Session_init.session = (MPI_Session *) session; \
};

#define GET_PTRS_VALUE_MPI_Session_init(args) { \
	if (args->MPI_Session_init.session != NULL) { \
		args->MPI_Session_init.session__ref.val = *args->MPI_Session_init.session; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_post` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_post` function call.
 *
 * @struct args_MPI_Win_post_t
 *
 * @note 
 *	int
 *	MPI_Win_post (
 *			MPI_Group group (struct mpi_group_t *)
 *			int mpi_assert (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_post
struct args_MPI_Win_post_t {
	MPI_Group group;
	int mpi_assert;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_post(activity) { \
	activity->mpi_args.MPI_Win_post.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Win_post.mpi_assert = (int) mpi_assert; \
	activity->mpi_args.MPI_Win_post.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Intercomm_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Intercomm_create` function call.
 *
 * @struct args_MPI_Intercomm_create_t
 *
 * @note 
 *	int
 *	MPI_Intercomm_create (
 *			MPI_Comm local_comm (struct mpi_communicator_t *)
 *			int local_leader (int)
 *			MPI_Comm bridge_comm (struct mpi_communicator_t *)
 *			int remote_leader (int)
 *			int tag (int)
 *			MPI_Comm * newintercomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Intercomm_create
struct args_MPI_Intercomm_create_t {
	MPI_Comm local_comm;
	int local_leader;
	MPI_Comm bridge_comm;
	int remote_leader;
	int tag;
	MPI_Comm * newintercomm;
	struct {
		MPI_Comm val;
	} newintercomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Intercomm_create(activity) { \
	activity->mpi_args.MPI_Intercomm_create.local_comm = (MPI_Comm) local_comm; \
	activity->mpi_args.MPI_Intercomm_create.local_leader = (int) local_leader; \
	activity->mpi_args.MPI_Intercomm_create.bridge_comm = (MPI_Comm) bridge_comm; \
	activity->mpi_args.MPI_Intercomm_create.remote_leader = (int) remote_leader; \
	activity->mpi_args.MPI_Intercomm_create.tag = (int) tag; \
	activity->mpi_args.MPI_Intercomm_create.newintercomm = (MPI_Comm *) newintercomm; \
};

#define GET_PTRS_VALUE_MPI_Intercomm_create(args) { \
	if (args->MPI_Intercomm_create.newintercomm != NULL) { \
		args->MPI_Intercomm_create.newintercomm__ref.val = *args->MPI_Intercomm_create.newintercomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_all_begin` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_all_begin` function call.
 *
 * @struct args_MPI_File_read_all_begin_t
 *
 * @note 
 *	int
 *	MPI_File_read_all_begin (
 *			MPI_File fh (struct mpi_file_t *)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_File_read_all_begin
struct args_MPI_File_read_all_begin_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_all_begin(activity) { \
	activity->mpi_args.MPI_File_read_all_begin.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_all_begin.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_all_begin.count = (int) count; \
	activity->mpi_args.MPI_File_read_all_begin.datatype = (MPI_Datatype) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ialltoallv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ialltoallv` function call.
 *
 * @struct args_MPI_Ialltoallv_t
 *
 * @note 
 *	int
 *	MPI_Ialltoallv (
 *			const void * sendbuf (const void *)
 *			const int[] sendcounts (const int[])
 *			const int[] sdispls (const int[])
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] rdispls (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ialltoallv
struct args_MPI_Ialltoallv_t {
	void * sendbuf;
	int(* sendcounts);
	struct {
		int val;
	} sendcounts__ref;
	int(* sdispls);
	struct {
		int val;
	} sdispls__ref;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* rdispls);
	struct {
		int val;
	} rdispls__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ialltoallv(activity) { \
	activity->mpi_args.MPI_Ialltoallv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ialltoallv.sendcounts = (int(*)) sendcounts; \
	activity->mpi_args.MPI_Ialltoallv.sdispls = (int(*)) sdispls; \
	activity->mpi_args.MPI_Ialltoallv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Ialltoallv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ialltoallv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Ialltoallv.rdispls = (int(*)) rdispls; \
	activity->mpi_args.MPI_Ialltoallv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Ialltoallv.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ialltoallv.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ialltoallv(args) { \
	if (args->MPI_Ialltoallv.sendcounts != NULL) { \
		args->MPI_Ialltoallv.sendcounts__ref.val = *args->MPI_Ialltoallv.sendcounts; \
	} \
	if (args->MPI_Ialltoallv.sdispls != NULL) { \
		args->MPI_Ialltoallv.sdispls__ref.val = *args->MPI_Ialltoallv.sdispls; \
	} \
	if (args->MPI_Ialltoallv.recvcounts != NULL) { \
		args->MPI_Ialltoallv.recvcounts__ref.val = *args->MPI_Ialltoallv.recvcounts; \
	} \
	if (args->MPI_Ialltoallv.rdispls != NULL) { \
		args->MPI_Ialltoallv.rdispls__ref.val = *args->MPI_Ialltoallv.rdispls; \
	} \
	if (args->MPI_Ialltoallv.request != NULL) { \
		args->MPI_Ialltoallv.request__ref.val = *args->MPI_Ialltoallv.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_delete` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_delete` function call.
 *
 * @struct args_MPI_File_delete_t
 *
 * @note 
 *	int
 *	MPI_File_delete (
 *			const char * filename (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *	)
 */
#if HAVE_MPI_File_delete
struct args_MPI_File_delete_t {
	char * filename;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} filename__ref;
	MPI_Info info;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_delete(activity) { \
	activity->mpi_args.MPI_File_delete.filename = (char *) filename; \
	activity->mpi_args.MPI_File_delete.info = (MPI_Info) info; \
};

#define GET_PTRS_VALUE_MPI_File_delete(args) { \
	if (args->MPI_File_delete.filename != NULL) { \
		strncpy(args->MPI_File_delete.filename__ref.val, args->MPI_File_delete.filename, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Dims_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Dims_create` function call.
 *
 * @struct args_MPI_Dims_create_t
 *
 * @note 
 *	int
 *	MPI_Dims_create (
 *			int nnodes (int)
 *			int ndims (int)
 *			int[] dims (int[])
 *	)
 */
#if HAVE_MPI_Dims_create
struct args_MPI_Dims_create_t {
	int nnodes;
	int ndims;
	int(* dims);
	struct {
		int val;
	} dims__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Dims_create(activity) { \
	activity->mpi_args.MPI_Dims_create.nnodes = (int) nnodes; \
	activity->mpi_args.MPI_Dims_create.ndims = (int) ndims; \
	activity->mpi_args.MPI_Dims_create.dims = (int(*)) dims; \
};

#define GET_PTRS_VALUE_MPI_Dims_create(args) { \
	if (args->MPI_Dims_create.dims != NULL) { \
		args->MPI_Dims_create.dims__ref.val = *args->MPI_Dims_create.dims; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Cart_sub` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Cart_sub` function call.
 *
 * @struct args_MPI_Cart_sub_t
 *
 * @note 
 *	int
 *	MPI_Cart_sub (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			const int[] remain_dims (const int[])
 *			MPI_Comm * new_comm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Cart_sub
struct args_MPI_Cart_sub_t {
	MPI_Comm comm;
	int(* remain_dims);
	struct {
		int val;
	} remain_dims__ref;
	MPI_Comm * new_comm;
	struct {
		MPI_Comm val;
	} new_comm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Cart_sub(activity) { \
	activity->mpi_args.MPI_Cart_sub.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Cart_sub.remain_dims = (int(*)) remain_dims; \
	activity->mpi_args.MPI_Cart_sub.new_comm = (MPI_Comm *) new_comm; \
};

#define GET_PTRS_VALUE_MPI_Cart_sub(args) { \
	if (args->MPI_Cart_sub.remain_dims != NULL) { \
		args->MPI_Cart_sub.remain_dims__ref.val = *args->MPI_Cart_sub.remain_dims; \
	} \
	if (args->MPI_Cart_sub.new_comm != NULL) { \
		args->MPI_Cart_sub.new_comm__ref.val = *args->MPI_Cart_sub.new_comm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_allocate` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_allocate` function call.
 *
 * @struct args_MPI_Win_allocate_t
 *
 * @note 
 *	int
 *	MPI_Win_allocate (
 *			MPI_Aint size (long)
 *			int disp_unit (int)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			void * baseptr (void *)
 *			MPI_Win * win (struct mpi_win_t **)
 *	)
 */
#if HAVE_MPI_Win_allocate
struct args_MPI_Win_allocate_t {
	MPI_Aint size;
	int disp_unit;
	MPI_Info info;
	MPI_Comm comm;
	void * baseptr;
	MPI_Win * win;
	struct {
		MPI_Win val;
	} win__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_allocate(activity) { \
	activity->mpi_args.MPI_Win_allocate.size = (MPI_Aint) size; \
	activity->mpi_args.MPI_Win_allocate.disp_unit = (int) disp_unit; \
	activity->mpi_args.MPI_Win_allocate.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Win_allocate.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Win_allocate.baseptr = (void *) baseptr; \
	activity->mpi_args.MPI_Win_allocate.win = (MPI_Win *) win; \
};

#define GET_PTRS_VALUE_MPI_Win_allocate(args) { \
	if (args->MPI_Win_allocate.win != NULL) { \
		args->MPI_Win_allocate.win__ref.val = *args->MPI_Win_allocate.win; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_get_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_get_errhandler` function call.
 *
 * @struct args_MPI_Session_get_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Session_get_errhandler (
 *			MPI_Session session (struct mpi_instance_t *)
 *			MPI_Errhandler * erhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_Session_get_errhandler
struct args_MPI_Session_get_errhandler_t {
	MPI_Session session;
	MPI_Errhandler * erhandler;
	struct {
		MPI_Errhandler val;
	} erhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_get_errhandler(activity) { \
	activity->mpi_args.MPI_Session_get_errhandler.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_get_errhandler.erhandler = (MPI_Errhandler *) erhandler; \
};

#define GET_PTRS_VALUE_MPI_Session_get_errhandler(args) { \
	if (args->MPI_Session_get_errhandler.erhandler != NULL) { \
		args->MPI_Session_get_errhandler.erhandler__ref.val = *args->MPI_Session_get_errhandler.erhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Parrived` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Parrived` function call.
 *
 * @struct args_MPI_Parrived_t
 *
 * @note 
 *	int
 *	MPI_Parrived (
 *			MPI_Request request (struct mpi_request_t *)
 *			int partition (int)
 *			int * flag (int *)
 *	)
 */
#if HAVE_MPI_Parrived
struct args_MPI_Parrived_t {
	MPI_Request request;
	int partition;
	int * flag;
	struct {
		int val;
	} flag__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Parrived(activity) { \
	activity->mpi_args.MPI_Parrived.request = (MPI_Request) request; \
	activity->mpi_args.MPI_Parrived.partition = (int) partition; \
	activity->mpi_args.MPI_Parrived.flag = (int *) flag; \
};

#define GET_PTRS_VALUE_MPI_Parrived(args) { \
	if (args->MPI_Parrived.flag != NULL) { \
		args->MPI_Parrived.flag__ref.val = *args->MPI_Parrived.flag; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_create_env` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_create_env` function call.
 *
 * @struct args_MPI_Info_create_env_t
 *
 * @note 
 *	int
 *	MPI_Info_create_env (
 *			int argc (int)
 *			char *[] argv (char *[])
 *			MPI_Info * info (struct mpi_info_t **)
 *	)
 */
#if HAVE_MPI_Info_create_env
struct args_MPI_Info_create_env_t {
	int argc;
	char *(* argv);
	struct {
		void* ptr1;
		char val[MPI_STRING_SIZE_MAX];
	} argv__ref;
	MPI_Info * info;
	struct {
		MPI_Info val;
	} info__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_create_env(activity) { \
	activity->mpi_args.MPI_Info_create_env.argc = (int) argc; \
	activity->mpi_args.MPI_Info_create_env.argv = (char *(*)) argv; \
	activity->mpi_args.MPI_Info_create_env.info = (MPI_Info *) info; \
};

#define GET_PTRS_VALUE_MPI_Info_create_env(args) { \
	if (args->MPI_Info_create_env.argv != NULL) { \
		args->MPI_Info_create_env.argv__ref.ptr1 = *args->MPI_Info_create_env.argv; \
		if (args->MPI_Info_create_env.argv__ref.ptr1 != NULL) { \
			strncpy(args->MPI_Info_create_env.argv__ref.val, args->MPI_Info_create_env.argv__ref.ptr1, MPI_STRING_SIZE_MAX-1); \
		} \
	} \
	if (args->MPI_Info_create_env.info != NULL) { \
		args->MPI_Info_create_env.info__ref.val = *args->MPI_Info_create_env.info; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_create_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_create_errhandler` function call.
 *
 * @struct args_MPI_File_create_errhandler_t
 *
 * @note 
 *	int
 *	MPI_File_create_errhandler (
 *			MPI_File_errhandler_function * function (void (*)(struct mpi_file_t * *, int *, ...))
 *			MPI_Errhandler * errhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_File_create_errhandler
struct args_MPI_File_create_errhandler_t {
	MPI_File_errhandler_function * function;
	MPI_Errhandler * errhandler;
	struct {
		MPI_Errhandler val;
	} errhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_create_errhandler(activity) { \
	activity->mpi_args.MPI_File_create_errhandler.function = (MPI_File_errhandler_function *) function; \
	activity->mpi_args.MPI_File_create_errhandler.errhandler = (MPI_Errhandler *) errhandler; \
};

#define GET_PTRS_VALUE_MPI_File_create_errhandler(args) { \
	if (args->MPI_File_create_errhandler.errhandler != NULL) { \
		args->MPI_File_create_errhandler.errhandler__ref.val = *args->MPI_File_create_errhandler.errhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ialltoall` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ialltoall` function call.
 *
 * @struct args_MPI_Ialltoall_t
 *
 * @note 
 *	int
 *	MPI_Ialltoall (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ialltoall
struct args_MPI_Ialltoall_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ialltoall(activity) { \
	activity->mpi_args.MPI_Ialltoall.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ialltoall.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Ialltoall.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Ialltoall.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ialltoall.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Ialltoall.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Ialltoall.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ialltoall.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ialltoall(args) { \
	if (args->MPI_Ialltoall.request != NULL) { \
		args->MPI_Ialltoall.request__ref.val = *args->MPI_Ialltoall.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Raccumulate` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Raccumulate` function call.
 *
 * @struct args_MPI_Raccumulate_t
 *
 * @note 
 *	int
 *	MPI_Raccumulate (
 *			const void * origin_addr (const void *)
 *			int origin_count (int)
 *			MPI_Datatype origin_datatype (struct mpi_datatype_t *)
 *			int target_rank (int)
 *			MPI_Aint target_disp (long)
 *			int target_count (int)
 *			MPI_Datatype target_datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Raccumulate
struct args_MPI_Raccumulate_t {
	void * origin_addr;
	int origin_count;
	MPI_Datatype origin_datatype;
	int target_rank;
	MPI_Aint target_disp;
	int target_count;
	MPI_Datatype target_datatype;
	MPI_Op op;
	MPI_Win win;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Raccumulate(activity) { \
	activity->mpi_args.MPI_Raccumulate.origin_addr = (void *) origin_addr; \
	activity->mpi_args.MPI_Raccumulate.origin_count = (int) origin_count; \
	activity->mpi_args.MPI_Raccumulate.origin_datatype = (MPI_Datatype) origin_datatype; \
	activity->mpi_args.MPI_Raccumulate.target_rank = (int) target_rank; \
	activity->mpi_args.MPI_Raccumulate.target_disp = (MPI_Aint) target_disp; \
	activity->mpi_args.MPI_Raccumulate.target_count = (int) target_count; \
	activity->mpi_args.MPI_Raccumulate.target_datatype = (MPI_Datatype) target_datatype; \
	activity->mpi_args.MPI_Raccumulate.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Raccumulate.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Raccumulate.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Raccumulate(args) { \
	if (args->MPI_Raccumulate.request != NULL) { \
		args->MPI_Raccumulate.request__ref.val = *args->MPI_Raccumulate.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_size_x` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_size_x` function call.
 *
 * @struct args_MPI_Type_size_x_t
 *
 * @note 
 *	int
 *	MPI_Type_size_x (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			MPI_Count * size (long long*)
 *	)
 */
#if HAVE_MPI_Type_size_x
struct args_MPI_Type_size_x_t {
	MPI_Datatype type;
	MPI_Count * size;
	struct {
		MPI_Count val;
	} size__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_size_x(activity) { \
	activity->mpi_args.MPI_Type_size_x.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_size_x.size = (MPI_Count *) size; \
};

#define GET_PTRS_VALUE_MPI_Type_size_x(args) { \
	if (args->MPI_Type_size_x.size != NULL) { \
		args->MPI_Type_size_x.size__ref.val = *args->MPI_Type_size_x.size; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_get_extent_x` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_get_extent_x` function call.
 *
 * @struct args_MPI_Type_get_extent_x_t
 *
 * @note 
 *	int
 *	MPI_Type_get_extent_x (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			MPI_Count * lb (long long*)
 *			MPI_Count * extent (long long*)
 *	)
 */
#if HAVE_MPI_Type_get_extent_x
struct args_MPI_Type_get_extent_x_t {
	MPI_Datatype type;
	MPI_Count * lb;
	struct {
		MPI_Count val;
	} lb__ref;
	MPI_Count * extent;
	struct {
		MPI_Count val;
	} extent__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_get_extent_x(activity) { \
	activity->mpi_args.MPI_Type_get_extent_x.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_get_extent_x.lb = (MPI_Count *) lb; \
	activity->mpi_args.MPI_Type_get_extent_x.extent = (MPI_Count *) extent; \
};

#define GET_PTRS_VALUE_MPI_Type_get_extent_x(args) { \
	if (args->MPI_Type_get_extent_x.lb != NULL) { \
		args->MPI_Type_get_extent_x.lb__ref.val = *args->MPI_Type_get_extent_x.lb; \
	} \
	if (args->MPI_Type_get_extent_x.extent != NULL) { \
		args->MPI_Type_get_extent_x.extent__ref.val = *args->MPI_Type_get_extent_x.extent; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_read_at_all_begin` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_read_at_all_begin` function call.
 *
 * @struct args_MPI_File_read_at_all_begin_t
 *
 * @note 
 *	int
 *	MPI_File_read_at_all_begin (
 *			MPI_File fh (struct mpi_file_t *)
 *			MPI_Offset offset (long long)
 *			void * buf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_File_read_at_all_begin
struct args_MPI_File_read_at_all_begin_t {
	MPI_File fh;
	MPI_Offset offset;
	void * buf;
	int count;
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_read_at_all_begin(activity) { \
	activity->mpi_args.MPI_File_read_at_all_begin.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_read_at_all_begin.offset = (MPI_Offset) offset; \
	activity->mpi_args.MPI_File_read_at_all_begin.buf = (void *) buf; \
	activity->mpi_args.MPI_File_read_at_all_begin.count = (int) count; \
	activity->mpi_args.MPI_File_read_at_all_begin.datatype = (MPI_Datatype) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Dist_graph_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Dist_graph_create` function call.
 *
 * @struct args_MPI_Dist_graph_create_t
 *
 * @note 
 *	int
 *	MPI_Dist_graph_create (
 *			MPI_Comm comm_old (struct mpi_communicator_t *)
 *			int n (int)
 *			const int[] nodes (const int[])
 *			const int[] degrees (const int[])
 *			const int[] targets (const int[])
 *			const int[] weights (const int[])
 *			MPI_Info info (struct mpi_info_t *)
 *			int reorder (int)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Dist_graph_create
struct args_MPI_Dist_graph_create_t {
	MPI_Comm comm_old;
	int n;
	int(* nodes);
	struct {
		int val;
	} nodes__ref;
	int(* degrees);
	struct {
		int val;
	} degrees__ref;
	int(* targets);
	struct {
		int val;
	} targets__ref;
	int(* weights);
	struct {
		int val;
	} weights__ref;
	MPI_Info info;
	int reorder;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Dist_graph_create(activity) { \
	activity->mpi_args.MPI_Dist_graph_create.comm_old = (MPI_Comm) comm_old; \
	activity->mpi_args.MPI_Dist_graph_create.n = (int) n; \
	activity->mpi_args.MPI_Dist_graph_create.nodes = (int(*)) nodes; \
	activity->mpi_args.MPI_Dist_graph_create.degrees = (int(*)) degrees; \
	activity->mpi_args.MPI_Dist_graph_create.targets = (int(*)) targets; \
	activity->mpi_args.MPI_Dist_graph_create.weights = (int(*)) weights; \
	activity->mpi_args.MPI_Dist_graph_create.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Dist_graph_create.reorder = (int) reorder; \
	activity->mpi_args.MPI_Dist_graph_create.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Dist_graph_create(args) { \
	if (args->MPI_Dist_graph_create.nodes != NULL) { \
		args->MPI_Dist_graph_create.nodes__ref.val = *args->MPI_Dist_graph_create.nodes; \
	} \
	if (args->MPI_Dist_graph_create.degrees != NULL) { \
		args->MPI_Dist_graph_create.degrees__ref.val = *args->MPI_Dist_graph_create.degrees; \
	} \
	if (args->MPI_Dist_graph_create.targets != NULL) { \
		args->MPI_Dist_graph_create.targets__ref.val = *args->MPI_Dist_graph_create.targets; \
	} \
	if (args->MPI_Dist_graph_create.weights != NULL) { \
		args->MPI_Dist_graph_create.weights__ref.val = *args->MPI_Dist_graph_create.weights; \
	} \
	if (args->MPI_Dist_graph_create.newcomm != NULL) { \
		args->MPI_Dist_graph_create.newcomm__ref.val = *args->MPI_Dist_graph_create.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_join` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_join` function call.
 *
 * @struct args_MPI_Comm_join_t
 *
 * @note 
 *	int
 *	MPI_Comm_join (
 *			int fd (int)
 *			MPI_Comm * intercomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_join
struct args_MPI_Comm_join_t {
	int fd;
	MPI_Comm * intercomm;
	struct {
		MPI_Comm val;
	} intercomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_join(activity) { \
	activity->mpi_args.MPI_Comm_join.fd = (int) fd; \
	activity->mpi_args.MPI_Comm_join.intercomm = (MPI_Comm *) intercomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_join(args) { \
	if (args->MPI_Comm_join.intercomm != NULL) { \
		args->MPI_Comm_join.intercomm__ref.val = *args->MPI_Comm_join.intercomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Gatherv_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Gatherv_init` function call.
 *
 * @struct args_MPI_Gatherv_init_t
 *
 * @note 
 *	int
 *	MPI_Gatherv_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Gatherv_init
struct args_MPI_Gatherv_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Gatherv_init(activity) { \
	activity->mpi_args.MPI_Gatherv_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Gatherv_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Gatherv_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Gatherv_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Gatherv_init.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Gatherv_init.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Gatherv_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Gatherv_init.root = (int) root; \
	activity->mpi_args.MPI_Gatherv_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Gatherv_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Gatherv_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Gatherv_init(args) { \
	if (args->MPI_Gatherv_init.recvcounts != NULL) { \
		args->MPI_Gatherv_init.recvcounts__ref.val = *args->MPI_Gatherv_init.recvcounts; \
	} \
	if (args->MPI_Gatherv_init.displs != NULL) { \
		args->MPI_Gatherv_init.displs__ref.val = *args->MPI_Gatherv_init.displs; \
	} \
	if (args->MPI_Gatherv_init.request != NULL) { \
		args->MPI_Gatherv_init.request__ref.val = *args->MPI_Gatherv_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_sync` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_sync` function call.
 *
 * @struct args_MPI_File_sync_t
 *
 * @note 
 *	int
 *	MPI_File_sync (
 *			MPI_File fh (struct mpi_file_t *)
 *	)
 */
#if HAVE_MPI_File_sync
struct args_MPI_File_sync_t {
	MPI_File fh;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_sync(activity) { \
	activity->mpi_args.MPI_File_sync.fh = (MPI_File) fh; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_accept` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_accept` function call.
 *
 * @struct args_MPI_Comm_accept_t
 *
 * @note 
 *	int
 *	MPI_Comm_accept (
 *			const char * port_name (const char *)
 *			MPI_Info info (struct mpi_info_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Comm * newcomm (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Comm_accept
struct args_MPI_Comm_accept_t {
	char * port_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} port_name__ref;
	MPI_Info info;
	int root;
	MPI_Comm comm;
	MPI_Comm * newcomm;
	struct {
		MPI_Comm val;
	} newcomm__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_accept(activity) { \
	activity->mpi_args.MPI_Comm_accept.port_name = (char *) port_name; \
	activity->mpi_args.MPI_Comm_accept.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Comm_accept.root = (int) root; \
	activity->mpi_args.MPI_Comm_accept.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Comm_accept.newcomm = (MPI_Comm *) newcomm; \
};

#define GET_PTRS_VALUE_MPI_Comm_accept(args) { \
	if (args->MPI_Comm_accept.port_name != NULL) { \
		strncpy(args->MPI_Comm_accept.port_name__ref.val, args->MPI_Comm_accept.port_name, MPI_STRING_SIZE_MAX-1); \
	} \
	if (args->MPI_Comm_accept.newcomm != NULL) { \
		args->MPI_Comm_accept.newcomm__ref.val = *args->MPI_Comm_accept.newcomm; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Ineighbor_allgather` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Ineighbor_allgather` function call.
 *
 * @struct args_MPI_Ineighbor_allgather_t
 *
 * @note 
 *	int
 *	MPI_Ineighbor_allgather (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Ineighbor_allgather
struct args_MPI_Ineighbor_allgather_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Ineighbor_allgather(activity) { \
	activity->mpi_args.MPI_Ineighbor_allgather.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Ineighbor_allgather.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Ineighbor_allgather.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Ineighbor_allgather.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Ineighbor_allgather.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Ineighbor_allgather.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Ineighbor_allgather.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Ineighbor_allgather.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Ineighbor_allgather(args) { \
	if (args->MPI_Ineighbor_allgather.request != NULL) { \
		args->MPI_Ineighbor_allgather.request__ref.val = *args->MPI_Ineighbor_allgather.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_dup` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_dup` function call.
 *
 * @struct args_MPI_Type_dup_t
 *
 * @note 
 *	int
 *	MPI_Type_dup (
 *			MPI_Datatype type (struct mpi_datatype_t *)
 *			MPI_Datatype * newtype (struct mpi_datatype_t **)
 *	)
 */
#if HAVE_MPI_Type_dup
struct args_MPI_Type_dup_t {
	MPI_Datatype type;
	MPI_Datatype * newtype;
	struct {
		MPI_Datatype val;
	} newtype__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_dup(activity) { \
	activity->mpi_args.MPI_Type_dup.type = (MPI_Datatype) type; \
	activity->mpi_args.MPI_Type_dup.newtype = (MPI_Datatype *) newtype; \
};

#define GET_PTRS_VALUE_MPI_Type_dup(args) { \
	if (args->MPI_Type_dup.newtype != NULL) { \
		args->MPI_Type_dup.newtype__ref.val = *args->MPI_Type_dup.newtype; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iwrite_shared` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iwrite_shared` function call.
 *
 * @struct args_MPI_File_iwrite_shared_t
 *
 * @note 
 *	int
 *	MPI_File_iwrite_shared (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iwrite_shared
struct args_MPI_File_iwrite_shared_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iwrite_shared(activity) { \
	activity->mpi_args.MPI_File_iwrite_shared.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iwrite_shared.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iwrite_shared.count = (int) count; \
	activity->mpi_args.MPI_File_iwrite_shared.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iwrite_shared.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iwrite_shared(args) { \
	if (args->MPI_File_iwrite_shared.request != NULL) { \
		args->MPI_File_iwrite_shared.request__ref.val = *args->MPI_File_iwrite_shared.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_get_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_get_errhandler` function call.
 *
 * @struct args_MPI_Win_get_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Win_get_errhandler (
 *			MPI_Win win (struct mpi_win_t *)
 *			MPI_Errhandler * errhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_Win_get_errhandler
struct args_MPI_Win_get_errhandler_t {
	MPI_Win win;
	MPI_Errhandler * errhandler;
	struct {
		MPI_Errhandler val;
	} errhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_get_errhandler(activity) { \
	activity->mpi_args.MPI_Win_get_errhandler.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_get_errhandler.errhandler = (MPI_Errhandler *) errhandler; \
};

#define GET_PTRS_VALUE_MPI_Win_get_errhandler(args) { \
	if (args->MPI_Win_get_errhandler.errhandler != NULL) { \
		args->MPI_Win_get_errhandler.errhandler__ref.val = *args->MPI_Win_get_errhandler.errhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Iscan` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Iscan` function call.
 *
 * @struct args_MPI_Iscan_t
 *
 * @note 
 *	int
 *	MPI_Iscan (
 *			const void * sendbuf (const void *)
 *			void * recvbuf (void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Op op (struct mpi_op_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Iscan
struct args_MPI_Iscan_t {
	void * sendbuf;
	void * recvbuf;
	int count;
	MPI_Datatype datatype;
	MPI_Op op;
	MPI_Comm comm;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Iscan(activity) { \
	activity->mpi_args.MPI_Iscan.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Iscan.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Iscan.count = (int) count; \
	activity->mpi_args.MPI_Iscan.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_Iscan.op = (MPI_Op) op; \
	activity->mpi_args.MPI_Iscan.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Iscan.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Iscan(args) { \
	if (args->MPI_Iscan.request != NULL) { \
		args->MPI_Iscan.request__ref.val = *args->MPI_Iscan.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_flush` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_flush` function call.
 *
 * @struct args_MPI_Win_flush_t
 *
 * @note 
 *	int
 *	MPI_Win_flush (
 *			int rank (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_flush
struct args_MPI_Win_flush_t {
	int rank;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_flush(activity) { \
	activity->mpi_args.MPI_Win_flush.rank = (int) rank; \
	activity->mpi_args.MPI_Win_flush.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Graph_create` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Graph_create` function call.
 *
 * @struct args_MPI_Graph_create_t
 *
 * @note 
 *	int
 *	MPI_Graph_create (
 *			MPI_Comm comm_old (struct mpi_communicator_t *)
 *			int nnodes (int)
 *			const int[] index (const int[])
 *			const int[] edges (const int[])
 *			int reorder (int)
 *			MPI_Comm * comm_graph (struct mpi_communicator_t **)
 *	)
 */
#if HAVE_MPI_Graph_create
struct args_MPI_Graph_create_t {
	MPI_Comm comm_old;
	int nnodes;
	int(* index);
	struct {
		int val;
	} index__ref;
	int(* edges);
	struct {
		int val;
	} edges__ref;
	int reorder;
	MPI_Comm * comm_graph;
	struct {
		MPI_Comm val;
	} comm_graph__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Graph_create(activity) { \
	activity->mpi_args.MPI_Graph_create.comm_old = (MPI_Comm) comm_old; \
	activity->mpi_args.MPI_Graph_create.nnodes = (int) nnodes; \
	activity->mpi_args.MPI_Graph_create.index = (int(*)) index; \
	activity->mpi_args.MPI_Graph_create.edges = (int(*)) edges; \
	activity->mpi_args.MPI_Graph_create.reorder = (int) reorder; \
	activity->mpi_args.MPI_Graph_create.comm_graph = (MPI_Comm *) comm_graph; \
};

#define GET_PTRS_VALUE_MPI_Graph_create(args) { \
	if (args->MPI_Graph_create.index != NULL) { \
		args->MPI_Graph_create.index__ref.val = *args->MPI_Graph_create.index; \
	} \
	if (args->MPI_Graph_create.edges != NULL) { \
		args->MPI_Graph_create.edges__ref.val = *args->MPI_Graph_create.edges; \
	} \
	if (args->MPI_Graph_create.comm_graph != NULL) { \
		args->MPI_Graph_create.comm_graph__ref.val = *args->MPI_Graph_create.comm_graph; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_set_name` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_set_name` function call.
 *
 * @struct args_MPI_Win_set_name_t
 *
 * @note 
 *	int
 *	MPI_Win_set_name (
 *			MPI_Win win (struct mpi_win_t *)
 *			const char * win_name (const char *)
 *	)
 */
#if HAVE_MPI_Win_set_name
struct args_MPI_Win_set_name_t {
	MPI_Win win;
	char * win_name;
	struct {
		char val[MPI_STRING_SIZE_MAX];
	} win_name__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_set_name(activity) { \
	activity->mpi_args.MPI_Win_set_name.win = (MPI_Win) win; \
	activity->mpi_args.MPI_Win_set_name.win_name = (char *) win_name; \
};

#define GET_PTRS_VALUE_MPI_Win_set_name(args) { \
	if (args->MPI_Win_set_name.win_name != NULL) { \
		strncpy(args->MPI_Win_set_name.win_name__ref.val, args->MPI_Win_set_name.win_name, MPI_STRING_SIZE_MAX-1); \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_create_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_create_errhandler` function call.
 *
 * @struct args_MPI_Win_create_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Win_create_errhandler (
 *			MPI_Win_errhandler_function * function (void (*)(struct mpi_win_t * *, int *, ...))
 *			MPI_Errhandler * errhandler (struct mpi_errhandler_t **)
 *	)
 */
#if HAVE_MPI_Win_create_errhandler
struct args_MPI_Win_create_errhandler_t {
	MPI_Win_errhandler_function * function;
	MPI_Errhandler * errhandler;
	struct {
		MPI_Errhandler val;
	} errhandler__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_create_errhandler(activity) { \
	activity->mpi_args.MPI_Win_create_errhandler.function = (MPI_Win_errhandler_function *) function; \
	activity->mpi_args.MPI_Win_create_errhandler.errhandler = (MPI_Errhandler *) errhandler; \
};

#define GET_PTRS_VALUE_MPI_Win_create_errhandler(args) { \
	if (args->MPI_Win_create_errhandler.errhandler != NULL) { \
		args->MPI_Win_create_errhandler.errhandler__ref.val = *args->MPI_Win_create_errhandler.errhandler; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Gather_init` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Gather_init` function call.
 *
 * @struct args_MPI_Gather_init_t
 *
 * @note 
 *	int
 *	MPI_Gather_init (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			int recvcount (int)
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			int root (int)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *			MPI_Info info (struct mpi_info_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_Gather_init
struct args_MPI_Gather_init_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int recvcount;
	MPI_Datatype recvtype;
	int root;
	MPI_Comm comm;
	MPI_Info info;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Gather_init(activity) { \
	activity->mpi_args.MPI_Gather_init.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Gather_init.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Gather_init.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Gather_init.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Gather_init.recvcount = (int) recvcount; \
	activity->mpi_args.MPI_Gather_init.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Gather_init.root = (int) root; \
	activity->mpi_args.MPI_Gather_init.comm = (MPI_Comm) comm; \
	activity->mpi_args.MPI_Gather_init.info = (MPI_Info) info; \
	activity->mpi_args.MPI_Gather_init.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_Gather_init(args) { \
	if (args->MPI_Gather_init.request != NULL) { \
		args->MPI_Gather_init.request__ref.val = *args->MPI_Gather_init.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Neighbor_allgatherv` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Neighbor_allgatherv` function call.
 *
 * @struct args_MPI_Neighbor_allgatherv_t
 *
 * @note 
 *	int
 *	MPI_Neighbor_allgatherv (
 *			const void * sendbuf (const void *)
 *			int sendcount (int)
 *			MPI_Datatype sendtype (struct mpi_datatype_t *)
 *			void * recvbuf (void *)
 *			const int[] recvcounts (const int[])
 *			const int[] displs (const int[])
 *			MPI_Datatype recvtype (struct mpi_datatype_t *)
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Neighbor_allgatherv
struct args_MPI_Neighbor_allgatherv_t {
	void * sendbuf;
	int sendcount;
	MPI_Datatype sendtype;
	void * recvbuf;
	int(* recvcounts);
	struct {
		int val;
	} recvcounts__ref;
	int(* displs);
	struct {
		int val;
	} displs__ref;
	MPI_Datatype recvtype;
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Neighbor_allgatherv(activity) { \
	activity->mpi_args.MPI_Neighbor_allgatherv.sendbuf = (void *) sendbuf; \
	activity->mpi_args.MPI_Neighbor_allgatherv.sendcount = (int) sendcount; \
	activity->mpi_args.MPI_Neighbor_allgatherv.sendtype = (MPI_Datatype) sendtype; \
	activity->mpi_args.MPI_Neighbor_allgatherv.recvbuf = (void *) recvbuf; \
	activity->mpi_args.MPI_Neighbor_allgatherv.recvcounts = (int(*)) recvcounts; \
	activity->mpi_args.MPI_Neighbor_allgatherv.displs = (int(*)) displs; \
	activity->mpi_args.MPI_Neighbor_allgatherv.recvtype = (MPI_Datatype) recvtype; \
	activity->mpi_args.MPI_Neighbor_allgatherv.comm = (MPI_Comm) comm; \
};

#define GET_PTRS_VALUE_MPI_Neighbor_allgatherv(args) { \
	if (args->MPI_Neighbor_allgatherv.recvcounts != NULL) { \
		args->MPI_Neighbor_allgatherv.recvcounts__ref.val = *args->MPI_Neighbor_allgatherv.recvcounts; \
	} \
	if (args->MPI_Neighbor_allgatherv.displs != NULL) { \
		args->MPI_Neighbor_allgatherv.displs__ref.val = *args->MPI_Neighbor_allgatherv.displs; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_iwrite` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_iwrite` function call.
 *
 * @struct args_MPI_File_iwrite_t
 *
 * @note 
 *	int
 *	MPI_File_iwrite (
 *			MPI_File fh (struct mpi_file_t *)
 *			const void * buf (const void *)
 *			int count (int)
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *			MPI_Request * request (struct mpi_request_t **)
 *	)
 */
#if HAVE_MPI_File_iwrite
struct args_MPI_File_iwrite_t {
	MPI_File fh;
	void * buf;
	int count;
	MPI_Datatype datatype;
	MPI_Request * request;
	struct {
		MPI_Request val;
	} request__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_iwrite(activity) { \
	activity->mpi_args.MPI_File_iwrite.fh = (MPI_File) fh; \
	activity->mpi_args.MPI_File_iwrite.buf = (void *) buf; \
	activity->mpi_args.MPI_File_iwrite.count = (int) count; \
	activity->mpi_args.MPI_File_iwrite.datatype = (MPI_Datatype) datatype; \
	activity->mpi_args.MPI_File_iwrite.request = (MPI_Request *) request; \
};

#define GET_PTRS_VALUE_MPI_File_iwrite(args) { \
	if (args->MPI_File_iwrite.request != NULL) { \
		args->MPI_File_iwrite.request__ref.val = *args->MPI_File_iwrite.request; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Buffer_attach` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Buffer_attach` function call.
 *
 * @struct args_MPI_Buffer_attach_t
 *
 * @note 
 *	int
 *	MPI_Buffer_attach (
 *			void * buffer (void *)
 *			int size (int)
 *	)
 */
#if HAVE_MPI_Buffer_attach
struct args_MPI_Buffer_attach_t {
	void * buffer;
	int size;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Buffer_attach(activity) { \
	activity->mpi_args.MPI_Buffer_attach.buffer = (void *) buffer; \
	activity->mpi_args.MPI_Buffer_attach.size = (int) size; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_set_errhandler` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_set_errhandler` function call.
 *
 * @struct args_MPI_Session_set_errhandler_t
 *
 * @note 
 *	int
 *	MPI_Session_set_errhandler (
 *			MPI_Session session (struct mpi_instance_t *)
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *	)
 */
#if HAVE_MPI_Session_set_errhandler
struct args_MPI_Session_set_errhandler_t {
	MPI_Session session;
	MPI_Errhandler errhandler;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_set_errhandler(activity) { \
	activity->mpi_args.MPI_Session_set_errhandler.session = (MPI_Session) session; \
	activity->mpi_args.MPI_Session_set_errhandler.errhandler = (MPI_Errhandler) errhandler; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_start` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_start` function call.
 *
 * @struct args_MPI_Win_start_t
 *
 * @note 
 *	int
 *	MPI_Win_start (
 *			MPI_Group group (struct mpi_group_t *)
 *			int mpi_assert (int)
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_start
struct args_MPI_Win_start_t {
	MPI_Group group;
	int mpi_assert;
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_start(activity) { \
	activity->mpi_args.MPI_Win_start.group = (MPI_Group) group; \
	activity->mpi_args.MPI_Win_start.mpi_assert = (int) mpi_assert; \
	activity->mpi_args.MPI_Win_start.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_f2c` function call.
 *
 * @struct args_MPI_Info_f2c_t
 *
 * @note 
 *	MPI_Info
 *	MPI_Info_f2c (
 *			int info (int)
 *	)
 */
#if HAVE_MPI_Info_f2c
struct args_MPI_Info_f2c_t {
	int info;
	MPI_Info retval;
};

#define GET_ARGS_VALUE_MPI_Info_f2c(activity) { \
	activity->mpi_args.MPI_Info_f2c.info = (int) info; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Info_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Info_c2f` function call.
 *
 * @struct args_MPI_Info_c2f_t
 *
 * @note 
 *	int
 *	MPI_Info_c2f (
 *			MPI_Info info (struct mpi_info_t *)
 *	)
 */
#if HAVE_MPI_Info_c2f
struct args_MPI_Info_c2f_t {
	MPI_Info info;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Info_c2f(activity) { \
	activity->mpi_args.MPI_Info_c2f.info = (MPI_Info) info; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Op_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Op_c2f` function call.
 *
 * @struct args_MPI_Op_c2f_t
 *
 * @note 
 *	int
 *	MPI_Op_c2f (
 *			MPI_Op op (struct mpi_op_t *)
 *	)
 */
#if HAVE_MPI_Op_c2f
struct args_MPI_Op_c2f_t {
	MPI_Op op;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Op_c2f(activity) { \
	activity->mpi_args.MPI_Op_c2f.op = (MPI_Op) op; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_c2f` function call.
 *
 * @struct args_MPI_Win_c2f_t
 *
 * @note 
 *	int
 *	MPI_Win_c2f (
 *			MPI_Win win (struct mpi_win_t *)
 *	)
 */
#if HAVE_MPI_Win_c2f
struct args_MPI_Win_c2f_t {
	MPI_Win win;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Win_c2f(activity) { \
	activity->mpi_args.MPI_Win_c2f.win = (MPI_Win) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_f2c` function call.
 *
 * @struct args_MPI_Group_f2c_t
 *
 * @note 
 *	MPI_Group
 *	MPI_Group_f2c (
 *			int group (int)
 *	)
 */
#if HAVE_MPI_Group_f2c
struct args_MPI_Group_f2c_t {
	int group;
	MPI_Group retval;
};

#define GET_ARGS_VALUE_MPI_Group_f2c(activity) { \
	activity->mpi_args.MPI_Group_f2c.group = (int) group; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_c2f` function call.
 *
 * @struct args_MPI_File_c2f_t
 *
 * @note 
 *	int
 *	MPI_File_c2f (
 *			MPI_File file (struct mpi_file_t *)
 *	)
 */
#if HAVE_MPI_File_c2f
struct args_MPI_File_c2f_t {
	MPI_File file;
	int retval;
};

#define GET_ARGS_VALUE_MPI_File_c2f(activity) { \
	activity->mpi_args.MPI_File_c2f.file = (MPI_File) file; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Request_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Request_c2f` function call.
 *
 * @struct args_MPI_Request_c2f_t
 *
 * @note 
 *	int
 *	MPI_Request_c2f (
 *			MPI_Request request (struct mpi_request_t *)
 *	)
 */
#if HAVE_MPI_Request_c2f
struct args_MPI_Request_c2f_t {
	MPI_Request request;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Request_c2f(activity) { \
	activity->mpi_args.MPI_Request_c2f.request = (MPI_Request) request; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_File_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_File_f2c` function call.
 *
 * @struct args_MPI_File_f2c_t
 *
 * @note 
 *	MPI_File
 *	MPI_File_f2c (
 *			int file (int)
 *	)
 */
#if HAVE_MPI_File_f2c
struct args_MPI_File_f2c_t {
	int file;
	MPI_File retval;
};

#define GET_ARGS_VALUE_MPI_File_f2c(activity) { \
	activity->mpi_args.MPI_File_f2c.file = (int) file; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_f2c` function call.
 *
 * @struct args_MPI_Session_f2c_t
 *
 * @note 
 *	MPI_Session
 *	MPI_Session_f2c (
 *			int session (int)
 *	)
 */
#if HAVE_MPI_Session_f2c
struct args_MPI_Session_f2c_t {
	int session;
	MPI_Session retval;
};

#define GET_ARGS_VALUE_MPI_Session_f2c(activity) { \
	activity->mpi_args.MPI_Session_f2c.session = (int) session; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_f082f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_f082f` function call.
 *
 * @struct args_MPI_Status_f082f_t
 *
 * @note 
 *	int
 *	MPI_Status_f082f (
 *			const MPI_F08_status * f08_status (const struct opaque * *)
 *			int * f_status (int *)
 *	)
 */
#if HAVE_MPI_Status_f082f
struct args_MPI_Status_f082f_t {
	MPI_F08_status * f08_status;
	struct {
		MPI_F08_status val;
	} f08_status__ref;
	int * f_status;
	struct {
		int val;
	} f_status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_f082f(activity) { \
	activity->mpi_args.MPI_Status_f082f.f08_status = (MPI_F08_status *) f08_status; \
	activity->mpi_args.MPI_Status_f082f.f_status = (int *) f_status; \
};

#define GET_PTRS_VALUE_MPI_Status_f082f(args) { \
	if (args->MPI_Status_f082f.f08_status != NULL) { \
		args->MPI_Status_f082f.f08_status__ref.val = *args->MPI_Status_f082f.f08_status; \
	} \
	if (args->MPI_Status_f082f.f_status != NULL) { \
		args->MPI_Status_f082f.f_status__ref.val = *args->MPI_Status_f082f.f_status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_c2f08` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_c2f08` function call.
 *
 * @struct args_MPI_Status_c2f08_t
 *
 * @note 
 *	int
 *	MPI_Status_c2f08 (
 *			const MPI_Status * c_status (const struct opaque * *)
 *			MPI_F08_status * f08_status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Status_c2f08
struct args_MPI_Status_c2f08_t {
	MPI_Status * c_status;
	struct {
		MPI_Status val;
	} c_status__ref;
	MPI_F08_status * f08_status;
	struct {
		MPI_F08_status val;
	} f08_status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_c2f08(activity) { \
	activity->mpi_args.MPI_Status_c2f08.c_status = (MPI_Status *) c_status; \
	activity->mpi_args.MPI_Status_c2f08.f08_status = (MPI_F08_status *) f08_status; \
};

#define GET_PTRS_VALUE_MPI_Status_c2f08(args) { \
	if (args->MPI_Status_c2f08.c_status != NULL) { \
		args->MPI_Status_c2f08.c_status__ref.val = *args->MPI_Status_c2f08.c_status; \
	} \
	if (args->MPI_Status_c2f08.f08_status != NULL) { \
		args->MPI_Status_c2f08.f08_status__ref.val = *args->MPI_Status_c2f08.f08_status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_f2c` function call.
 *
 * @struct args_MPI_Type_f2c_t
 *
 * @note 
 *	MPI_Datatype
 *	MPI_Type_f2c (
 *			int datatype (int)
 *	)
 */
#if HAVE_MPI_Type_f2c
struct args_MPI_Type_f2c_t {
	int datatype;
	MPI_Datatype retval;
};

#define GET_ARGS_VALUE_MPI_Type_f2c(activity) { \
	activity->mpi_args.MPI_Type_f2c.datatype = (int) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Message_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Message_c2f` function call.
 *
 * @struct args_MPI_Message_c2f_t
 *
 * @note 
 *	int
 *	MPI_Message_c2f (
 *			MPI_Message message (struct mpi_message_t *)
 *	)
 */
#if HAVE_MPI_Message_c2f
struct args_MPI_Message_c2f_t {
	MPI_Message message;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Message_c2f(activity) { \
	activity->mpi_args.MPI_Message_c2f.message = (MPI_Message) message; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Session_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Session_c2f` function call.
 *
 * @struct args_MPI_Session_c2f_t
 *
 * @note 
 *	int
 *	MPI_Session_c2f (
 *			const MPI_Session session (const struct mpi_instance_t *)
 *	)
 */
#if HAVE_MPI_Session_c2f
struct args_MPI_Session_c2f_t {
	MPI_Session session;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Session_c2f(activity) { \
	activity->mpi_args.MPI_Session_c2f.session = (MPI_Session) session; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Message_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Message_f2c` function call.
 *
 * @struct args_MPI_Message_f2c_t
 *
 * @note 
 *	MPI_Message
 *	MPI_Message_f2c (
 *			int message (int)
 *	)
 */
#if HAVE_MPI_Message_f2c
struct args_MPI_Message_f2c_t {
	int message;
	MPI_Message retval;
};

#define GET_ARGS_VALUE_MPI_Message_f2c(activity) { \
	activity->mpi_args.MPI_Message_f2c.message = (int) message; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Errhandler_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Errhandler_f2c` function call.
 *
 * @struct args_MPI_Errhandler_f2c_t
 *
 * @note 
 *	MPI_Errhandler
 *	MPI_Errhandler_f2c (
 *			int errhandler (int)
 *	)
 */
#if HAVE_MPI_Errhandler_f2c
struct args_MPI_Errhandler_f2c_t {
	int errhandler;
	MPI_Errhandler retval;
};

#define GET_ARGS_VALUE_MPI_Errhandler_f2c(activity) { \
	activity->mpi_args.MPI_Errhandler_f2c.errhandler = (int) errhandler; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Request_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Request_f2c` function call.
 *
 * @struct args_MPI_Request_f2c_t
 *
 * @note 
 *	MPI_Request
 *	MPI_Request_f2c (
 *			int request (int)
 *	)
 */
#if HAVE_MPI_Request_f2c
struct args_MPI_Request_f2c_t {
	int request;
	MPI_Request retval;
};

#define GET_ARGS_VALUE_MPI_Request_f2c(activity) { \
	activity->mpi_args.MPI_Request_f2c.request = (int) request; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_c2f` function call.
 *
 * @struct args_MPI_Status_c2f_t
 *
 * @note 
 *	int
 *	MPI_Status_c2f (
 *			const MPI_Status * c_status (const struct opaque * *)
 *			int * f_status (int *)
 *	)
 */
#if HAVE_MPI_Status_c2f
struct args_MPI_Status_c2f_t {
	MPI_Status * c_status;
	struct {
		MPI_Status val;
	} c_status__ref;
	int * f_status;
	struct {
		int val;
	} f_status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_c2f(activity) { \
	activity->mpi_args.MPI_Status_c2f.c_status = (MPI_Status *) c_status; \
	activity->mpi_args.MPI_Status_c2f.f_status = (int *) f_status; \
};

#define GET_PTRS_VALUE_MPI_Status_c2f(args) { \
	if (args->MPI_Status_c2f.c_status != NULL) { \
		args->MPI_Status_c2f.c_status__ref.val = *args->MPI_Status_c2f.c_status; \
	} \
	if (args->MPI_Status_c2f.f_status != NULL) { \
		args->MPI_Status_c2f.f_status__ref.val = *args->MPI_Status_c2f.f_status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_f2c` function call.
 *
 * @struct args_MPI_Comm_f2c_t
 *
 * @note 
 *	MPI_Comm
 *	MPI_Comm_f2c (
 *			int comm (int)
 *	)
 */
#if HAVE_MPI_Comm_f2c
struct args_MPI_Comm_f2c_t {
	int comm;
	MPI_Comm retval;
};

#define GET_ARGS_VALUE_MPI_Comm_f2c(activity) { \
	activity->mpi_args.MPI_Comm_f2c.comm = (int) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Comm_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Comm_c2f` function call.
 *
 * @struct args_MPI_Comm_c2f_t
 *
 * @note 
 *	int
 *	MPI_Comm_c2f (
 *			MPI_Comm comm (struct mpi_communicator_t *)
 *	)
 */
#if HAVE_MPI_Comm_c2f
struct args_MPI_Comm_c2f_t {
	MPI_Comm comm;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Comm_c2f(activity) { \
	activity->mpi_args.MPI_Comm_c2f.comm = (MPI_Comm) comm; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Group_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Group_c2f` function call.
 *
 * @struct args_MPI_Group_c2f_t
 *
 * @note 
 *	int
 *	MPI_Group_c2f (
 *			MPI_Group group (struct mpi_group_t *)
 *	)
 */
#if HAVE_MPI_Group_c2f
struct args_MPI_Group_c2f_t {
	MPI_Group group;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Group_c2f(activity) { \
	activity->mpi_args.MPI_Group_c2f.group = (MPI_Group) group; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Win_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Win_f2c` function call.
 *
 * @struct args_MPI_Win_f2c_t
 *
 * @note 
 *	MPI_Win
 *	MPI_Win_f2c (
 *			int win (int)
 *	)
 */
#if HAVE_MPI_Win_f2c
struct args_MPI_Win_f2c_t {
	int win;
	MPI_Win retval;
};

#define GET_ARGS_VALUE_MPI_Win_f2c(activity) { \
	activity->mpi_args.MPI_Win_f2c.win = (int) win; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_f082c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_f082c` function call.
 *
 * @struct args_MPI_Status_f082c_t
 *
 * @note 
 *	int
 *	MPI_Status_f082c (
 *			const MPI_F08_status * f08_status (const struct opaque * *)
 *			MPI_Status * c_status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Status_f082c
struct args_MPI_Status_f082c_t {
	MPI_F08_status * f08_status;
	struct {
		MPI_F08_status val;
	} f08_status__ref;
	MPI_Status * c_status;
	struct {
		MPI_Status val;
	} c_status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_f082c(activity) { \
	activity->mpi_args.MPI_Status_f082c.f08_status = (MPI_F08_status *) f08_status; \
	activity->mpi_args.MPI_Status_f082c.c_status = (MPI_Status *) c_status; \
};

#define GET_PTRS_VALUE_MPI_Status_f082c(args) { \
	if (args->MPI_Status_f082c.f08_status != NULL) { \
		args->MPI_Status_f082c.f08_status__ref.val = *args->MPI_Status_f082c.f08_status; \
	} \
	if (args->MPI_Status_f082c.c_status != NULL) { \
		args->MPI_Status_f082c.c_status__ref.val = *args->MPI_Status_f082c.c_status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Errhandler_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Errhandler_c2f` function call.
 *
 * @struct args_MPI_Errhandler_c2f_t
 *
 * @note 
 *	int
 *	MPI_Errhandler_c2f (
 *			MPI_Errhandler errhandler (struct mpi_errhandler_t *)
 *	)
 */
#if HAVE_MPI_Errhandler_c2f
struct args_MPI_Errhandler_c2f_t {
	MPI_Errhandler errhandler;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Errhandler_c2f(activity) { \
	activity->mpi_args.MPI_Errhandler_c2f.errhandler = (MPI_Errhandler) errhandler; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_f2f08` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_f2f08` function call.
 *
 * @struct args_MPI_Status_f2f08_t
 *
 * @note 
 *	int
 *	MPI_Status_f2f08 (
 *			const int * f_status (const int *)
 *			MPI_F08_status * f08_status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Status_f2f08
struct args_MPI_Status_f2f08_t {
	int * f_status;
	struct {
		int val;
	} f_status__ref;
	MPI_F08_status * f08_status;
	struct {
		MPI_F08_status val;
	} f08_status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_f2f08(activity) { \
	activity->mpi_args.MPI_Status_f2f08.f_status = (int *) f_status; \
	activity->mpi_args.MPI_Status_f2f08.f08_status = (MPI_F08_status *) f08_status; \
};

#define GET_PTRS_VALUE_MPI_Status_f2f08(args) { \
	if (args->MPI_Status_f2f08.f_status != NULL) { \
		args->MPI_Status_f2f08.f_status__ref.val = *args->MPI_Status_f2f08.f_status; \
	} \
	if (args->MPI_Status_f2f08.f08_status != NULL) { \
		args->MPI_Status_f2f08.f08_status__ref.val = *args->MPI_Status_f2f08.f08_status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Type_c2f` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Type_c2f` function call.
 *
 * @struct args_MPI_Type_c2f_t
 *
 * @note 
 *	int
 *	MPI_Type_c2f (
 *			MPI_Datatype datatype (struct mpi_datatype_t *)
 *	)
 */
#if HAVE_MPI_Type_c2f
struct args_MPI_Type_c2f_t {
	MPI_Datatype datatype;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Type_c2f(activity) { \
	activity->mpi_args.MPI_Type_c2f.datatype = (MPI_Datatype) datatype; \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Status_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Status_f2c` function call.
 *
 * @struct args_MPI_Status_f2c_t
 *
 * @note 
 *	int
 *	MPI_Status_f2c (
 *			const int * f_status (const int *)
 *			MPI_Status * c_status (struct opaque **)
 *	)
 */
#if HAVE_MPI_Status_f2c
struct args_MPI_Status_f2c_t {
	int * f_status;
	struct {
		int val;
	} f_status__ref;
	MPI_Status * c_status;
	struct {
		MPI_Status val;
	} c_status__ref;
	int retval;
};

#define GET_ARGS_VALUE_MPI_Status_f2c(activity) { \
	activity->mpi_args.MPI_Status_f2c.f_status = (int *) f_status; \
	activity->mpi_args.MPI_Status_f2c.c_status = (MPI_Status *) c_status; \
};

#define GET_PTRS_VALUE_MPI_Status_f2c(args) { \
	if (args->MPI_Status_f2c.f_status != NULL) { \
		args->MPI_Status_f2c.f_status__ref.val = *args->MPI_Status_f2c.f_status; \
	} \
	if (args->MPI_Status_f2c.c_status != NULL) { \
		args->MPI_Status_f2c.c_status__ref.val = *args->MPI_Status_f2c.c_status; \
	} \
};

#endif

/**
 * @brief Structure to hold the arguments for the `MPI_Op_f2c` function.
 *
 * This structure encapsulates the parameters and return value used in the 
 * `MPI_Op_f2c` function call.
 *
 * @struct args_MPI_Op_f2c_t
 *
 * @note 
 *	MPI_Op
 *	MPI_Op_f2c (
 *			int op (int)
 *	)
 */
#if HAVE_MPI_Op_f2c
struct args_MPI_Op_f2c_t {
	int op;
	MPI_Op retval;
};

#define GET_ARGS_VALUE_MPI_Op_f2c(activity) { \
	activity->mpi_args.MPI_Op_f2c.op = (int) op; \
};

#endif



/**
 * @brief Union representing argument structures for different MPI API calls.
 *
 * This union allows storing parameters for various MPI API functions,
 * ensuring type safety and efficient memory usage.
 *
 * @union mpi_api_args_u 
 * @typedef mpi_api_args_t 
 */
typedef union mpi_api_args_u {
    FOR_EACH_MPI_FUNC(GET_ARGS_STRUCT_OF)
} mpi_api_args_t;


/**
 * @brief Retrieves pointer-based argument values for MPI API calls.
 *
 * This function extracts pointer-based arguments from the provided `mpi_api_args_t`
 * structure based on the given MPI API ID.
 *
 * @param[in] id The MPI API function identifier.
 * @param[in,out] args Pointer to the MPI API arguments structure.
 * @param[in] is_enter Boolean flag indicating whether this function is handling an "enter" or "exit" event.
 */
static inline void get_mpi_pointed_args_for(mpi_api_id_t id, mpi_api_args_t* args, bool is_enter) 
{
    if (!is_enter) {
        switch(id) {
			#if HAVE_MPI_Init
			case MPI_API_ID_MPI_Init : 
				GET_PTRS_VALUE_MPI_Init(args);
				return;
			#endif
			#if HAVE_MPI_Init_thread
			case MPI_API_ID_MPI_Init_thread : 
				GET_PTRS_VALUE_MPI_Init_thread(args);
				return;
			#endif
			#if HAVE_MPI_Initialized
			case MPI_API_ID_MPI_Initialized : 
				GET_PTRS_VALUE_MPI_Initialized(args);
				return;
			#endif
			#if HAVE_MPI_Query_thread
			case MPI_API_ID_MPI_Query_thread : 
				GET_PTRS_VALUE_MPI_Query_thread(args);
				return;
			#endif
			#if HAVE_MPI_Recv
			case MPI_API_ID_MPI_Recv : 
				GET_PTRS_VALUE_MPI_Recv(args);
				return;
			#endif
			#if HAVE_MPI_Sendrecv
			case MPI_API_ID_MPI_Sendrecv : 
				GET_PTRS_VALUE_MPI_Sendrecv(args);
				return;
			#endif
			#if HAVE_MPI_Sendrecv_replace
			case MPI_API_ID_MPI_Sendrecv_replace : 
				GET_PTRS_VALUE_MPI_Sendrecv_replace(args);
				return;
			#endif
			#if HAVE_MPI_Isend
			case MPI_API_ID_MPI_Isend : 
				GET_PTRS_VALUE_MPI_Isend(args);
				return;
			#endif
			#if HAVE_MPI_Irecv
			case MPI_API_ID_MPI_Irecv : 
				GET_PTRS_VALUE_MPI_Irecv(args);
				return;
			#endif
			#if HAVE_MPI_Wait
			case MPI_API_ID_MPI_Wait : 
				GET_PTRS_VALUE_MPI_Wait(args);
				return;
			#endif
			#if HAVE_MPI_Waitall
			case MPI_API_ID_MPI_Waitall : 
				GET_PTRS_VALUE_MPI_Waitall(args);
				return;
			#endif
			#if HAVE_MPI_Waitany
			case MPI_API_ID_MPI_Waitany : 
				GET_PTRS_VALUE_MPI_Waitany(args);
				return;
			#endif
			#if HAVE_MPI_Waitsome
			case MPI_API_ID_MPI_Waitsome : 
				GET_PTRS_VALUE_MPI_Waitsome(args);
				return;
			#endif
			#if HAVE_MPI_Test
			case MPI_API_ID_MPI_Test : 
				GET_PTRS_VALUE_MPI_Test(args);
				return;
			#endif
			#if HAVE_MPI_Testall
			case MPI_API_ID_MPI_Testall : 
				GET_PTRS_VALUE_MPI_Testall(args);
				return;
			#endif
			#if HAVE_MPI_Testany
			case MPI_API_ID_MPI_Testany : 
				GET_PTRS_VALUE_MPI_Testany(args);
				return;
			#endif
			#if HAVE_MPI_Testsome
			case MPI_API_ID_MPI_Testsome : 
				GET_PTRS_VALUE_MPI_Testsome(args);
				return;
			#endif
			#if HAVE_MPI_Cancel
			case MPI_API_ID_MPI_Cancel : 
				GET_PTRS_VALUE_MPI_Cancel(args);
				return;
			#endif
			#if HAVE_MPI_Type_contiguous
			case MPI_API_ID_MPI_Type_contiguous : 
				GET_PTRS_VALUE_MPI_Type_contiguous(args);
				return;
			#endif
			#if HAVE_MPI_Type_vector
			case MPI_API_ID_MPI_Type_vector : 
				GET_PTRS_VALUE_MPI_Type_vector(args);
				return;
			#endif
			#if HAVE_MPI_Type_indexed
			case MPI_API_ID_MPI_Type_indexed : 
				GET_PTRS_VALUE_MPI_Type_indexed(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_indexed_block
			case MPI_API_ID_MPI_Type_create_indexed_block : 
				GET_PTRS_VALUE_MPI_Type_create_indexed_block(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_struct
			case MPI_API_ID_MPI_Type_create_struct : 
				GET_PTRS_VALUE_MPI_Type_create_struct(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_resized
			case MPI_API_ID_MPI_Type_create_resized : 
				GET_PTRS_VALUE_MPI_Type_create_resized(args);
				return;
			#endif
			#if HAVE_MPI_Type_commit
			case MPI_API_ID_MPI_Type_commit : 
				GET_PTRS_VALUE_MPI_Type_commit(args);
				return;
			#endif
			#if HAVE_MPI_Get_count
			case MPI_API_ID_MPI_Get_count : 
				GET_PTRS_VALUE_MPI_Get_count(args);
				return;
			#endif
			#if HAVE_MPI_Get_elements
			case MPI_API_ID_MPI_Get_elements : 
				GET_PTRS_VALUE_MPI_Get_elements(args);
				return;
			#endif
			#if HAVE_MPI_Pack
			case MPI_API_ID_MPI_Pack : 
				GET_PTRS_VALUE_MPI_Pack(args);
				return;
			#endif
			#if HAVE_MPI_Unpack
			case MPI_API_ID_MPI_Unpack : 
				GET_PTRS_VALUE_MPI_Unpack(args);
				return;
			#endif
			#if HAVE_MPI_Pack_size
			case MPI_API_ID_MPI_Pack_size : 
				GET_PTRS_VALUE_MPI_Pack_size(args);
				return;
			#endif
			#if HAVE_MPI_Gatherv
			case MPI_API_ID_MPI_Gatherv : 
				GET_PTRS_VALUE_MPI_Gatherv(args);
				return;
			#endif
			#if HAVE_MPI_Scatterv
			case MPI_API_ID_MPI_Scatterv : 
				GET_PTRS_VALUE_MPI_Scatterv(args);
				return;
			#endif
			#if HAVE_MPI_Allgatherv
			case MPI_API_ID_MPI_Allgatherv : 
				GET_PTRS_VALUE_MPI_Allgatherv(args);
				return;
			#endif
			#if HAVE_MPI_Alltoallv
			case MPI_API_ID_MPI_Alltoallv : 
				GET_PTRS_VALUE_MPI_Alltoallv(args);
				return;
			#endif
			#if HAVE_MPI_Reduce_scatter
			case MPI_API_ID_MPI_Reduce_scatter : 
				GET_PTRS_VALUE_MPI_Reduce_scatter(args);
				return;
			#endif
			#if HAVE_MPI_Comm_size
			case MPI_API_ID_MPI_Comm_size : 
				GET_PTRS_VALUE_MPI_Comm_size(args);
				return;
			#endif
			#if HAVE_MPI_Comm_rank
			case MPI_API_ID_MPI_Comm_rank : 
				GET_PTRS_VALUE_MPI_Comm_rank(args);
				return;
			#endif
			#if HAVE_MPI_Comm_group
			case MPI_API_ID_MPI_Comm_group : 
				GET_PTRS_VALUE_MPI_Comm_group(args);
				return;
			#endif
			#if HAVE_MPI_Comm_dup
			case MPI_API_ID_MPI_Comm_dup : 
				GET_PTRS_VALUE_MPI_Comm_dup(args);
				return;
			#endif
			#if HAVE_MPI_Comm_create
			case MPI_API_ID_MPI_Comm_create : 
				GET_PTRS_VALUE_MPI_Comm_create(args);
				return;
			#endif
			#if HAVE_MPI_Comm_split
			case MPI_API_ID_MPI_Comm_split : 
				GET_PTRS_VALUE_MPI_Comm_split(args);
				return;
			#endif
			#if HAVE_MPI_Comm_test_inter
			case MPI_API_ID_MPI_Comm_test_inter : 
				GET_PTRS_VALUE_MPI_Comm_test_inter(args);
				return;
			#endif
			#if HAVE_MPI_Comm_remote_size
			case MPI_API_ID_MPI_Comm_remote_size : 
				GET_PTRS_VALUE_MPI_Comm_remote_size(args);
				return;
			#endif
			#if HAVE_MPI_Comm_remote_group
			case MPI_API_ID_MPI_Comm_remote_group : 
				GET_PTRS_VALUE_MPI_Comm_remote_group(args);
				return;
			#endif
			#if HAVE_MPI_Comm_compare
			case MPI_API_ID_MPI_Comm_compare : 
				GET_PTRS_VALUE_MPI_Comm_compare(args);
				return;
			#endif
			#if HAVE_MPI_Comm_create_keyval
			case MPI_API_ID_MPI_Comm_create_keyval : 
				GET_PTRS_VALUE_MPI_Comm_create_keyval(args);
				return;
			#endif
			#if HAVE_MPI_Comm_get_attr
			case MPI_API_ID_MPI_Comm_get_attr : 
				GET_PTRS_VALUE_MPI_Comm_get_attr(args);
				return;
			#endif
			#if HAVE_MPI_Comm_get_name
			case MPI_API_ID_MPI_Comm_get_name : 
				GET_PTRS_VALUE_MPI_Comm_get_name(args);
				return;
			#endif
			#if HAVE_MPI_Comm_set_name
			case MPI_API_ID_MPI_Comm_set_name : 
				GET_PTRS_VALUE_MPI_Comm_set_name(args);
				return;
			#endif
			#if HAVE_MPI_Group_size
			case MPI_API_ID_MPI_Group_size : 
				GET_PTRS_VALUE_MPI_Group_size(args);
				return;
			#endif
			#if HAVE_MPI_Group_rank
			case MPI_API_ID_MPI_Group_rank : 
				GET_PTRS_VALUE_MPI_Group_rank(args);
				return;
			#endif
			#if HAVE_MPI_Group_translate_ranks
			case MPI_API_ID_MPI_Group_translate_ranks : 
				GET_PTRS_VALUE_MPI_Group_translate_ranks(args);
				return;
			#endif
			#if HAVE_MPI_Group_compare
			case MPI_API_ID_MPI_Group_compare : 
				GET_PTRS_VALUE_MPI_Group_compare(args);
				return;
			#endif
			#if HAVE_MPI_Group_union
			case MPI_API_ID_MPI_Group_union : 
				GET_PTRS_VALUE_MPI_Group_union(args);
				return;
			#endif
			#if HAVE_MPI_Group_intersection
			case MPI_API_ID_MPI_Group_intersection : 
				GET_PTRS_VALUE_MPI_Group_intersection(args);
				return;
			#endif
			#if HAVE_MPI_Group_difference
			case MPI_API_ID_MPI_Group_difference : 
				GET_PTRS_VALUE_MPI_Group_difference(args);
				return;
			#endif
			#if HAVE_MPI_Group_incl
			case MPI_API_ID_MPI_Group_incl : 
				GET_PTRS_VALUE_MPI_Group_incl(args);
				return;
			#endif
			#if HAVE_MPI_Group_excl
			case MPI_API_ID_MPI_Group_excl : 
				GET_PTRS_VALUE_MPI_Group_excl(args);
				return;
			#endif
			#if HAVE_MPI_Group_range_incl
			case MPI_API_ID_MPI_Group_range_incl : 
				GET_PTRS_VALUE_MPI_Group_range_incl(args);
				return;
			#endif
			#if HAVE_MPI_Group_range_excl
			case MPI_API_ID_MPI_Group_range_excl : 
				GET_PTRS_VALUE_MPI_Group_range_excl(args);
				return;
			#endif
			#if HAVE_MPI_Op_create
			case MPI_API_ID_MPI_Op_create : 
				GET_PTRS_VALUE_MPI_Op_create(args);
				return;
			#endif
			#if HAVE_MPI_Get_address
			case MPI_API_ID_MPI_Get_address : 
				GET_PTRS_VALUE_MPI_Get_address(args);
				return;
			#endif
			#if HAVE_MPI_Get_elements_x
			case MPI_API_ID_MPI_Get_elements_x : 
				GET_PTRS_VALUE_MPI_Get_elements_x(args);
				return;
			#endif
			#if HAVE_MPI_Cart_shift
			case MPI_API_ID_MPI_Cart_shift : 
				GET_PTRS_VALUE_MPI_Cart_shift(args);
				return;
			#endif
			#if HAVE_MPI_File_get_byte_offset
			case MPI_API_ID_MPI_File_get_byte_offset : 
				GET_PTRS_VALUE_MPI_File_get_byte_offset(args);
				return;
			#endif
			#if HAVE_MPI_Win_get_info
			case MPI_API_ID_MPI_Win_get_info : 
				GET_PTRS_VALUE_MPI_Win_get_info(args);
				return;
			#endif
			#if HAVE_MPI_Rput
			case MPI_API_ID_MPI_Rput : 
				GET_PTRS_VALUE_MPI_Rput(args);
				return;
			#endif
			#if HAVE_MPI_Dist_graph_neighbors_count
			case MPI_API_ID_MPI_Dist_graph_neighbors_count : 
				GET_PTRS_VALUE_MPI_Dist_graph_neighbors_count(args);
				return;
			#endif
			#if HAVE_MPI_Ireduce
			case MPI_API_ID_MPI_Ireduce : 
				GET_PTRS_VALUE_MPI_Ireduce(args);
				return;
			#endif
			#if HAVE_MPI_Psend_init
			case MPI_API_ID_MPI_Psend_init : 
				GET_PTRS_VALUE_MPI_Psend_init(args);
				return;
			#endif
			#if HAVE_MPI_Reduce_init
			case MPI_API_ID_MPI_Reduce_init : 
				GET_PTRS_VALUE_MPI_Reduce_init(args);
				return;
			#endif
			#if HAVE_MPI_Rsend_init
			case MPI_API_ID_MPI_Rsend_init : 
				GET_PTRS_VALUE_MPI_Rsend_init(args);
				return;
			#endif
			#if HAVE_MPI_File_write_at_all
			case MPI_API_ID_MPI_File_write_at_all : 
				GET_PTRS_VALUE_MPI_File_write_at_all(args);
				return;
			#endif
			#if HAVE_MPI_File_write_ordered_end
			case MPI_API_ID_MPI_File_write_ordered_end : 
				GET_PTRS_VALUE_MPI_File_write_ordered_end(args);
				return;
			#endif
			#if HAVE_MPI_Win_shared_query
			case MPI_API_ID_MPI_Win_shared_query : 
				GET_PTRS_VALUE_MPI_Win_shared_query(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_name
			case MPI_API_ID_MPI_Type_get_name : 
				GET_PTRS_VALUE_MPI_Type_get_name(args);
				return;
			#endif
			#if HAVE_MPI_File_get_atomicity
			case MPI_API_ID_MPI_File_get_atomicity : 
				GET_PTRS_VALUE_MPI_File_get_atomicity(args);
				return;
			#endif
			#if HAVE_MPI_Group_from_session_pset
			case MPI_API_ID_MPI_Group_from_session_pset : 
				GET_PTRS_VALUE_MPI_Group_from_session_pset(args);
				return;
			#endif
			#if HAVE_MPI_Comm_idup
			case MPI_API_ID_MPI_Comm_idup : 
				GET_PTRS_VALUE_MPI_Comm_idup(args);
				return;
			#endif
			#if HAVE_MPI_Win_get_name
			case MPI_API_ID_MPI_Win_get_name : 
				GET_PTRS_VALUE_MPI_Win_get_name(args);
				return;
			#endif
			#if HAVE_MPI_Allgatherv_init
			case MPI_API_ID_MPI_Allgatherv_init : 
				GET_PTRS_VALUE_MPI_Allgatherv_init(args);
				return;
			#endif
			#if HAVE_MPI_Comm_dup_with_info
			case MPI_API_ID_MPI_Comm_dup_with_info : 
				GET_PTRS_VALUE_MPI_Comm_dup_with_info(args);
				return;
			#endif
			#if HAVE_MPI_Session_get_num_psets
			case MPI_API_ID_MPI_Session_get_num_psets : 
				GET_PTRS_VALUE_MPI_Session_get_num_psets(args);
				return;
			#endif
			#if HAVE_MPI_Igather
			case MPI_API_ID_MPI_Igather : 
				GET_PTRS_VALUE_MPI_Igather(args);
				return;
			#endif
			#if HAVE_MPI_File_read_at
			case MPI_API_ID_MPI_File_read_at : 
				GET_PTRS_VALUE_MPI_File_read_at(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_hvector
			case MPI_API_ID_MPI_Type_create_hvector : 
				GET_PTRS_VALUE_MPI_Type_create_hvector(args);
				return;
			#endif
			#if HAVE_MPI_Grequest_start
			case MPI_API_ID_MPI_Grequest_start : 
				GET_PTRS_VALUE_MPI_Grequest_start(args);
				return;
			#endif
			#if HAVE_MPI_Bsend_init
			case MPI_API_ID_MPI_Bsend_init : 
				GET_PTRS_VALUE_MPI_Bsend_init(args);
				return;
			#endif
			#if HAVE_MPI_Type_set_name
			case MPI_API_ID_MPI_Type_set_name : 
				GET_PTRS_VALUE_MPI_Type_set_name(args);
				return;
			#endif
			#if HAVE_MPI_Comm_split_type
			case MPI_API_ID_MPI_Comm_split_type : 
				GET_PTRS_VALUE_MPI_Comm_split_type(args);
				return;
			#endif
			#if HAVE_MPI_File_read_at_all_end
			case MPI_API_ID_MPI_File_read_at_all_end : 
				GET_PTRS_VALUE_MPI_File_read_at_all_end(args);
				return;
			#endif
			#if HAVE_MPI_File_write_all
			case MPI_API_ID_MPI_File_write_all : 
				GET_PTRS_VALUE_MPI_File_write_all(args);
				return;
			#endif
			#if HAVE_MPI_Improbe
			case MPI_API_ID_MPI_Improbe : 
				GET_PTRS_VALUE_MPI_Improbe(args);
				return;
			#endif
			#if HAVE_MPI_Comm_get_info
			case MPI_API_ID_MPI_Comm_get_info : 
				GET_PTRS_VALUE_MPI_Comm_get_info(args);
				return;
			#endif
			#if HAVE_MPI_File_read_all_end
			case MPI_API_ID_MPI_File_read_all_end : 
				GET_PTRS_VALUE_MPI_File_read_all_end(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_f90_integer
			case MPI_API_ID_MPI_Type_create_f90_integer : 
				GET_PTRS_VALUE_MPI_Type_create_f90_integer(args);
				return;
			#endif
			#if HAVE_MPI_Exscan_init
			case MPI_API_ID_MPI_Exscan_init : 
				GET_PTRS_VALUE_MPI_Exscan_init(args);
				return;
			#endif
			#if HAVE_MPI_Ibsend
			case MPI_API_ID_MPI_Ibsend : 
				GET_PTRS_VALUE_MPI_Ibsend(args);
				return;
			#endif
			#if HAVE_MPI_Ialltoallw
			case MPI_API_ID_MPI_Ialltoallw : 
				GET_PTRS_VALUE_MPI_Ialltoallw(args);
				return;
			#endif
			#if HAVE_MPI_Comm_create_from_group
			case MPI_API_ID_MPI_Comm_create_from_group : 
				GET_PTRS_VALUE_MPI_Comm_create_from_group(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_contents
			case MPI_API_ID_MPI_Type_get_contents : 
				GET_PTRS_VALUE_MPI_Type_get_contents(args);
				return;
			#endif
			#if HAVE_MPI_File_iwrite_at
			case MPI_API_ID_MPI_File_iwrite_at : 
				GET_PTRS_VALUE_MPI_File_iwrite_at(args);
				return;
			#endif
			#if HAVE_MPI_Status_set_elements
			case MPI_API_ID_MPI_Status_set_elements : 
				GET_PTRS_VALUE_MPI_Status_set_elements(args);
				return;
			#endif
			#if HAVE_MPI_File_read_ordered
			case MPI_API_ID_MPI_File_read_ordered : 
				GET_PTRS_VALUE_MPI_File_read_ordered(args);
				return;
			#endif
			#if HAVE_MPI_Is_thread_main
			case MPI_API_ID_MPI_Is_thread_main : 
				GET_PTRS_VALUE_MPI_Is_thread_main(args);
				return;
			#endif
			#if HAVE_MPI_Allreduce_init
			case MPI_API_ID_MPI_Allreduce_init : 
				GET_PTRS_VALUE_MPI_Allreduce_init(args);
				return;
			#endif
			#if HAVE_MPI_Info_get_valuelen
			case MPI_API_ID_MPI_Info_get_valuelen : 
				GET_PTRS_VALUE_MPI_Info_get_valuelen(args);
				return;
			#endif
			#if HAVE_MPI_Comm_create_errhandler
			case MPI_API_ID_MPI_Comm_create_errhandler : 
				GET_PTRS_VALUE_MPI_Comm_create_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Info_get_nthkey
			case MPI_API_ID_MPI_Info_get_nthkey : 
				GET_PTRS_VALUE_MPI_Info_get_nthkey(args);
				return;
			#endif
			#if HAVE_MPI_Ssend_init
			case MPI_API_ID_MPI_Ssend_init : 
				GET_PTRS_VALUE_MPI_Ssend_init(args);
				return;
			#endif
			#if HAVE_MPI_Cart_create
			case MPI_API_ID_MPI_Cart_create : 
				GET_PTRS_VALUE_MPI_Cart_create(args);
				return;
			#endif
			#if HAVE_MPI_Scan_init
			case MPI_API_ID_MPI_Scan_init : 
				GET_PTRS_VALUE_MPI_Scan_init(args);
				return;
			#endif
			#if HAVE_MPI_Irsend
			case MPI_API_ID_MPI_Irsend : 
				GET_PTRS_VALUE_MPI_Irsend(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_alltoallv
			case MPI_API_ID_MPI_Neighbor_alltoallv : 
				GET_PTRS_VALUE_MPI_Neighbor_alltoallv(args);
				return;
			#endif
			#if HAVE_MPI_Pready_list
			case MPI_API_ID_MPI_Pready_list : 
				GET_PTRS_VALUE_MPI_Pready_list(args);
				return;
			#endif
			#if HAVE_MPI_Alltoallw_init
			case MPI_API_ID_MPI_Alltoallw_init : 
				GET_PTRS_VALUE_MPI_Alltoallw_init(args);
				return;
			#endif
			#if HAVE_MPI_Dist_graph_create_adjacent
			case MPI_API_ID_MPI_Dist_graph_create_adjacent : 
				GET_PTRS_VALUE_MPI_Dist_graph_create_adjacent(args);
				return;
			#endif
			#if HAVE_MPI_Reduce_scatter_init
			case MPI_API_ID_MPI_Reduce_scatter_init : 
				GET_PTRS_VALUE_MPI_Reduce_scatter_init(args);
				return;
			#endif
			#if HAVE_MPI_Comm_get_parent
			case MPI_API_ID_MPI_Comm_get_parent : 
				GET_PTRS_VALUE_MPI_Comm_get_parent(args);
				return;
			#endif
			#if HAVE_MPI_Info_set
			case MPI_API_ID_MPI_Info_set : 
				GET_PTRS_VALUE_MPI_Info_set(args);
				return;
			#endif
			#if HAVE_MPI_Keyval_create
			case MPI_API_ID_MPI_Keyval_create : 
				GET_PTRS_VALUE_MPI_Keyval_create(args);
				return;
			#endif
			#if HAVE_MPI_Comm_connect
			case MPI_API_ID_MPI_Comm_connect : 
				GET_PTRS_VALUE_MPI_Comm_connect(args);
				return;
			#endif
			#if HAVE_MPI_Scatterv_init
			case MPI_API_ID_MPI_Scatterv_init : 
				GET_PTRS_VALUE_MPI_Scatterv_init(args);
				return;
			#endif
			#if HAVE_MPI_File_write_at_all_end
			case MPI_API_ID_MPI_File_write_at_all_end : 
				GET_PTRS_VALUE_MPI_File_write_at_all_end(args);
				return;
			#endif
			#if HAVE_MPI_File_write_all_end
			case MPI_API_ID_MPI_File_write_all_end : 
				GET_PTRS_VALUE_MPI_File_write_all_end(args);
				return;
			#endif
			#if HAVE_MPI_Buffer_detach
			case MPI_API_ID_MPI_Buffer_detach : 
				GET_PTRS_VALUE_MPI_Buffer_detach(args);
				return;
			#endif
			#if HAVE_MPI_Startall
			case MPI_API_ID_MPI_Startall : 
				GET_PTRS_VALUE_MPI_Startall(args);
				return;
			#endif
			#if HAVE_MPI_File_read_ordered_end
			case MPI_API_ID_MPI_File_read_ordered_end : 
				GET_PTRS_VALUE_MPI_File_read_ordered_end(args);
				return;
			#endif
			#if HAVE_MPI_File_write_at
			case MPI_API_ID_MPI_File_write_at : 
				GET_PTRS_VALUE_MPI_File_write_at(args);
				return;
			#endif
			#if HAVE_MPI_Session_get_pset_info
			case MPI_API_ID_MPI_Session_get_pset_info : 
				GET_PTRS_VALUE_MPI_Session_get_pset_info(args);
				return;
			#endif
			#if HAVE_MPI_Topo_test
			case MPI_API_ID_MPI_Topo_test : 
				GET_PTRS_VALUE_MPI_Topo_test(args);
				return;
			#endif
			#if HAVE_MPI_Comm_disconnect
			case MPI_API_ID_MPI_Comm_disconnect : 
				GET_PTRS_VALUE_MPI_Comm_disconnect(args);
				return;
			#endif
			#if HAVE_MPI_Add_error_class
			case MPI_API_ID_MPI_Add_error_class : 
				GET_PTRS_VALUE_MPI_Add_error_class(args);
				return;
			#endif
			#if HAVE_MPI_Ireduce_scatter
			case MPI_API_ID_MPI_Ireduce_scatter : 
				GET_PTRS_VALUE_MPI_Ireduce_scatter(args);
				return;
			#endif
			#if HAVE_MPI_Cart_map
			case MPI_API_ID_MPI_Cart_map : 
				GET_PTRS_VALUE_MPI_Cart_map(args);
				return;
			#endif
			#if HAVE_MPI_Intercomm_merge
			case MPI_API_ID_MPI_Intercomm_merge : 
				GET_PTRS_VALUE_MPI_Intercomm_merge(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_hindexed
			case MPI_API_ID_MPI_Type_create_hindexed : 
				GET_PTRS_VALUE_MPI_Type_create_hindexed(args);
				return;
			#endif
			#if HAVE_MPI_Info_get_nkeys
			case MPI_API_ID_MPI_Info_get_nkeys : 
				GET_PTRS_VALUE_MPI_Info_get_nkeys(args);
				return;
			#endif
			#if HAVE_MPI_File_read
			case MPI_API_ID_MPI_File_read : 
				GET_PTRS_VALUE_MPI_File_read(args);
				return;
			#endif
			#if HAVE_MPI_Ineighbor_allgatherv
			case MPI_API_ID_MPI_Ineighbor_allgatherv : 
				GET_PTRS_VALUE_MPI_Ineighbor_allgatherv(args);
				return;
			#endif
			#if HAVE_MPI_Status_set_elements_x
			case MPI_API_ID_MPI_Status_set_elements_x : 
				GET_PTRS_VALUE_MPI_Status_set_elements_x(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_f90_real
			case MPI_API_ID_MPI_Type_create_f90_real : 
				GET_PTRS_VALUE_MPI_Type_create_f90_real(args);
				return;
			#endif
			#if HAVE_MPI_Probe
			case MPI_API_ID_MPI_Probe : 
				GET_PTRS_VALUE_MPI_Probe(args);
				return;
			#endif
			#if HAVE_MPI_File_close
			case MPI_API_ID_MPI_File_close : 
				GET_PTRS_VALUE_MPI_File_close(args);
				return;
			#endif
			#if HAVE_MPI_Request_get_status
			case MPI_API_ID_MPI_Request_get_status : 
				GET_PTRS_VALUE_MPI_Request_get_status(args);
				return;
			#endif
			#if HAVE_MPI_Rget_accumulate
			case MPI_API_ID_MPI_Rget_accumulate : 
				GET_PTRS_VALUE_MPI_Rget_accumulate(args);
				return;
			#endif
			#if HAVE_MPI_File_iread_all
			case MPI_API_ID_MPI_File_iread_all : 
				GET_PTRS_VALUE_MPI_File_iread_all(args);
				return;
			#endif
			#if HAVE_MPI_Isendrecv
			case MPI_API_ID_MPI_Isendrecv : 
				GET_PTRS_VALUE_MPI_Isendrecv(args);
				return;
			#endif
			#if HAVE_MPI_Pack_external
			case MPI_API_ID_MPI_Pack_external : 
				GET_PTRS_VALUE_MPI_Pack_external(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_envelope
			case MPI_API_ID_MPI_Type_get_envelope : 
				GET_PTRS_VALUE_MPI_Type_get_envelope(args);
				return;
			#endif
			#if HAVE_MPI_Win_create
			case MPI_API_ID_MPI_Win_create : 
				GET_PTRS_VALUE_MPI_Win_create(args);
				return;
			#endif
			#if HAVE_MPI_Isendrecv_replace
			case MPI_API_ID_MPI_Isendrecv_replace : 
				GET_PTRS_VALUE_MPI_Isendrecv_replace(args);
				return;
			#endif
			#if HAVE_MPI_Cartdim_get
			case MPI_API_ID_MPI_Cartdim_get : 
				GET_PTRS_VALUE_MPI_Cartdim_get(args);
				return;
			#endif
			#if HAVE_MPI_Dist_graph_neighbors
			case MPI_API_ID_MPI_Dist_graph_neighbors : 
				GET_PTRS_VALUE_MPI_Dist_graph_neighbors(args);
				return;
			#endif
			#if HAVE_MPI_Pack_external_size
			case MPI_API_ID_MPI_Pack_external_size : 
				GET_PTRS_VALUE_MPI_Pack_external_size(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_f90_complex
			case MPI_API_ID_MPI_Type_create_f90_complex : 
				GET_PTRS_VALUE_MPI_Type_create_f90_complex(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_alltoallw_init
			case MPI_API_ID_MPI_Neighbor_alltoallw_init : 
				GET_PTRS_VALUE_MPI_Neighbor_alltoallw_init(args);
				return;
			#endif
			#if HAVE_MPI_Rget
			case MPI_API_ID_MPI_Rget : 
				GET_PTRS_VALUE_MPI_Rget(args);
				return;
			#endif
			#if HAVE_MPI_Win_create_keyval
			case MPI_API_ID_MPI_Win_create_keyval : 
				GET_PTRS_VALUE_MPI_Win_create_keyval(args);
				return;
			#endif
			#if HAVE_MPI_Op_commutative
			case MPI_API_ID_MPI_Op_commutative : 
				GET_PTRS_VALUE_MPI_Op_commutative(args);
				return;
			#endif
			#if HAVE_MPI_Scatter_init
			case MPI_API_ID_MPI_Scatter_init : 
				GET_PTRS_VALUE_MPI_Scatter_init(args);
				return;
			#endif
			#if HAVE_MPI_Info_get_string
			case MPI_API_ID_MPI_Info_get_string : 
				GET_PTRS_VALUE_MPI_Info_get_string(args);
				return;
			#endif
			#if HAVE_MPI_Mrecv
			case MPI_API_ID_MPI_Mrecv : 
				GET_PTRS_VALUE_MPI_Mrecv(args);
				return;
			#endif
			#if HAVE_MPI_Open_port
			case MPI_API_ID_MPI_Open_port : 
				GET_PTRS_VALUE_MPI_Open_port(args);
				return;
			#endif
			#if HAVE_MPI_Cart_get
			case MPI_API_ID_MPI_Cart_get : 
				GET_PTRS_VALUE_MPI_Cart_get(args);
				return;
			#endif
			#if HAVE_MPI_Lookup_name
			case MPI_API_ID_MPI_Lookup_name : 
				GET_PTRS_VALUE_MPI_Lookup_name(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_extent
			case MPI_API_ID_MPI_Type_get_extent : 
				GET_PTRS_VALUE_MPI_Type_get_extent(args);
				return;
			#endif
			#if HAVE_MPI_Comm_spawn
			case MPI_API_ID_MPI_Comm_spawn : 
				GET_PTRS_VALUE_MPI_Comm_spawn(args);
				return;
			#endif
			#if HAVE_MPI_Unpublish_name
			case MPI_API_ID_MPI_Unpublish_name : 
				GET_PTRS_VALUE_MPI_Unpublish_name(args);
				return;
			#endif
			#if HAVE_MPI_File_get_group
			case MPI_API_ID_MPI_File_get_group : 
				GET_PTRS_VALUE_MPI_File_get_group(args);
				return;
			#endif
			#if HAVE_MPI_File_iread_at_all
			case MPI_API_ID_MPI_File_iread_at_all : 
				GET_PTRS_VALUE_MPI_File_iread_at_all(args);
				return;
			#endif
			#if HAVE_MPI_Graphdims_get
			case MPI_API_ID_MPI_Graphdims_get : 
				GET_PTRS_VALUE_MPI_Graphdims_get(args);
				return;
			#endif
			#if HAVE_MPI_File_iread_shared
			case MPI_API_ID_MPI_File_iread_shared : 
				GET_PTRS_VALUE_MPI_File_iread_shared(args);
				return;
			#endif
			#if HAVE_MPI_Comm_idup_with_info
			case MPI_API_ID_MPI_Comm_idup_with_info : 
				GET_PTRS_VALUE_MPI_Comm_idup_with_info(args);
				return;
			#endif
			#if HAVE_MPI_Get_version
			case MPI_API_ID_MPI_Get_version : 
				GET_PTRS_VALUE_MPI_Get_version(args);
				return;
			#endif
			#if HAVE_MPI_Intercomm_create_from_groups
			case MPI_API_ID_MPI_Intercomm_create_from_groups : 
				GET_PTRS_VALUE_MPI_Intercomm_create_from_groups(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_alltoallv_init
			case MPI_API_ID_MPI_Neighbor_alltoallv_init : 
				GET_PTRS_VALUE_MPI_Neighbor_alltoallv_init(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_darray
			case MPI_API_ID_MPI_Type_create_darray : 
				GET_PTRS_VALUE_MPI_Type_create_darray(args);
				return;
			#endif
			#if HAVE_MPI_File_get_position_shared
			case MPI_API_ID_MPI_File_get_position_shared : 
				GET_PTRS_VALUE_MPI_File_get_position_shared(args);
				return;
			#endif
			#if HAVE_MPI_Win_get_group
			case MPI_API_ID_MPI_Win_get_group : 
				GET_PTRS_VALUE_MPI_Win_get_group(args);
				return;
			#endif
			#if HAVE_MPI_Error_class
			case MPI_API_ID_MPI_Error_class : 
				GET_PTRS_VALUE_MPI_Error_class(args);
				return;
			#endif
			#if HAVE_MPI_Win_get_attr
			case MPI_API_ID_MPI_Win_get_attr : 
				GET_PTRS_VALUE_MPI_Win_get_attr(args);
				return;
			#endif
			#if HAVE_MPI_Ireduce_scatter_block
			case MPI_API_ID_MPI_Ireduce_scatter_block : 
				GET_PTRS_VALUE_MPI_Ireduce_scatter_block(args);
				return;
			#endif
			#if HAVE_MPI_Status_set_cancelled
			case MPI_API_ID_MPI_Status_set_cancelled : 
				GET_PTRS_VALUE_MPI_Status_set_cancelled(args);
				return;
			#endif
			#if HAVE_MPI_Win_test
			case MPI_API_ID_MPI_Win_test : 
				GET_PTRS_VALUE_MPI_Win_test(args);
				return;
			#endif
			#if HAVE_MPI_Test_cancelled
			case MPI_API_ID_MPI_Test_cancelled : 
				GET_PTRS_VALUE_MPI_Test_cancelled(args);
				return;
			#endif
			#if HAVE_MPI_Error_string
			case MPI_API_ID_MPI_Error_string : 
				GET_PTRS_VALUE_MPI_Error_string(args);
				return;
			#endif
			#if HAVE_MPI_Graph_neighbors_count
			case MPI_API_ID_MPI_Graph_neighbors_count : 
				GET_PTRS_VALUE_MPI_Graph_neighbors_count(args);
				return;
			#endif
			#if HAVE_MPI_Session_create_errhandler
			case MPI_API_ID_MPI_Session_create_errhandler : 
				GET_PTRS_VALUE_MPI_Session_create_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Iscatter
			case MPI_API_ID_MPI_Iscatter : 
				GET_PTRS_VALUE_MPI_Iscatter(args);
				return;
			#endif
			#if HAVE_MPI_File_read_all
			case MPI_API_ID_MPI_File_read_all : 
				GET_PTRS_VALUE_MPI_File_read_all(args);
				return;
			#endif
			#if HAVE_MPI_File_get_errhandler
			case MPI_API_ID_MPI_File_get_errhandler : 
				GET_PTRS_VALUE_MPI_File_get_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Session_finalize
			case MPI_API_ID_MPI_Session_finalize : 
				GET_PTRS_VALUE_MPI_Session_finalize(args);
				return;
			#endif
			#if HAVE_MPI_File_iwrite_all
			case MPI_API_ID_MPI_File_iwrite_all : 
				GET_PTRS_VALUE_MPI_File_iwrite_all(args);
				return;
			#endif
			#if HAVE_MPI_Alltoallv_init
			case MPI_API_ID_MPI_Alltoallv_init : 
				GET_PTRS_VALUE_MPI_Alltoallv_init(args);
				return;
			#endif
			#if HAVE_MPI_File_get_position
			case MPI_API_ID_MPI_File_get_position : 
				GET_PTRS_VALUE_MPI_File_get_position(args);
				return;
			#endif
			#if HAVE_MPI_File_write_shared
			case MPI_API_ID_MPI_File_write_shared : 
				GET_PTRS_VALUE_MPI_File_write_shared(args);
				return;
			#endif
			#if HAVE_MPI_Win_create_dynamic
			case MPI_API_ID_MPI_Win_create_dynamic : 
				GET_PTRS_VALUE_MPI_Win_create_dynamic(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_alltoallw
			case MPI_API_ID_MPI_Neighbor_alltoallw : 
				GET_PTRS_VALUE_MPI_Neighbor_alltoallw(args);
				return;
			#endif
			#if HAVE_MPI_Iexscan
			case MPI_API_ID_MPI_Iexscan : 
				GET_PTRS_VALUE_MPI_Iexscan(args);
				return;
			#endif
			#if HAVE_MPI_Graph_map
			case MPI_API_ID_MPI_Graph_map : 
				GET_PTRS_VALUE_MPI_Graph_map(args);
				return;
			#endif
			#if HAVE_MPI_Recv_init
			case MPI_API_ID_MPI_Recv_init : 
				GET_PTRS_VALUE_MPI_Recv_init(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_subarray
			case MPI_API_ID_MPI_Type_create_subarray : 
				GET_PTRS_VALUE_MPI_Type_create_subarray(args);
				return;
			#endif
			#if HAVE_MPI_Comm_create_group
			case MPI_API_ID_MPI_Comm_create_group : 
				GET_PTRS_VALUE_MPI_Comm_create_group(args);
				return;
			#endif
			#if HAVE_MPI_Allgather_init
			case MPI_API_ID_MPI_Allgather_init : 
				GET_PTRS_VALUE_MPI_Allgather_init(args);
				return;
			#endif
			#if HAVE_MPI_Reduce_scatter_block_init
			case MPI_API_ID_MPI_Reduce_scatter_block_init : 
				GET_PTRS_VALUE_MPI_Reduce_scatter_block_init(args);
				return;
			#endif
			#if HAVE_MPI_Type_match_size
			case MPI_API_ID_MPI_Type_match_size : 
				GET_PTRS_VALUE_MPI_Type_match_size(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_true_extent
			case MPI_API_ID_MPI_Type_get_true_extent : 
				GET_PTRS_VALUE_MPI_Type_get_true_extent(args);
				return;
			#endif
			#if HAVE_MPI_Alltoall_init
			case MPI_API_ID_MPI_Alltoall_init : 
				GET_PTRS_VALUE_MPI_Alltoall_init(args);
				return;
			#endif
			#if HAVE_MPI_Send_init
			case MPI_API_ID_MPI_Send_init : 
				GET_PTRS_VALUE_MPI_Send_init(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_allgather_init
			case MPI_API_ID_MPI_Neighbor_allgather_init : 
				GET_PTRS_VALUE_MPI_Neighbor_allgather_init(args);
				return;
			#endif
			#if HAVE_MPI_Ibcast
			case MPI_API_ID_MPI_Ibcast : 
				GET_PTRS_VALUE_MPI_Ibcast(args);
				return;
			#endif
			#if HAVE_MPI_File_iread
			case MPI_API_ID_MPI_File_iread : 
				GET_PTRS_VALUE_MPI_File_iread(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_alltoall_init
			case MPI_API_ID_MPI_Neighbor_alltoall_init : 
				GET_PTRS_VALUE_MPI_Neighbor_alltoall_init(args);
				return;
			#endif
			#if HAVE_MPI_Cart_rank
			case MPI_API_ID_MPI_Cart_rank : 
				GET_PTRS_VALUE_MPI_Cart_rank(args);
				return;
			#endif
			#if HAVE_MPI_Publish_name
			case MPI_API_ID_MPI_Publish_name : 
				GET_PTRS_VALUE_MPI_Publish_name(args);
				return;
			#endif
			#if HAVE_MPI_File_write
			case MPI_API_ID_MPI_File_write : 
				GET_PTRS_VALUE_MPI_File_write(args);
				return;
			#endif
			#if HAVE_MPI_Register_datarep
			case MPI_API_ID_MPI_Register_datarep : 
				GET_PTRS_VALUE_MPI_Register_datarep(args);
				return;
			#endif
			#if HAVE_MPI_Ineighbor_alltoall
			case MPI_API_ID_MPI_Ineighbor_alltoall : 
				GET_PTRS_VALUE_MPI_Ineighbor_alltoall(args);
				return;
			#endif
			#if HAVE_MPI_Iallgatherv
			case MPI_API_ID_MPI_Iallgatherv : 
				GET_PTRS_VALUE_MPI_Iallgatherv(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_allgatherv_init
			case MPI_API_ID_MPI_Neighbor_allgatherv_init : 
				GET_PTRS_VALUE_MPI_Neighbor_allgatherv_init(args);
				return;
			#endif
			#if HAVE_MPI_Iprobe
			case MPI_API_ID_MPI_Iprobe : 
				GET_PTRS_VALUE_MPI_Iprobe(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_true_extent_x
			case MPI_API_ID_MPI_Type_get_true_extent_x : 
				GET_PTRS_VALUE_MPI_Type_get_true_extent_x(args);
				return;
			#endif
			#if HAVE_MPI_Unpack_external
			case MPI_API_ID_MPI_Unpack_external : 
				GET_PTRS_VALUE_MPI_Unpack_external(args);
				return;
			#endif
			#if HAVE_MPI_Mprobe
			case MPI_API_ID_MPI_Mprobe : 
				GET_PTRS_VALUE_MPI_Mprobe(args);
				return;
			#endif
			#if HAVE_MPI_Add_error_code
			case MPI_API_ID_MPI_Add_error_code : 
				GET_PTRS_VALUE_MPI_Add_error_code(args);
				return;
			#endif
			#if HAVE_MPI_File_read_at_all
			case MPI_API_ID_MPI_File_read_at_all : 
				GET_PTRS_VALUE_MPI_File_read_at_all(args);
				return;
			#endif
			#if HAVE_MPI_Iscatterv
			case MPI_API_ID_MPI_Iscatterv : 
				GET_PTRS_VALUE_MPI_Iscatterv(args);
				return;
			#endif
			#if HAVE_MPI_Iallreduce
			case MPI_API_ID_MPI_Iallreduce : 
				GET_PTRS_VALUE_MPI_Iallreduce(args);
				return;
			#endif
			#if HAVE_MPI_Get_processor_name
			case MPI_API_ID_MPI_Get_processor_name : 
				GET_PTRS_VALUE_MPI_Get_processor_name(args);
				return;
			#endif
			#if HAVE_MPI_Start
			case MPI_API_ID_MPI_Start : 
				GET_PTRS_VALUE_MPI_Start(args);
				return;
			#endif
			#if HAVE_MPI_File_get_type_extent
			case MPI_API_ID_MPI_File_get_type_extent : 
				GET_PTRS_VALUE_MPI_File_get_type_extent(args);
				return;
			#endif
			#if HAVE_MPI_File_read_shared
			case MPI_API_ID_MPI_File_read_shared : 
				GET_PTRS_VALUE_MPI_File_read_shared(args);
				return;
			#endif
			#if HAVE_MPI_File_open
			case MPI_API_ID_MPI_File_open : 
				GET_PTRS_VALUE_MPI_File_open(args);
				return;
			#endif
			#if HAVE_MPI_File_get_amode
			case MPI_API_ID_MPI_File_get_amode : 
				GET_PTRS_VALUE_MPI_File_get_amode(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_hindexed_block
			case MPI_API_ID_MPI_Type_create_hindexed_block : 
				GET_PTRS_VALUE_MPI_Type_create_hindexed_block(args);
				return;
			#endif
			#if HAVE_MPI_Cart_coords
			case MPI_API_ID_MPI_Cart_coords : 
				GET_PTRS_VALUE_MPI_Cart_coords(args);
				return;
			#endif
			#if HAVE_MPI_Issend
			case MPI_API_ID_MPI_Issend : 
				GET_PTRS_VALUE_MPI_Issend(args);
				return;
			#endif
			#if HAVE_MPI_Graph_get
			case MPI_API_ID_MPI_Graph_get : 
				GET_PTRS_VALUE_MPI_Graph_get(args);
				return;
			#endif
			#if HAVE_MPI_Ineighbor_alltoallw
			case MPI_API_ID_MPI_Ineighbor_alltoallw : 
				GET_PTRS_VALUE_MPI_Ineighbor_alltoallw(args);
				return;
			#endif
			#if HAVE_MPI_File_iread_at
			case MPI_API_ID_MPI_File_iread_at : 
				GET_PTRS_VALUE_MPI_File_iread_at(args);
				return;
			#endif
			#if HAVE_MPI_Session_get_info
			case MPI_API_ID_MPI_Session_get_info : 
				GET_PTRS_VALUE_MPI_Session_get_info(args);
				return;
			#endif
			#if HAVE_MPI_Session_get_nth_pset
			case MPI_API_ID_MPI_Session_get_nth_pset : 
				GET_PTRS_VALUE_MPI_Session_get_nth_pset(args);
				return;
			#endif
			#if HAVE_MPI_Type_create_keyval
			case MPI_API_ID_MPI_Type_create_keyval : 
				GET_PTRS_VALUE_MPI_Type_create_keyval(args);
				return;
			#endif
			#if HAVE_MPI_Attr_get
			case MPI_API_ID_MPI_Attr_get : 
				GET_PTRS_VALUE_MPI_Attr_get(args);
				return;
			#endif
			#if HAVE_MPI_Add_error_string
			case MPI_API_ID_MPI_Add_error_string : 
				GET_PTRS_VALUE_MPI_Add_error_string(args);
				return;
			#endif
			#if HAVE_MPI_Ineighbor_alltoallv
			case MPI_API_ID_MPI_Ineighbor_alltoallv : 
				GET_PTRS_VALUE_MPI_Ineighbor_alltoallv(args);
				return;
			#endif
			#if HAVE_MPI_Imrecv
			case MPI_API_ID_MPI_Imrecv : 
				GET_PTRS_VALUE_MPI_Imrecv(args);
				return;
			#endif
			#if HAVE_MPI_Alltoallw
			case MPI_API_ID_MPI_Alltoallw : 
				GET_PTRS_VALUE_MPI_Alltoallw(args);
				return;
			#endif
			#if HAVE_MPI_Bcast_init
			case MPI_API_ID_MPI_Bcast_init : 
				GET_PTRS_VALUE_MPI_Bcast_init(args);
				return;
			#endif
			#if HAVE_MPI_Ibarrier
			case MPI_API_ID_MPI_Ibarrier : 
				GET_PTRS_VALUE_MPI_Ibarrier(args);
				return;
			#endif
			#if HAVE_MPI_File_iwrite_at_all
			case MPI_API_ID_MPI_File_iwrite_at_all : 
				GET_PTRS_VALUE_MPI_File_iwrite_at_all(args);
				return;
			#endif
			#if HAVE_MPI_File_get_size
			case MPI_API_ID_MPI_File_get_size : 
				GET_PTRS_VALUE_MPI_File_get_size(args);
				return;
			#endif
			#if HAVE_MPI_Barrier_init
			case MPI_API_ID_MPI_Barrier_init : 
				GET_PTRS_VALUE_MPI_Barrier_init(args);
				return;
			#endif
			#if HAVE_MPI_File_get_view
			case MPI_API_ID_MPI_File_get_view : 
				GET_PTRS_VALUE_MPI_File_get_view(args);
				return;
			#endif
			#if HAVE_MPI_Win_allocate_shared
			case MPI_API_ID_MPI_Win_allocate_shared : 
				GET_PTRS_VALUE_MPI_Win_allocate_shared(args);
				return;
			#endif
			#if HAVE_MPI_Close_port
			case MPI_API_ID_MPI_Close_port : 
				GET_PTRS_VALUE_MPI_Close_port(args);
				return;
			#endif
			#if HAVE_MPI_Finalized
			case MPI_API_ID_MPI_Finalized : 
				GET_PTRS_VALUE_MPI_Finalized(args);
				return;
			#endif
			#if HAVE_MPI_Info_dup
			case MPI_API_ID_MPI_Info_dup : 
				GET_PTRS_VALUE_MPI_Info_dup(args);
				return;
			#endif
			#if HAVE_MPI_Info_get
			case MPI_API_ID_MPI_Info_get : 
				GET_PTRS_VALUE_MPI_Info_get(args);
				return;
			#endif
			#if HAVE_MPI_Get_library_version
			case MPI_API_ID_MPI_Get_library_version : 
				GET_PTRS_VALUE_MPI_Get_library_version(args);
				return;
			#endif
			#if HAVE_MPI_Info_create
			case MPI_API_ID_MPI_Info_create : 
				GET_PTRS_VALUE_MPI_Info_create(args);
				return;
			#endif
			#if HAVE_MPI_Iallgather
			case MPI_API_ID_MPI_Iallgather : 
				GET_PTRS_VALUE_MPI_Iallgather(args);
				return;
			#endif
			#if HAVE_MPI_Comm_spawn_multiple
			case MPI_API_ID_MPI_Comm_spawn_multiple : 
				GET_PTRS_VALUE_MPI_Comm_spawn_multiple(args);
				return;
			#endif
			#if HAVE_MPI_Precv_init
			case MPI_API_ID_MPI_Precv_init : 
				GET_PTRS_VALUE_MPI_Precv_init(args);
				return;
			#endif
			#if HAVE_MPI_File_set_view
			case MPI_API_ID_MPI_File_set_view : 
				GET_PTRS_VALUE_MPI_File_set_view(args);
				return;
			#endif
			#if HAVE_MPI_Type_size
			case MPI_API_ID_MPI_Type_size : 
				GET_PTRS_VALUE_MPI_Type_size(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_attr
			case MPI_API_ID_MPI_Type_get_attr : 
				GET_PTRS_VALUE_MPI_Type_get_attr(args);
				return;
			#endif
			#if HAVE_MPI_File_write_ordered
			case MPI_API_ID_MPI_File_write_ordered : 
				GET_PTRS_VALUE_MPI_File_write_ordered(args);
				return;
			#endif
			#if HAVE_MPI_File_get_info
			case MPI_API_ID_MPI_File_get_info : 
				GET_PTRS_VALUE_MPI_File_get_info(args);
				return;
			#endif
			#if HAVE_MPI_Graph_neighbors
			case MPI_API_ID_MPI_Graph_neighbors : 
				GET_PTRS_VALUE_MPI_Graph_neighbors(args);
				return;
			#endif
			#if HAVE_MPI_Igatherv
			case MPI_API_ID_MPI_Igatherv : 
				GET_PTRS_VALUE_MPI_Igatherv(args);
				return;
			#endif
			#if HAVE_MPI_Info_delete
			case MPI_API_ID_MPI_Info_delete : 
				GET_PTRS_VALUE_MPI_Info_delete(args);
				return;
			#endif
			#if HAVE_MPI_Comm_get_errhandler
			case MPI_API_ID_MPI_Comm_get_errhandler : 
				GET_PTRS_VALUE_MPI_Comm_get_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Session_init
			case MPI_API_ID_MPI_Session_init : 
				GET_PTRS_VALUE_MPI_Session_init(args);
				return;
			#endif
			#if HAVE_MPI_Intercomm_create
			case MPI_API_ID_MPI_Intercomm_create : 
				GET_PTRS_VALUE_MPI_Intercomm_create(args);
				return;
			#endif
			#if HAVE_MPI_Ialltoallv
			case MPI_API_ID_MPI_Ialltoallv : 
				GET_PTRS_VALUE_MPI_Ialltoallv(args);
				return;
			#endif
			#if HAVE_MPI_File_delete
			case MPI_API_ID_MPI_File_delete : 
				GET_PTRS_VALUE_MPI_File_delete(args);
				return;
			#endif
			#if HAVE_MPI_Dims_create
			case MPI_API_ID_MPI_Dims_create : 
				GET_PTRS_VALUE_MPI_Dims_create(args);
				return;
			#endif
			#if HAVE_MPI_Cart_sub
			case MPI_API_ID_MPI_Cart_sub : 
				GET_PTRS_VALUE_MPI_Cart_sub(args);
				return;
			#endif
			#if HAVE_MPI_Win_allocate
			case MPI_API_ID_MPI_Win_allocate : 
				GET_PTRS_VALUE_MPI_Win_allocate(args);
				return;
			#endif
			#if HAVE_MPI_Session_get_errhandler
			case MPI_API_ID_MPI_Session_get_errhandler : 
				GET_PTRS_VALUE_MPI_Session_get_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Parrived
			case MPI_API_ID_MPI_Parrived : 
				GET_PTRS_VALUE_MPI_Parrived(args);
				return;
			#endif
			#if HAVE_MPI_Info_create_env
			case MPI_API_ID_MPI_Info_create_env : 
				GET_PTRS_VALUE_MPI_Info_create_env(args);
				return;
			#endif
			#if HAVE_MPI_File_create_errhandler
			case MPI_API_ID_MPI_File_create_errhandler : 
				GET_PTRS_VALUE_MPI_File_create_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Ialltoall
			case MPI_API_ID_MPI_Ialltoall : 
				GET_PTRS_VALUE_MPI_Ialltoall(args);
				return;
			#endif
			#if HAVE_MPI_Raccumulate
			case MPI_API_ID_MPI_Raccumulate : 
				GET_PTRS_VALUE_MPI_Raccumulate(args);
				return;
			#endif
			#if HAVE_MPI_Type_size_x
			case MPI_API_ID_MPI_Type_size_x : 
				GET_PTRS_VALUE_MPI_Type_size_x(args);
				return;
			#endif
			#if HAVE_MPI_Type_get_extent_x
			case MPI_API_ID_MPI_Type_get_extent_x : 
				GET_PTRS_VALUE_MPI_Type_get_extent_x(args);
				return;
			#endif
			#if HAVE_MPI_Dist_graph_create
			case MPI_API_ID_MPI_Dist_graph_create : 
				GET_PTRS_VALUE_MPI_Dist_graph_create(args);
				return;
			#endif
			#if HAVE_MPI_Comm_join
			case MPI_API_ID_MPI_Comm_join : 
				GET_PTRS_VALUE_MPI_Comm_join(args);
				return;
			#endif
			#if HAVE_MPI_Gatherv_init
			case MPI_API_ID_MPI_Gatherv_init : 
				GET_PTRS_VALUE_MPI_Gatherv_init(args);
				return;
			#endif
			#if HAVE_MPI_Comm_accept
			case MPI_API_ID_MPI_Comm_accept : 
				GET_PTRS_VALUE_MPI_Comm_accept(args);
				return;
			#endif
			#if HAVE_MPI_Ineighbor_allgather
			case MPI_API_ID_MPI_Ineighbor_allgather : 
				GET_PTRS_VALUE_MPI_Ineighbor_allgather(args);
				return;
			#endif
			#if HAVE_MPI_Type_dup
			case MPI_API_ID_MPI_Type_dup : 
				GET_PTRS_VALUE_MPI_Type_dup(args);
				return;
			#endif
			#if HAVE_MPI_File_iwrite_shared
			case MPI_API_ID_MPI_File_iwrite_shared : 
				GET_PTRS_VALUE_MPI_File_iwrite_shared(args);
				return;
			#endif
			#if HAVE_MPI_Win_get_errhandler
			case MPI_API_ID_MPI_Win_get_errhandler : 
				GET_PTRS_VALUE_MPI_Win_get_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Iscan
			case MPI_API_ID_MPI_Iscan : 
				GET_PTRS_VALUE_MPI_Iscan(args);
				return;
			#endif
			#if HAVE_MPI_Graph_create
			case MPI_API_ID_MPI_Graph_create : 
				GET_PTRS_VALUE_MPI_Graph_create(args);
				return;
			#endif
			#if HAVE_MPI_Win_set_name
			case MPI_API_ID_MPI_Win_set_name : 
				GET_PTRS_VALUE_MPI_Win_set_name(args);
				return;
			#endif
			#if HAVE_MPI_Win_create_errhandler
			case MPI_API_ID_MPI_Win_create_errhandler : 
				GET_PTRS_VALUE_MPI_Win_create_errhandler(args);
				return;
			#endif
			#if HAVE_MPI_Gather_init
			case MPI_API_ID_MPI_Gather_init : 
				GET_PTRS_VALUE_MPI_Gather_init(args);
				return;
			#endif
			#if HAVE_MPI_Neighbor_allgatherv
			case MPI_API_ID_MPI_Neighbor_allgatherv : 
				GET_PTRS_VALUE_MPI_Neighbor_allgatherv(args);
				return;
			#endif
			#if HAVE_MPI_File_iwrite
			case MPI_API_ID_MPI_File_iwrite : 
				GET_PTRS_VALUE_MPI_File_iwrite(args);
				return;
			#endif
			#if HAVE_MPI_Status_f082f
			case MPI_API_ID_MPI_Status_f082f : 
				GET_PTRS_VALUE_MPI_Status_f082f(args);
				return;
			#endif
			#if HAVE_MPI_Status_c2f08
			case MPI_API_ID_MPI_Status_c2f08 : 
				GET_PTRS_VALUE_MPI_Status_c2f08(args);
				return;
			#endif
			#if HAVE_MPI_Status_c2f
			case MPI_API_ID_MPI_Status_c2f : 
				GET_PTRS_VALUE_MPI_Status_c2f(args);
				return;
			#endif
			#if HAVE_MPI_Status_f082c
			case MPI_API_ID_MPI_Status_f082c : 
				GET_PTRS_VALUE_MPI_Status_f082c(args);
				return;
			#endif
			#if HAVE_MPI_Status_f2f08
			case MPI_API_ID_MPI_Status_f2f08 : 
				GET_PTRS_VALUE_MPI_Status_f2f08(args);
				return;
			#endif
			#if HAVE_MPI_Status_f2c
			case MPI_API_ID_MPI_Status_f2c : 
				GET_PTRS_VALUE_MPI_Status_f2c(args);
				return;
			#endif
            default : break;
        }
    } else {
        switch(id) {
			#if HAVE_MPI_Request_free
			case MPI_API_ID_MPI_Request_free : 
				GET_PTRS_VALUE_MPI_Request_free(args);
				return;
			#endif
			#if HAVE_MPI_Type_free
			case MPI_API_ID_MPI_Type_free : 
				GET_PTRS_VALUE_MPI_Type_free(args);
				return;
			#endif
			#if HAVE_MPI_Comm_free
			case MPI_API_ID_MPI_Comm_free : 
				GET_PTRS_VALUE_MPI_Comm_free(args);
				return;
			#endif
			#if HAVE_MPI_Group_free
			case MPI_API_ID_MPI_Group_free : 
				GET_PTRS_VALUE_MPI_Group_free(args);
				return;
			#endif
			#if HAVE_MPI_Op_free
			case MPI_API_ID_MPI_Op_free : 
				GET_PTRS_VALUE_MPI_Op_free(args);
				return;
			#endif
			#if HAVE_MPI_Errhandler_free
			case MPI_API_ID_MPI_Errhandler_free : 
				GET_PTRS_VALUE_MPI_Errhandler_free(args);
				return;
			#endif
			#if HAVE_MPI_Info_free
			case MPI_API_ID_MPI_Info_free : 
				GET_PTRS_VALUE_MPI_Info_free(args);
				return;
			#endif
			#if HAVE_MPI_Keyval_free
			case MPI_API_ID_MPI_Keyval_free : 
				GET_PTRS_VALUE_MPI_Keyval_free(args);
				return;
			#endif
			#if HAVE_MPI_Win_free
			case MPI_API_ID_MPI_Win_free : 
				GET_PTRS_VALUE_MPI_Win_free(args);
				return;
			#endif
			#if HAVE_MPI_Comm_free_keyval
			case MPI_API_ID_MPI_Comm_free_keyval : 
				GET_PTRS_VALUE_MPI_Comm_free_keyval(args);
				return;
			#endif
			#if HAVE_MPI_Type_free_keyval
			case MPI_API_ID_MPI_Type_free_keyval : 
				GET_PTRS_VALUE_MPI_Type_free_keyval(args);
				return;
			#endif
			#if HAVE_MPI_Win_free_keyval
			case MPI_API_ID_MPI_Win_free_keyval : 
				GET_PTRS_VALUE_MPI_Win_free_keyval(args);
				return;
			#endif
            default : break;
        }
    }
}
#endif // MPI_API_HELPER_H